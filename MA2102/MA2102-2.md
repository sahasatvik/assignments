# Linear Algebra I

$
\newcommand\C{\mathbb{C}}
\newcommand\R{\mathbb{R}}
\newcommand\Q{\mathbb{Q}}
\newcommand\Z{\mathbb{Z}}
\newcommand\N{\mathbb{N}}
\newcommand\ve[1]{\boldsymbol{#1}}
\newcommand\norm[1]{\left\Vert #1 \right\Vert}
\def\u{\ve{u}}
\def\v{\ve{v}}
\def\w{\ve{w}}
\def\span{\operatorname{span}}
\def\Id{\operatorname{Id}}
\def\kernel{\operatorname{ker}}
\def\image{\operatorname{im}}
\def\L{\mathscr{L}}
\def\ev{\operatorname{ev}}
$

## Linear maps
A linear map $T\colon V \to W$ between vector spaces (over a field $F$) is such that $T(\v + \w) = T(\v) + T(\w)$ and $T(\lambda\v) = \lambda T(\v)$ for all $\v, \w \in V$ and scalars $\lambda$.

This immediately forces the following properties.

- $T(\ve0_V) = \ve0_W$
- $T(\lambda_1\v_1 + \dots + \lambda_n\v_n) = \lambda_1T(\v_1) + \dots + \lambda_nT(\v_n)$.

The following are all linear maps.

- **Zero/trivial map**: $$\mathbb{O}\colon V\to W, \quad\v \mapsto \ve0.$$
- **Identity map**: $$\Id\colon V\to V,\quad \v \mapsto \v.$$
- **Scaling**: For some fixed scalar $c$, $$T\colon \R\to\R, \quad x \mapsto cx. $$

> Indeed, *all* linear maps from $\R$ to $\R$ are scaling maps.

- **Dilation**: For some fixed $c \in F$,
$$T\colon V\to V,\quad \v \mapsto c\v.$$
- **Matrices**: For some $A \in M_{m\times n}(\R)$,
$$L_A\colon \R^n\to\R^m,\quad \v\mapsto A\v.$$
- **Reflections**: For example, a reflection of $\R^2$ about the $x$-axis. Note that this is a linear map of order 2, *i.e.* $T\circ T = \Id$. Note that reflections about lines *not* through the origin are not linear maps, since the zero vector must map to itself.
- **Rotations**: As before, the rotation must be about the zero vector.
- **Projections**: For example, the projection of $\R^2$ onto the $x$-axis. Note that projections are of the form $T\circ T = T$.
- **Inclusions**: For example, $$T\colon \R\to\R^2,\quad x\mapsto(x, mx).$$
- **Differentiation**: $$\mathscr{D}\colon P_n(\R) \to P_n(\R), \quad p \mapsto p'.$$
- **Integration**: $$\mathscr{I}\colon C(\R) \to \R, \quad f \mapsto \int_0^1 f(t)\:dt,$$
Another integration map, this time between polynomials is $$\mathscr{I}\colon P(\R) \to P(\R), \quad p(x) \mapsto \int_0^x p(t)\:dt.$$
- **Linear combination**: Given $\v_1, \dots, \v_n \in V$,
$$ T\colon \R^n \to V, \quad (\lambda_1, \dots, \lambda_n) \mapsto \lambda_1\v_1 + \dots + \lambda_n\v_n. $$
Note that if $\{\v_i\}$ is linearly independent, then this linear map is also injective.
Again, if $\{\v_i\}$ spans $V$, then this linear map is also surjective. Thus, $T$ is a linear bijection when we choose $\{\v_i\}$ as a basis of $V$.
- **Quotient**: Given a subspace $W\subseteq V$,
$$ Q\colon V \to V/W, \quad \v \mapsto [\v] = \v + W.$$
Note that $Q$ is surjective by construction.

Note that any linear combination of linear maps is also a linear map. This hints at the fact that the set of all linear maps between $V$ and $W$ forms a vector space of its own.

#### Rank and Nullity

Let $T\colon V\to W$ be a linear map. The *null space* or *kernel* of $T$ is the set of all vectors in $V$ which get mapped to $\ve0_W$. The *range* or *image* of $T$ is the set of all vectors in $W$ which have a pre-image in $V$.
$$ \begin{align}\kernel{T} &= \{\v \in V: T(\v) = \ve0_W\}, \\
\image{T} &= \{T(\v): \v \in V\}. \end{align} $$
It is easily verified that $\kernel{T}$ and $\image{T}$ are subspaces of $V$.

If $V$ is finite dimensional, we define the *nullity* of $T$ as the dimension of its kernel, and the *rank* of $T$ as the dimension of its image.

Note that if $T$ is injective, $\kernel{T} = \{\ve0_W\}$. Also, if $\kernel{T} = \{\ve0_W\}$, given any $\v_1, \v_2 \in V$ such that $T(\v_1) = T(\v_2)$, we have $T(\v_1 - \v_2) = \ve0$, which forces $\v_1 - \v_2 = \ve0_W$, *i.e.* $T$ is injective. Thus, a linear map is injective *iff* its nullity is zero.

#### Rank-Nullity Theorem
Let $V$ be a finite dimensional vector space and let $T\colon V\to W$ be a linear map. Then,
$$ \dim(V) = \dim(\image{T}) + \dim(\kernel{T}). $$
In other words, the dimension $n$ of $V$ is precisely the sum of the rank and nullity of $T$.

As a corollary, suppose that $\dim{V} = \dim{W}$, where $V$ is finite dimensional. Then, the linear map $T\colon V \to W$ is injective *iff* $T$ is surjective.

Consider a linear bijection $L\colon V \to V$. Note that the inverse map $L^{-1}$ is also linear. With this, we begin our proof of the theorem.

Let $k = \dim(\kernel{T}) \leq \dim(V) = n$, and let $\{\v_1, \dots \v_k\}$ be a basis of $\kernel{T}$. Using the Replacement Theorem, we extend this basis to of $V$, $\beta = \{\v_1, \dots, \v_k, \v_{k + 1}, \dots, \v_n\}$.
Note that $\{T(\v_{1}), \dots, T(\v_n)\}$ spans $\image{T}$. For $i = 1, \dots, k$, $T(\v_i) = \ve0$. Thus, we claim that $\{T(\v_{k + 1}), \dots, T(\v_n)\}$ is a basis of $\image{T}$. To do this, it is sufficient to show that this set is linearly independent. Suppose that for $\lambda_i \in F$,
$$ \lambda_{k + 1}T(\v_{k + 1}) + \dots + \lambda_nT(\v_n) = \ve0. $$
Using linearity, we see that $T(\sum {\lambda_i\v_i}) = \ve0$.
We thus find $\mu_1, \dots, \mu_k$ such that
$$ \lambda_{k + 1}\v_{k + 1} + \dots + \lambda_n\v_n = \ve0 = \mu_1\v_1 + \dots + \mu_k\v_k. $$
From the linear independence of $\beta$, we see that $\lambda_i = \mu_i = 0$, so the set $\{\v_{k + 1}, \dots, \v_{n}\}$ is linearly independent, hence a basis of $\image{T}$. Thus, $\dim(\image{T}) = n - k$, which proves the theorem.

#### Vector spaces of linear maps
We denote
$$ \L(V, W) = \{T\colon V \to W, T \text{ is a linear map}\}.$$
We also use the shorthand $\L(V, V) = \L(V)$.

It is easily seen that $\L(V, W)$ is a vector space, with the standard definitions of addition and scaling of functions. The zero map $\mathbb{O}$ is its additive identity.

> **Question**: What is the dimension of $\L(\R, \R^n)$?  
> *Response*: Consider $T \in \L(\R, \R^n)$. Since $\{1\}$ is a basis of $\R$, $T$ is fully determined by $T(1)$. Thus, we look at the evaluation map
$$ \ev\colon \L(\R, \R^n) \to \R^n, \quad T \mapsto T(1). $$
This is clearly a linear map. Furthermore, we note that $\ev$ is a bijection, since there is always a linear map $T$ such that $T(1) = \v$ given any $\v \in \R^n$, namely $T(\lambda) = \lambda\v$ for all $\lambda \in R$. Also, this is the only possible map because of the linearity of $T$. This means that $\ev^{-1}$ is also a linear bijection. Thus, we have an identification between elements of $\L(\R, \R^n)$ and $\R^n$, in the form of the *linear isomorphism* $\ev$. We will later see that this means $\dim\L(\R, \R^n) = n$.

#

> Note that we can similarly identify elements of $\L(\R^n, \R^m)$ with elements of $M_{m\times n}(\R)$.
 
### Linear isomorphisms
A *linear isomorphism* $T\colon V \to W$ is a linear map which is injective and surjective.

Two vector spaces are called *isomorphic* if there exists a linear isomorphism between them.

Note that isomorphism is an equivalence relation. Every vector space is isomorphic to itself (reflexivity), if $V$ is isomorphic to $W$, then $W$ is isomorphic to $V$ (symmetry), and if $V$, $W$ and $W$, $U$ are isomorphic, then so are $V$, $U$ (transitivity).
Thus, we can partition the set of all vector spaces over a field $F$ using this relation. We will see that vector spaces in the same equivalence class share the same dimension.

Let $T \in \L(V, W)$. If $\{\w_1, \dots, \w_n\} \subset W$ is linearly independent, and $T$ is surjective, with $T(\v_i) = \w_i$, then the set $\{\v_1, \dots, \v_n\}$ is also linearly independent.
If $\{\v_1, \dots, \v_n\} \subset V$ is linearly independent, and $T$ is injective with $T(\v_i) = \w_i$, then the set $\{\w_1, \dots, \w_n\}$ is linearly independent. Both follow immediately by applying the linearity of $T$ on the linear combinations $\sum \mu_i\v_i$ and $\sum \lambda_i\w_i$.

If $T$ is a linear isomorphism, and $\{\v_1, \dots, \v_n\}$ is a basis of $V$, then $\{\w_1, \dots, \w_n\}$ where $T(\v_i) = \w_i$ is a basis of $W$. The converse is also true, since $T^{-1}$ is also a linear isomorphism. Specifically, $\dim{V} = \dim{W}$, if either one has been shown to be finite dimensional.