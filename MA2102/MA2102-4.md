# Linear Algebra I

$
\newcommand\C{\mathbb{C}}
\newcommand\R{\mathbb{R}}
\newcommand\Q{\mathbb{Q}}
\newcommand\Z{\mathbb{Z}}
\newcommand\N{\mathbb{N}}
\newcommand\ve[1]{\boldsymbol{#1}}
\newcommand\norm[1]{\left\Vert #1 \right\Vert}
\newcommand\ip[2]{\left\langle #1,\, #2 \right\rangle}
\def\u{\ve{u}}
\def\v{\ve{v}}
\def\w{\ve{w}}
\def\e{\ve{e}}
\def\span{\operatorname{span}}
\def\Id{\operatorname{Id}}
\def\kernel{\operatorname{ker}}
\def\image{\operatorname{im}}
\def\L{\mathscr{L}}
\def\T{\mathscr{T}}
\def\ev{\operatorname{ev}}
\def\trace{\operatorname{trace}}
\def\diag{\operatorname{diag}}
\def\rank{\operatorname{rank}}
\def\adj{\operatorname{adj}}
$
## Eigenvectors and eigenvalues
Let $T\colon V \to V$ be a linear map. We note that if $[T]_\beta$ is a diagonal matrix for some ordered basis $\beta$ of $V$, the computation of quantities such as the trace and determinants of $T^k$ becomes simple.
If such a $\beta$ exists, then the linear map $T$ is called *diagonalizable*.

A matrix $A \in M_n(F)$ is called *diagonalizable* if $A$ is similar to a diagonal matrix.

Suppose $T$ is diagonalizable, and $[T]_\beta$ is diagonal with respect to the basis $\beta = \{\v_1, \dots, \v_n\}$. Then,
$$[T]_\beta = D(\lambda_1, \dots, \lambda_n)$$
for scalars $\lambda_i$. This means that $T(\v_j) = \lambda_j\v_j$. Such a basis $\beta$ is called an *eigenbasis*.

A non-zero vector $\v\in V$ is called an *eigenvector* if $T\v = \lambda\v$ for some scalar $\lambda$. The scalar $\lambda$ is called the *eigenvalue* of the eigenvector $\v$.

Note that all vectors in the null space of a linear map are eigenvectors, with eigenvalue zero.

> The only possible eigenvalues of a projection map are $0$ and $1$. This is because $P^2 = P$, so if $P\v = \lambda\v$ for $\v \neq 0$, then $\lambda^2\v = \lambda\v$.

The following conditions are equivalent: $\lambda$ is an eigenvalue of $T$, $T - \lambda I_V$ is not invertible, $\det(T - \lambda I_v) = 0$.

Let $T\colon V \to V$ be a linear map. If $lambda$ is an eigenvalue, its *eigenspace* $E_\lambda$ is defined as the set of all vectors having that eigenvalue.
$$ E_{\lambda_i} = \{\v\in V: T\v = \lambda_i\v\}. $$
If $T$ is diagonalizable with distinct eigenvalues $\lambda_1, \dots, \lambda_k$, then there exists a basis $\beta = \{\v_1, \dots, \v_n\}$ of eigenvectors, *i.e.* an eigenbasis. We can rearrange the $\v_j$ into $k$ groups such that the $i^\text{th}$ group of $r_i$ vectors have eigenvalue $\lambda_i$. Clearly, $r_1 + \dots + r_k = n$. Thus, the $i^\text{th}$ group of eigenvectors, say $\beta_i$, is a basis of the eigenspace $E_{\lambda_i}$. Thus, it follows that
$$ V = E_{\lambda_1} \oplus \dots \oplus E_{\lambda_k}. $$

> Note that for a projection map $P$, the eigenspaces $E_0$ and $E_1$ are precisely the kernel and image of $P$.

#### Characteristic polynomials
For a linear map $T\colon V \to V$, we consider the polynomial
$$ p_T(x) = \det(T - xI_V). $$
The roots of this polynomial are the eigenvalues of $T$.

The *Cayley-Hamilton theorem* states that a matrix satisfies its characteristic polynomial.

The unique monic polynomial of minimal degree which is satisfied by $A$ is called its *minimal polynomial*.

Similar matrices have the same characteristic and minimal polynomials.

#### Inner products
For vector spaces over $\R$ or $\C$, we may introduce an extra structure in order to eventually talk about lengths and angles.

An *inner product* is a vector space $V$ equipped with a map
$$ \ip{\cdot}{\cdot}\colon V\times V \to F, $$
which satisfies the following.

- **Linearity in the first variable**: $\ip{\v_1 + \v_2}{\w} = \ip{\v_1}{\w} + \ip{\v_2}{\w}$, and $\ip{\lambda\v}{\w} = \lambda\ip{\v}{\w}$.
- **Conjugate symmetry**: $\ip{\v}{\w} = \overline{\ip{\w}{\v}}$.
- **Positivity**: $\ip{\v}{\v} > 0$ if $\v \neq 0$.

> Note that even if $V$ is defined over the field $\C$, conjugate symmetry demands that $\ip{\v}{\v} = \overline{\ip{\v}{\v}}$, which means that the inner product $\ip{\v}{\v}$ must be real.

Some more properties are immediately evident.

- $\ip{\ve0}{\v} = 0$.
- $\ip{\v}{\w_1 + \w_2} = \ip{\v}{\w_1} + \ip{\v}{\w_2}$.
- $\ip{\v}{\lambda\w} = \overline{\lambda}\ip{\v}{\w}$.

The *length* of a vector $\v$ in an inner product space is defined as $\norm{\v} = \ip{\v}{\v}^{1 /2}$. Note that by positivity, the length of $\v$ is zero *iff* $\v = \ve0$.

> Note that if $\ip{\v_1}{\v} = \ip{\v_2}{\v}$ for all $\v\in V$, then $\v_1 = \v_2$. We show this by noting that linearity gives $\ip{\v_1 - \v_2}{\v} = 0$ for all $\v$. In particular, set $\v = \v_1 - \v_2$, which forces $\v_1 - \v_2 = \ve0$.

It can be shown that the length satisfies the parallelogram law,
$$ \norm{\v + \w}^2 + \norm{\v - \w}^2 = 2\norm{\v}^2 + 2\norm{\w}^2. $$

The *Cauchy-Schwarz* inequality states that
$$ \norm{\ip{\v}{\w}} \leq \norm{\v}\cdot\norm{\w}. $$

This leads to the *triangle inequality*
$$ \norm{\v + \w} \leq \norm{\v} + \norm{\w}. $$

The standard inner product on $\C^n$ is defined as
$$ \ip{\v}{\w} = \sum_i v_i\overline{w_i}. $$

The inner product on complex functions over $[0, 2\pi]$ may be defined as
$$ \ip{f}{g} = \frac{1}{2\pi}\int_0^{2\pi} f(x)\overline{g(x)}\:dx.$$

The inner product on matrices over $\C$ may be defined as
$$ \ip{A}{B} = \trace(B^*A). $$
Note that $B^*$ is the conjugate transpose of $B$.