\documentclass[10pt]{article}

\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{bm}
\def\x{\bm{x}}
\def\y{\bm{y}}
\def\v{\bm{v}}
\def\w{\bm{w}}
\def\p{\bm{p}}
\def\q{\bm{q}}
\def\rank{\operatorname{rank}}
\def\dim{\operatorname{dim}}
\newcommand\ip[2]{\langle #1,\, #2 \rangle}

\newtheorem{lemma}{Lemma}

\title{MA2102 - Solution II}
\author{Satvik Saha}
\date{}

\geometry{a4paper, margin=1in}
\setlength\parindent{0pt}

\begin{document}
        \par\textbf{IISER Kolkata} \hfill \textbf{Problem II}
        \vspace{3pt}
        \hrule
        \vspace{3pt}
        \begin{center}
                \LARGE{\textbf{MA2102 : Linear Algebra I}}
        \end{center}
        \vspace{3pt}
        \hrule
        \vspace{3pt}
        Satvik Saha, \texttt{19MS154}\hfill\today
        \vspace{20pt}

        A square, $n \times n$ real matrix $A$ is such that $A A^\top$ is diagonal, with each diagonal entry non-zero.
        Show that the rows of $A$ are orthogonal.
        Is it true that the columns are orthogonal? 
        
        \paragraph{Solution}
        Let $A \in M_n(\mathbb{R})$ be enumerated as 
        \[
                A = \begin{bmatrix}
                        a_{11} & a_{12} & \cdots & a_{1n} \\
                        a_{21} & a_{22} & \cdots & a_{2n} \\
                        \vdots & \vdots & \ddots & \vdots \\
                        a_{n1} & a_{n2} & \cdots & a_{nn}
                \end{bmatrix}
                = \begin{bmatrix}
                        \v_1 \\
                        \v_2 \\
                        \vdots \\
                        \v_n 
                \end{bmatrix}
                = \begin{bmatrix}
                        \w_1 & \w_2 & \cdots & \w_n
                \end{bmatrix}.
        \]
        Here, the rows of $A$ are the row vectors $\v_i \in \mathbb{R}^n$ and the columns of $A$ are the column vectors $\w_i \in \mathbb{R}^n$. \\

        Recall that the standard inner product for row vectors in $\mathbb{R}^n$ is defined as
        \[
                \ip{\cdot}{\cdot}\colon \mathbb{R}^n\times \mathbb{R}^n \to \mathbb{R}, \qquad \ip{\x}{\y} = \x\y^\top.
        \]
        In other words,
        \[
                \ip{\x}{\y} \,=\, \sum_{i = 1}^n x_i y_i.
        \]
        We say that the row vectors $\x, \y \in \mathbb{R}^n$ are orthogonal if $\ip{\x}{\y} = 0$.

        \begin{lemma}
                For a row vector $\x \in \mathbb{R}^n$, $\ip{\x}{\x} = 0$ if and only if $\x = \mathbf{0}$.
                \begin{proof}
                        Note that the components $x_i \in \mathbb{R}$, so we can write
                        \[
                                \ip\x\x = \sum_{n = 1}^n x_i x_i = x_1^2 + x_2^2 + \dots + x_n^2 \geq x_j^2,
                        \]
                        for all $j = 1, 2, \dots, n$. If $\ip\x\x = 0$, then we have $0 \geq x_j^2$, which forces $x_j = 0$.
                        Hence, $\x = \mathbf{0}$. \\

                        Conversely, if $\x = \mathbf{0}$, we clearly see that $\ip\x\x = \sum_i x_i^2 = 0$.
                \end{proof}
        \end{lemma}

        To relate the elements of the matrix product with the inner products of the rows and columns, we make use of the following lemma.
        \begin{lemma}
                Let $P \in M_{m \times n}(\mathbb{R})$ and $Q \in M_{n \times l}(\mathbb{R})$.
                Then, the $i, j^\text{th}$ element of the product $PQ$ is given by
                \[
                        [PQ]_{ij} \,=\, \ip{\p_i}{\q_j^\top},
                \]
                where $\p_i \in \mathbb{R}^n$ is the $i^\text{th}$ row of $P$, and $\q_j \in \mathbb{R}^n$ is the $j^\text{th}$
                column of $Q$.
                \begin{proof}
                        This is purely computational, from the definition of matrix multiplication.
                        \[
                                [PQ]_{ij} \,=\, \sum_{k = 1}^n p_{ik}q_{kj} \,=\, \p_i\q_j \,=\, \p_i (\q_j^\top)^\top \,=\, \ip{\p_i}{\q_j^\top}. \qedhere
                        \]
                \end{proof}
        \end{lemma}

        With this, we prove the desired statement. Let $A A^\top$ be diagonal, where for non-zero scalars $\lambda_i \in \mathbb{R}$, we have
        \[
                [A A^\top]_{ij} = \lambda_i \delta_{ij}.
        \]
        This means that the $i, j^\text{th}$ element of the product $A A^\top$ is zero precisely when $i \neq j$.
        However, note that the $j^\text{th}$ column of $A^\top$ is the $j^\text{th}$ row of $A$. Thus, we have
        \[
                [A A^\top]_{ij} = \v_i\v_j^\top = \ip{\v_i}{\v_j}.
        \]
        This gives $\ip{\v_i}{\v_j} = \lambda_i\delta_{ij}$, i.e.\ $\ip{\v_i}{\v_j} = 0$ if and only if $i \neq j$.
        In other words, $\v_i$ and $\v_j$ are orthogonal when $i \neq j$ and each $\v_i$ is non-zero.
        Thus, the rows of $A$ are orthogonal. \\

        The columns of $A$ satisfying $A A^\top$ need not be orthogonal. We supply the following counterexample.
        \[
                A = \begin{bmatrix}
                        1 & -1 \\ 2 & 2
                \end{bmatrix}.
        \]
        Note that the rows are orthogonal because 
        \[
                A A^\top \,=\, \begin{bmatrix}
                        1 & -1 \\ 2 & 2
                \end{bmatrix}
                \begin{bmatrix}
                        1 & 2 \\ -1 & 2
                \end{bmatrix}
                \,=\, \begin{bmatrix}
                        2 & 0 \\ 0 & 8
                \end{bmatrix}
        \]
        However, the inner product of the two columns is
        \[
                \begin{bmatrix}
                        1 \\ 2
                \end{bmatrix}^\top
                \begin{bmatrix}
                        -1 \\ 2
                \end{bmatrix}
                \,=\, 1(-1) + 2(2) = 3 \neq 0.
        \]
        The standard inner product for two column vectors $\x, \y \in \mathbb{R}^n$ has been defined as $\x^\top\y = \sum_i x_iy_i$.

\end{document}
