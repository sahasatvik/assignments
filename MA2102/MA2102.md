# Linear Algebra I

$
\newcommand\C{\mathbb{C}}
\newcommand\R{\mathbb{R}}
\newcommand\Q{\mathbb{Q}}
\newcommand\Z{\mathbb{Z}}
\newcommand\N{\mathbb{N}}
\newcommand\ve[1]{\boldsymbol{#1}}
\newcommand\norm[1]{\left\Vert #1 \right\Vert}
\def\u{\ve{u}}
\def\v{\ve{v}}
\def\w{\ve{w}}
\def\span{\operatorname{span}}
$


## Fields
In brief, a field $F$ is a set with two binary operations $+$ and $\times$ such that

- $+$ and $\times$ are commutative.
- $+$ has an additive identity and inverses.
- $\times$ has a multiplicative identity and inverses for non-zero elements.
- $\times$ distributes over $+$.

We also assume that the additive and multiplicative identities are distinct by convention.

Some examples of fields are $\Q$, $\R$, $\C$, $\Q[\sqrt{2}] = \{p + q\sqrt{2} : p, q \in \Q\}$.

> **Question**: Does $\{0, 1\}$ form a field?  
> *Response*: The set $\{0, 1\}$ forms a field when $+$ is defined as the $\operatorname{XOR}$ operation and $\times$ is defined as the $\operatorname{AND}$ operation. This is a finite field, called the Galois field of two elements $GF(2)$ or $\Z_2$.

Note that any subfield $F \subset \R$ must contain $\Q$. To see this, note that $0, 1 \in F$. Thus, $F$ contains all positive integers by the closure of addition. The existence of additive inverses means that $F$ also contains the negative integers. Now, every non-zero integer must have a multiplicative inverse, so all rationals of the form $1/q$ are contained in $F$. Scaling these by integers $q \in F$ shows that $F$ contains all numbers of the form $p/q$ for integers $p, q$ where $q \neq 0$. Thus, $\Q \subseteq F$.


## Vector spaces

A *vector space* $V$ over a field $F$ is a set with a binary operation $+\colon V\times V \to V$ and an operation $\cdot\,\colon F\times V \to V$, satisfying the following axioms.

- $\u + \v \in V$ for all $\u, \v \in V$.
- $a\u \in V$ for all $a \in F$, $\u \in V$.
- $\u + \v = \v + \u$ for all $\u, \v \in V$.
- $(\u + \v) + \w = \u + (\v + \w)$ for all $\u, \v, \w \in V$
- There exists $\ve{0} \in V$ such that $\ve{0} + \u = \u = \u + \ve{0}$ for all $\u \in V$.
- For all $\v \in V$, there exists $\u \in V$ such that $\u + \v = \ve{0}$. We denote $\v = - \u$.
- $c(\u + \v) = c\u + c\v$ for all $c \in F$, $\u, \v \in V$.
- $(ab)\u = a (b\u)$ for all $a,b \in F$, $\u \in V$.
- $(a + b)\u = a\u + b\u$ for all $a,b \in F$, $\u \in V$.
- There exists $1 \in F$ such that $1\u = \u$ for all $\u \in V$.

A *vector subspace* $W$ is a subset of a vector space $V$ obeys the following axioms.

- $\u + \v \in W$ for all $\u, \v \in W$.
- $a\u \in W$ for all $a \in F$, $\u \in W$.
- $\ve{0} \in W$, where $\ve{0}$ is the zero element of $V$.

Then, $W$ is itself a vector space, called a subspace of $V$.

Some examples of vector spaces are $\{\ve{0}\}$ over $\R$ (the zero vector space), $\R^n$ over $\R$.

### Matrices as vectors
Consider the set of matrices with real valued entries, $M_{m\times n}(\R)$. With addition defined entry-wise with $(0)_{ij}$ as the identity, scalar multiplication defined as $(cA)_{ij} = c(A)_{ij}$, this forms a vector space over $\R$.

The vector space of square matrices $M_n(\R)$ has several interesting subspaces.

- The set of symmetric matrices, $\operatorname{sym}_n = \{A \in M_n(\R): A = A^T\}$. Note that there are $1 + 2 + \dots + n$ 'free entries', so this space looks very similar to $\R^{1 + 2 + \dots + n}$.
- The set of traceless matrices, $W_n = \{A \in M_n(\R): \operatorname{trace}{A} = 0\}$. Again, this subspace looks like $\R^{n^2-1}$.
- The set of diagonal matrices, $D_n =\{A \in M_n{\R}: A_{ij} = 0 \text{ if }i\neq j\}$. Again, this subspace looks like $\R^n$.
- The set of scalar matrices, $S_n = \{\lambda I_n \in M_n(\R): \lambda \in \R \}$. Again, this subspace looks like $\R$. This also happens to be a field.

Note that $S_n\ \subset D_n \subset \operatorname{sym}_n$.

### Functions as vectors
Let $S$ be a set and define
$$ \mathscr{F}(S, \R) = \{f\colon S \to \R\}. $$
We define our operations as follows.

- $(f + g)(s) = f(s) + g(s)$ for all $f, g \in \mathscr{F}(S, \R)$.
- $\ve{0}(s) = 0$ is the additive identity.
- $(cf)(s) = c f(s)$ for all $f \in \mathscr{F}(S, \R)$.

Then $\mathscr{F}(S, \R)$ is a vector space over $\R$.

Note that when $S = \{1, 2, \dots, n\}$, this vector space seems very similar to $\R^n$.

> **Question**: What structure is $\mathscr{F}(\N, \R)$ similar to?  
> *Response*: When $S = \N$, this vector space is very similar to the set of all sequences of real numbers.

### Polynomials as vectors
Let $P_n(\R)$ be the set of all polynomials with real coefficients and degree at most $n$, where $n \in \Z_{\ge0}$. We note that there is a bijection between such polynomials and the tuples of their coefficients, which are elements of $\R^{n+1}$. Thus, we can show that $P_n(\R)$ is a vector space over $\R$.

Note that $P_n(\R) \subset P_{n+1}(\R)$. The union of all such sets, *i.e.* the set of all real polynomials $P(\R) = \bigcup_{n\geq 0} P_n(\R)$ is also a vector space.

> **Question**: Does $P(\R)$ have any other structure?  
> *Response*: By defining multiplication in the usual way, we see that $P(\R)$ forms a *ring*.

It is easily verified that the set of even polynomials forms a subspace of $P(\R)$. Another subspace is the set of truncated polynomials, which for fixed $k \in \N$ have the coefficients of $1, x, \dots, x^k$ all set to $0$.

> **Question**: Does the set of polynomials of degree *exactly* equal to some $k \in \N$, together with the zero polynomial, form a vector space?  
> *Response*: No, since addition is not closed in this structure. The degree of the sum of two polynomials certainly cannot increase, but may decrease. A simple example is $(x + 1) - (x) = 1$.

### Vector subspaces
We may ask what are the vector subspaces of $\R^2$, apart from the trivial zero vector space and $\R^2$ itself. We see that all lines through the origin, *i.e.* $L_m = \{(x, y) \in \R^2: y = mx\}$, are vector subspaces of $\R^2$. Furthermore, we can show that these are the *only* subspaces of $\R$.

To prove this, note that subspace of $\R^2$ must contain the zero element of $\R^2$, which is $(0, 0)$. Let our subspace $L$ include this point, and let $L$ contain at least one more distinct point $\v \in \R^2$. By closure of addition and scaling, all points of the form $\lambda\v$ are contained in $L$, for all $\lambda \in \R$. So far, this is simply $L_m$, and it can be verified that this is indeed a subspace. Now, let a non-zero point not on this line be in $L$, say $\w \in L$ such that $\w \notin \{\lambda\v: \lambda \in \R\}$. Thus, there is no $\lambda \in \R$ such that $\w = \lambda \v$.
Again, by closure, $L$ contains all points of the form $a\w + b\v$, for all $a,b \in \R$. Let $\u \in \R^2$ be arbitrary. Setting $\Delta = w_xv_y - w_yv_x$, note that $\Delta \neq 0$, since if it were zero, then $\w = \lambda \v $ for $\lambda = w_x/v_x = w_y/v_y$. (A small detail here is that if $v_x = 0$, this forces $w_x = 0$ and $v_y, w_y \neq 0$ as well, since $\v, \w \neq \ve0$. Thus, $\lambda$ is well-defined. There is an analogous case with $v_y = 0 \implies v_x, w_x \neq 0$.)
It is easily verified that $$\u = (u_xv_y - u_yv_x)\w/\Delta - (u_xw_y - u_yw_x)\v/\Delta .$$ 
Thus, $\u \in L$ for arbitrary $\u \in \R^2$, so $L$ is the entirety of $\R^2$. This means that the only subspaces of $\R^2$ are $\{\ve0\}$, $L_m$ and $\R^2$.

> The subspaces $L_m \subset \R^2$ are called *lines*. On a 2D plane, these clearly represent straight lines of slope $m$.
> Note that the line $L = \{(x, y) \in \R^2: x = 0\}$ is also a subspace of $\R^2$, which is awkwardly expressed in our $L_m$ notation. The notation $L_\v = \{\lambda\v: \lambda\in\R\}$ for some $\v \in \R^2$ is preferable.
> Later, we will see that for $\v,\w \in \R^3$, the *planes* $M_{\v,\w} = \{\alpha\v + \beta\w: \alpha, \beta \in \R\}$ are all subspaces of $\R^3$.
> This naturally leads to the idea that linear combinations of vectors form subspaces. The set of linear combinations of some vectors is called their *span*.

### Linear combinations of vectors
Let $V$ be a vector space over the field $F$.
We define the *span* of a set of vectors $W = \{\w_1, \w_2, \dots\} \subseteq V$ as follows.
$$ \span{W} = \{\lambda_1\w_1 + \lambda_2\w_2 + \dots : \w_i \in V, \lambda_i \in F\}. $$

A set of vectors $\{\w_i\}$ is said to be *linearly independent* if there do not exist scalars $0 \neq \lambda_i \in F$ such that $ \lambda_1\w_1 + \lambda_2\w_2 + \dots = \ve0$.

It is easily verified that for any set of vectors $W\subseteq V$, $\span{W}$ forms a subspace of $V$.

Some other properties of the span are

- $\span\emptyset = \{\ve{0}\}$.
- $\span(\span S) = \span{S}$. Note that $\span W = W$ iff $W$ is a subspace.
- $\span W_1 \subseteq \span W_2$ if $W_1 \subseteq W_2$.

We say that a vector space $V$ is *generated* by $W \subseteq V$ if
$V = \span{W}$.