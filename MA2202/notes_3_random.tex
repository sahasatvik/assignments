\documentclass[11pt]{article}

\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{amsmath, amssymb, amsthm}
\usepackage[%
    hidealllines=true,%
    innerbottommargin=15,%
    nobreak=true,%
]{mdframed}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{lipsum}

\geometry{a4paper, margin=1in, headheight=14pt}

\pagestyle{fancy}
\fancyhf{}
\renewcommand\headrulewidth{0.4pt}
\fancyhead[L]{\scshape MA2201: Probability I}
\fancyhead[R]{\scshape Random variables}
\rfoot{\footnotesize\it Updated on \today}
\cfoot{\thepage}

\def\C{\mathbb{C}}
\def\R{\mathbb{R}}
\def\Q{\mathbb{Q}}
\def\Z{\mathbb{Z}}
\def\N{\mathbb{N}}
\newcommand\ddx[1]{\frac{d #1}{d x}}
\newcommand\ddt[1]{\frac{d #1}{d t}}
\newcommand\dd[3][]{\frac{d^{#1}{#2}}{d {#3}^{#1}}}
\newcommand\ppx[1]{\frac{\partial #1}{\partial x}}
\newcommand\ppt[1]{\frac{\partial #1}{\partial t}}
\newcommand\pp[3][]{\frac{\partial^{#1}{#2}}{\partial {#3}^{#1}}}
\newcommand\norm[1]{\left\lVert#1\right\rVert}
\newcommand\E[1]{E\left[#1\right]}
\newcommand\var[1]{\operatorname{Var}[#1]}
\newcommand\cov[1]{\operatorname{Cov}[#1]}

\newcounter{module}
\setcounter{module}{3}

\newmdtheoremenv[%
    backgroundcolor=blue!10!white,%
]{theorem}{Theorem}[module]
\newmdtheoremenv[%
    backgroundcolor=violet!10!white,%
]{corollary}{Corollary}[theorem]
\newmdtheoremenv[%
    backgroundcolor=teal!10!white,%
]{lemma}[theorem]{Lemma}

\theoremstyle{definition}
\newmdtheoremenv[%
    backgroundcolor=green!10!white,%
]{definition}{Definition}[module]
\newmdtheoremenv[%
    backgroundcolor=red!10!white,%
]{exercise}{Exercise}[module]

\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{example}{Example}
\newtheorem*{solution}{Solution}

\surroundwithmdframed[%
    linecolor=black!20!white,%
    hidealllines=false,%
    innertopmargin=5,%
    innerbottommargin=10,%
    skipabove=0,%
    skipbelow=0,%
]{example}

\numberwithin{equation}{module}

\title{
    \Large\textsc{MA2202: Probability I} \\
    % \vspace{10pt}
    \Huge \textbf{Random variables} \\
    \vspace{5pt}
    \Large{Spring 2021}
}
\author{
    \large Satvik Saha%
    % \thanks{Email: \tt ss19ms154@iiserkol.ac.in}
    \\\textsc{\small 19MS154}
}
\date{\normalsize
    \textit{Indian Institute of Science Education and Research, Kolkata, \\
    Mohanpur, West Bengal, 741246, India.} \\
    % \vspace{10pt}
    % \today
}

\begin{document}
    \maketitle

    \begin{definition}[Random variable]
        Given a probability space $(\Omega, \mathcal{E}, P)$, a function $X\colon
        \Omega \to \R$ is called a random variable if $X^{-1}(r, \infty)
        \in \mathcal{E}$ for all $r \in \R$.

        \begin{remark}
            For some $S \subseteq \R$, we denote \[
                P(X \in S) = P(\{\omega \in \Omega\colon X(\omega) \in S\}).
            \] 
        \end{remark}
    \end{definition}
    
    \begin{definition}[Discrete random variable]
        A random variable which can assume only a countably infinite number of
        values is called a discrete random variable.
    \end{definition}
    \begin{example}
        Let $X\colon \Omega \to \R$ denote the number of heads obtained when a fair
        coin is tossed thrice. Note that $\Omega = \{0, 1, 2, 3\}$. Thus, 
        $P(X = 0) = P(X = 4) = 1 /8$ and $P(X = 1) = P(X = 2) = 3 /8$.
    \end{example}

    \begin{definition}[Probability distribution]
        The probability distribution of a random variable $X$ is the set of pairs
        $(X(A), P(A))$ for all $A \in \mathcal{E}$.
    \end{definition}

    \begin{definition}[Probability mass function]
        Let $X$ be a discrete random variable. The probability mass function of $X$
        is the function $p_X\colon \R \to [0, 1]$, \[
            p_X(\alpha) = P(X = \alpha).
        \]

        \begin{remark}
            Since $X$ is a discrete random variable, the set $S = \{x \in \R \colon
            p_X(\alpha) > 0\}$ is countable, and \[
                \sum_{x \in S} p_X(x) = 1.
            \] 
        \end{remark}
    \end{definition}
    
    \begin{definition}[Expectation]
        The expectation of $g(X)$, for $g\colon \R \to \R$ and a discrete random
        variable $X$ is defined as \[
            \E{g(X)} = \sum_{x \in S} g(x)\,p_X(x),
        \] if the series converges absolutely.
    \end{definition}
    \begin{example}
        The $n$\textsuperscript{th} moment of a discrete random variable $\E{X^n}$
        is defined as \[
            \E{X^n} = \sum_{x \in S} x^n\,p_X(x),
        \] if the series converges absolutely.

        The first moment $\mu = \E{X}$ is called the mean.
        The second moment $\sigma^2 = \E{(X - \mu)^2}$ is called the variance.
        Note that \[
            \sigma^2 = \sum (x - \mu)^2\, p(x) = \sum x^2\,p(x) - 2\mu x\,p(x) +
            \mu^2 p(x).
        \] Simplifying, \[
            \sigma^2 = \E{X^2} - \E{X}^2.
        \] 
    \end{example}

    \begin{definition}[Cumulative distribution function]
        The cumulative distribution function of a random variable $X$ is
        defined as the function $F_X\colon \R \to [0, 1]$, \[
            F_X(\alpha) = P(X \leq \alpha).
        \] 
    \end{definition}

    \begin{definition}[Continuous random variable]
        A continuous random variable $X$ is such that its cumulative distribution
        function $F_X$ is continuous.
    \end{definition}

    \begin{definition}[Probability density function]
        Let $X$ be a continuous random variable with a cumulative distribution
        function $F_X$. If we write \[
            F_X(\alpha) = \int_{-\infty}^\alpha f_X(x)\:dx
        \] for all $\alpha \in \R$, then $f_X$ is a probability density function.
        If $f_X$ is continuous, then the Fundamental Theorem of Calculus guarantees
        that $f_X(x) = F_X'(x)$.
        \begin{remark}
            Note that we can write \[
                P(\alpha \leq X \leq \beta) = \int_\alpha^\beta f_X(x)\:dx.
            \] We also demand \[
                \int_{-\infty}^{+\infty} f_X(x) \:dx.
            \] 
        \end{remark}
    \end{definition}
    \begin{example}
        The uniform distribution on an interval $[a, b] \subset \R$ is defined using
        the probability density function \[
            f_X(x) = \begin{cases}
                \frac{1}{b - a}, &\text{ if }a \leq x \leq b \\
                0, &\text{ otherwise }
            \end{cases}
        \]
    \end{example}

    \begin{definition}[Expectation]
        The expectation of $g(X)$, for $g\colon \R \to \R$ and a continuous random
        variable $X$ is defined as \[
            \E{g(X)} = \int_{-\infty}^{+\infty} g(X)\, f_X(x)\:dx
        \] if the integral converges absolutely.
    \end{definition}


    \begin{definition}[Mixed random variable]
        A random variable whose cumulative distribution function $F_X$ is
        discontinuous at countably many points, with $F_X$ being continuous and
        strictly increasing in at least one interval is called a mixed random
        variable.
    \end{definition}

    \begin{definition}[Conditional probability distributions]
        Let $X$ and $Y$ be two random variables, and let $A, B \subseteq \R$.
        If $P(Y \in B) > 0$, then \[
            P(X \in A \,|\, Y\in B)\, P(Y \in B) = P(X \in A, Y \in B).
        \] 
    \end{definition}
    \begin{definition}[Independent random variables]
        We say that $X$ and $Y$ are independent if \[
            P(X \in A, Y \in B) = P(X \in A)\, P(Y \in B)
        \] for all $A, B \subseteq \R$.
    \end{definition}

    \begin{example}
        Consider an experiment where a fair die is rolled until a 6 is obtained, and
        let $X$ be the random variable denoting the number of throws.  Also, let $Y$
        be a random variable which is $1$ if the first even outcome is a 6, and 0
        otherwise. Observe that $P(X = 1) = 1 / 6$ and $P(Y = 1) = 1 / 3$. Also,
        $P(X = 1, Y = 1) = 1 / 6 \neq P(X = 1)\, P(Y = 1)$, hence $X$ and $Y$ are
        not independent random variables.

        It can be shown that \[
            P(X = n \,|\, Y = 1) = \frac{1}{2^n}.
        \] 
    \end{example}

    \section*{Bernoulli distribution}
    Consider an experiment with one trial, which has two possible outcomes; the
    probability of a success is given by $p$ and the probability of a failure is $q
    = 1 - p$. The discrete random variable such that $X = 1$ on success and $X = 0$
    on failure is said to follow the Bernoulli distribution. Note that the
    expectation value is simply \[
        \E{X} = p.
    \] 

    \section*{Binomial distribution}
    Consider an experiment with $n$ Bernoulli trials. We could let $X_i$ be a
    random variable denoting the outcome of the $i$\textsuperscript{th} trial, so
    we demand that $\{X_i\}$ are independent and identically distributed.
    The distribution of the sum $X = X_1 + \dots + X_n$ follows the Binomial
    distribution $B(n, p)$, where \[
        P(X = k) = \binom{n}{k}p^kq^{n-k}.
    \] The expectation value is given by \[
        \E{X} = \sum_{k = 0}^n k\binom{n}{k}p^kq^{n - k} = np.
    \] This is more easily seen from the linearity of expectation, \[
        \E{X} = \sum_{i = 1}^n \E{X_i} = np.
    \] 

    \section*{Geometric distribution}
    Consider an experiment where Bernoulli trials are repeated until success. The
    random variable denoting the number of trials required is said to follow the
    geometric distribution, where the probability that $n$ trials were required is
    given by \[
        P(X = n) = pq^{n - 1}.
    \] 

    \section*{Pascal distribution}
    Consider an experiment where Bernoulli trials are repeated until $k$ successes.
    The random variable denoting the number of trials required follows the Pascal,
    or negative binomial distribution. The probability that $n \geq k$ trials were
    required is given by \[
        P(X = n) = \binom{n - 1}{k - 1}p^k q^{n - k}.
    \] Note that we choose $k - 1$ successes from $n - 1$ trials, since the last
    trial is by definition a success.

    \begin{remark}
        This can be derived by noting that if $X_i \sim
        \operatorname{Geometric}(p)$, then the random variable $X \sim
        \operatorname{Pascal}(k, p)$ is simply \[
            X = X_1 + X_2 + \dots + X_k,
        \] so for success in $n$ trials, we find $P(X = n)$. This means that we need
        to sum the probabilities for all possible $X_i = n_i$, where $n = \sum n_i$.
        There are $\binom{n - 1}{k - 1}$ such solutions, and each of these solutions
        must occur with probability $p^k q^{n - k}$, denoting $k$ successes and the
        remaining $n - k$ failures.
    \end{remark}

    \section*{Hypergeometric distribution}
    Consider an experiment where we choose $n$ balls randomly from a population of
    $N$ distinct balls, of which $m$ are red. The random variable denoting the
    number of red balls follows the hypergeometric distribution.  The probability of
    obtaining $k \leq m$ red balls is given by \[
        P(X = k) = \frac{\binom{m}{k}\binom{N - m}{n - k}}{\binom{N}{n}}.
    \] 

    \section*{Poisson distribution}
    The probability mass function of the Poisson distribution is given by \[
        P(X = k) = \frac{\lambda^k}{k!}e^{-\lambda},
    \] where $k \in \N$. This is generally used to model the number of times an
    event occurs in a given interval. This is a limiting form of the binomial
    distribution. Note that the expectation value is given by \[
        \E{X} = \sum_{k = 0}^\infty k\cdot \frac{\lambda^k}{k!}e^{-\lambda} =
        \lambda.
    \] In addition, the variance is also given by $\sigma^2 = \lambda$.

    \section*{Exponential distribution}
    The probability density function of the exponential distribution is given by \[
        f_X(x) = \lambda e^{-\lambda x}, 
    \] where $x \geq 0$ and $0$ elsewhere.
    This is generally used to model the waiting time between successive events.
    Note that the expectation value is given by \[
        \E{X} = \int_0^\infty \lambda x e^{-\lambda x}\:dx = \frac{1}{\lambda},
    \] In addition, the variance is given by $\sigma^2 = 1 /\lambda^2$.

    \begin{definition}[Memorylessness]
        The probability distribution of a non-negative discrete random variable $X$
        is called memoryless if for all $m, n \in \Z_{\geq 0}$, \[
            P(X > m + n\,|\, X > n) = P(X > m).
        \] 
        \begin{remark}
            This can be written as \[
                P(X > m + n) = P(X > m) \, P(X > n).
            \] 
        \end{remark}
        \noindent Again, a continuous random variable $Y$ is called memoryless if
        for all $s, t \geq 0$, \[
            P(X > s + t\,|\, X > t) = P(X > s).
        \] 
    \end{definition}
    \begin{example}
        Suppose that $X\sim\operatorname{Geometric}(p)$. Then, \[
            P(X > n) = \sum_{k = n + 1}^\infty pq^{k - 1} = q^n,
        \] where $q = 1 - p$. Also, \[
            P(X > m + n, X > n) = P(X > m + n) = q^{m + n},
        \] which gives \[
            P(X > m + n, X > n) = q^m\cdot q^n = P(X > m)\, P(X > n).
        \] 
    \end{example}
    \begin{example}
        Suppose that $Y\sim\operatorname{Exponential}(\lambda)$. Then, \[
            P(Y > s) = \int_s^\infty \lambda e^{-\lambda t}\:dt = e^{-\lambda s}.
        \] Thus, \[
            P(Y > s + t) = P(Y > s)\, P(Y > t).
        \] 
    \end{example}
    
    \section*{Normal distribution}
    The probability density function of the normal distribution is given by \[
        f_X(x) = \frac{1}{\sigma\sqrt{2\pi}}e^{-(x - \mu)^2/2\sigma^2}.
    \] 

    \begin{exercise}
        Consider a continuous random variable with probability density function \[
            f(x) = c e^{-x^2 /2}.
        \] Determine the constant $c$.
        \begin{solution}
            Define \[
                I = \int_{-\infty}^{+\infty} e^{-x^2 /2} \:dx.
            \] By using two dummy variables $x$ and $y$, we can write \[
                I^2 = \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} e^{-(x^2 +
                y^2) / 2}\:dx\:dy.
            \] We can shift to polar coordinates by using the transformations \[
                x = r\cos\theta, \qquad y = r\sin\theta, \qquad dx\:dy =
                r\:d\theta\:dr.
            \] Thus, \[
                I^2 = \int_{0}^\infty \int_{0}^{2\pi} re^{-r^2 / 2}\:d\theta\:dr =
                \int_0^{2\pi} d\theta \int_0^\infty re^{-r^2 / 2}\:dr.
            \] Now, the second integral can be computed as \[
                \int_0^\infty re^{-r^2 / 2}\:dr = \int_0^\infty e^{-r^2 / 2} \:d(r^2
                / 2) = -e^{-r^2 / 2}\,\Big|_0^\infty = 1.
            \] Thus, we have \[
                I = \int_{-\infty}^{+\infty} e^{-x^2 / 2} \:dx = \sqrt{2\pi}.
            \] This is called a Gaussian integral. We must thus have $c = 1
            /\sqrt{2\pi}$.
        \end{solution}
    \end{exercise}

    \begin{exercise}
        Find the moments of the standard normal distribution.
        \begin{solution}
            Note that $\E{X^{2n - 1}} = 0$, because the integral \[
                \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{+\infty} x^{2n - 1}e^{-x^2 /
                2}\:dx = 0.
            \] The function we are integrating over is an odd function. Now, we
            consider the second moment. Compute \[
                \int_{-\infty}^{+\infty} x^2 e^{-x^2 / 2}\:dx = 2 \int_0^\infty x^2
                e^{-x^2 / 2}\:dx = -2xe^{-x^2 / 2}\Big|_0^\infty + 2\int_0^\infty
                e^{-x^2 / 2}\:dx = \sqrt{2\pi}.
            \] Thus, $\E{X^2} = 1$, which means that the variance is also $1$. For
            higher moments, note that \[
                \int x^{2n}e^{-x^2 / 2} = -x^{2n - 1}e^{-x^2 / 2} + (2n - 1)
                \int x^{2n - 2} e^{-x^2 / 2}\:dx,
            \] which gives the recurrence \[
                \E{X^{2n}} = (2n - 1)\, \E{X^{2n - 2}} = (2n - 1)!!.
            \] 
        \end{solution}
    \end{exercise}
    Suppose that $Z \sim \operatorname{Normal}(\mu, \sigma^2)$, note that $(Z - \mu)
    / \sigma$ is a standard normal variable. The cumulative distribution function of
    a standard normal variable is \[
        \Phi(x) = \frac{1}{\sqrt{2\pi}} \int_{\infty}^x e^{-t^2 / 2}\:dt.
    \] Thus, \[
        P(Z \leq z) = P\left(\frac{Z - \mu}{\sigma} \leq \frac{z -
        \mu}{\sigma}\right) = \Phi\left(\frac{z - \mu}{\sigma}\right).
    \] Thus, we see that \[
        \E{(Z - \mu)^n} = \begin{cases}
            0, &\text{ if } n \text{ is odd }\\
            \sigma^n(n - 1)!!, &\text{ if } n \text{ is even }
        \end{cases}.
    \] 

    \begin{theorem}[Markov's inequality]
        Let $X$ be a non-negative random variable. Then for all $a > 0$, \[
            P(X \geq a) \leq \frac{\E{X}}{a}.
        \] 
    \end{theorem}
    \begin{proof}
        Let $u_a\colon \R_{\geq 0} \to \{0, 1\}$ be a step function where
        $u_a(x) = 1$ when $x \geq a$ and $0$ otherwise. Note that \[
            u_a(x) \leq \frac{x}{a}.
        \] Then, \[
            P(X \geq a) = \E{u_a(X)} \leq \E{\frac{X}{a}} = \frac{\E{X}}{a}.
        \] The first step follows since \[
            P(X \geq a) = \sum_{n = a}^\infty P(X = n) = \sum_{n = 0}^\infty
            u_a(x)\,P(X = n) = \E{u_a(X)}
        \] for discrete random variables and \[
            P(X \geq a) = \int_a^\infty f_X(x)\:dx = \int_{0}^{\infty}
            u_a(x)f_X(x)\:dx = \E{u_a(X)}
        \] for continuous random variables.
        The second step followed since $\E{Y} \geq 0$ for non-negative random
        variables $Y$.
    \end{proof}
    \begin{corollary}[Chebyshev's inequality]
        If $X$ is a random variable, then for all $a > 0$, \[
            P(|X - \mu| \geq a) \leq \frac{\sigma^2}{a^2}.
        \] 
    \end{corollary}
    \begin{proof}
        Use Markov's inequality on the non-negative random variable $(X - \mu)^2$.
    \end{proof}

    \begin{definition}[Covariance]
        Let $X$, $Y$ be two random variables. The covariance of $X$ and $Y$ is
        defined as \[
            \cov{X, Y} = \E{(X - \E{X})(Y - \E{Y})}.
        \] 
        \begin{remark}
            This can be simplified as \[
                \cov{X, Y} = \E{XY} - \E{X}\E{Y}.
            \] 
        \end{remark}
    \end{definition}

    \begin{definition}[Correlation coefficient]
        The correlation coefficient is defined as \[
            \rho_{X, Y} = \frac{\cov{X, Y}}{\E{X}\, \E{Y}}.
        \]
    \end{definition}

    \begin{theorem}[Weak Law of Large Numbers]
        Suppose that $X_1, \dots, X_n$ are identical and independent random
        variables, with mean $\mu$ and variance $\sigma^2$. Define \[
            \overline{X}_n = \frac{1}{n}\sum_{i = 1}^n X_i.
        \] Then, $\E{\overline{X}_n} = \mu$, and $\var{\overline{X}_n} = \sigma^2 /
        n$. From Chebyshev, \[
            \lim_{n \to \infty} P(|\overline{X}_n - \mu| \geq \epsilon) = 0.
        \] 
    \end{theorem}


\end{document}
% vim: set tabstop=4 shiftwidth=4 softtabstop=4:
