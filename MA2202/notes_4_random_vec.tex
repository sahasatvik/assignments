\documentclass[11pt]{article}

\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{amsmath, amssymb, amsthm}
\usepackage[%
    hidealllines=true,%
    innerbottommargin=15,%
    nobreak=true,%
]{mdframed}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{lipsum}
\usepackage{bm}

\geometry{a4paper, margin=1in, headheight=14pt}

\pagestyle{fancy}
\fancyhf{}
\renewcommand\headrulewidth{0.4pt}
\fancyhead[L]{\scshape MA2201: Probability I}
\fancyhead[R]{\scshape Random variables}
\rfoot{\footnotesize\it Updated on \today}
\cfoot{\thepage}

\def\C{\mathbb{C}}
\def\R{\mathbb{R}}
\def\Q{\mathbb{Q}}
\def\Z{\mathbb{Z}}
\def\N{\mathbb{N}}
\newcommand\ddx[1]{\frac{d #1}{d x}}
\newcommand\ddt[1]{\frac{d #1}{d t}}
\newcommand\dd[3][]{\frac{d^{#1}{#2}}{d {#3}^{#1}}}
\newcommand\ppx[1]{\frac{\partial #1}{\partial x}}
\newcommand\ppt[1]{\frac{\partial #1}{\partial t}}
\newcommand\pp[3][]{\frac{\partial^{#1}{#2}}{\partial {#3}^{#1}}}
\newcommand\norm[1]{\left\lVert#1\right\rVert}
\newcommand\E[1]{E\left[#1\right]}
\newcommand\var[1]{\operatorname{Var}[#1]}
\newcommand\cov[1]{\operatorname{Cov}[#1]}
\renewcommand\vec[1]{\boldsymbol{#1}}

\def\vX{\vec{X}}
\def\vt{\vec{t}}

\newcounter{module}
\setcounter{module}{4}

\newmdtheoremenv[%
    backgroundcolor=blue!10!white,%
]{theorem}{Theorem}[module]
\newmdtheoremenv[%
    backgroundcolor=violet!10!white,%
]{corollary}{Corollary}[theorem]
\newmdtheoremenv[%
    backgroundcolor=teal!10!white,%
]{lemma}[theorem]{Lemma}

\theoremstyle{definition}
\newmdtheoremenv[%
    backgroundcolor=green!10!white,%
]{definition}{Definition}[module]
\newmdtheoremenv[%
    backgroundcolor=red!10!white,%
]{exercise}{Exercise}[module]

\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{example}{Example}
\newtheorem*{solution}{Solution}

\surroundwithmdframed[%
    linecolor=black!20!white,%
    hidealllines=false,%
    innertopmargin=5,%
    innerbottommargin=10,%
    skipabove=0,%
    skipbelow=0,%
]{example}

\numberwithin{equation}{module}

\title{
    \Large\textsc{MA2202: Probability I} \\
    % \vspace{10pt}
    \Huge \textbf{Random vectors} \\
    \vspace{5pt}
    \Large{Spring 2021}
}
\author{
    \large Satvik Saha%
    % \thanks{Email: \tt ss19ms154@iiserkol.ac.in}
    \\\textsc{\small 19MS154}
}
\date{\normalsize
    \textit{Indian Institute of Science Education and Research, Kolkata, \\
    Mohanpur, West Bengal, 741246, India.} \\
    % \vspace{10pt}
    % \today
}

\begin{document}
    \maketitle

    \begin{definition}[Random vector]
        A random vector $\vX\colon \Omega \to \R^n$ is a tuple of random 
        variables $X_i\colon \Omega \to \R$.
    \end{definition}
    \begin{definition}[Joint cumulative distribution function]
        The joint cumulative distribution function of a random vector $\vX$ is
        the map $F_{\vX}\colon \R^n \to [0, 1]$, given as \[
            F_{\vX}(\vec{s}) = P(X_1 \leq s_1, \dots, X_n \leq s_n).
        \] 
    \end{definition}

    \begin{definition}[Joint probability mass function]
        If $X_i$ are discrete random variables, their joint probability mass 
        function is the map $p_{\vX} \colon \R^n \to [0, 1]$, \[
            p_{\vX}(\vec{s}) = P(X_1 = s_1, \dots, X_n = s_n).
        \] 
    \end{definition}

    \begin{definition}[Joint probability density function]
        Suppose that \[
            F_{\vX}(\vec{s}) = \int_{-\infty}^{s_n} \dots \int_{-\infty}^{s_1}
            f_{\vX}(t_1, \dots, t_n)\:dt_1\dots\:dt_n,
        \] then $f_{\vX}\colon \R^n \to [0, 1]$ is the probability density
        function corresponding to the joint cumulative distribution function
        $F_{\vX}$.

        \begin{remark}
            If $f_{\vX}$ is continuous, then \[
                f_{\vX} = \frac{\partial F_{\vX}(t_1, \dots, t_n)}{\partial
                t_1\dots\partial t_n}.
            \] 
        \end{remark}
    \end{definition}

    \begin{definition}[Joint moment generating function]
        Let $\vX$ be a random vector. Then, its joint moment generating function
        is defined as \[
            M_{\vX}(\vt) = \E{e^{\vt^\top \!\vX}} = \E{e^{t_1X_1 +
            \dots + t_nX_n}}.
        \] 
        \begin{remark}
            If $X_1, \dots, X_n$ are independent, then \[
                M_{\vX}(\vt) = \prod M_{X_i}(t_i).
            \] 
        \end{remark} 
    \end{definition}

    \begin{theorem}
        If $X$ and $Y$ are independent continuous random variables, then the
        probability density function of their sum is the convolution $f_{X + Y} =
        f_X * f_Y$, \[
            f_{X + Y}(x) = \int_{\R} f_X(x - t)\,f_Y(t)\:dt.
        \] 
    \end{theorem}
    \begin{example}
        When $X$ and $Y$ are identical and uniform on $[0, 1]$, then \[
            f_{X + Y}(x) = \int_0^1 f(x - t) \:dt = \begin{cases}
                x, &\text{ if }a\in [0, 1], \\
                2 - x, &\text{ if }a\in [1, 2], \\
                0, &\text{ otherwise }.
            \end{cases}
        \] Also, \[
            M_{X + Y}(t) = (M(t))^2 = \frac{1}{t^2}(e^t - 1)^2.
        \] 
    \end{example}

    \begin{definition}[Conditional distribution]
        Let $X$ and $Y$ be two discrete random variables. We write \[
            P(X = s\,|\, Y = t) = \frac{P(X = s, Y = t)}{P(Y = t)}
        \] for $P(Y = t) > 0$. We also have \[
            P(X \leq s \,|\, Y = t) = \sum_{r \leq s}P(X = r\,|\, Y = t).
        \]
        If $X$ and $Y$ are continuous random variables, then the conditional
        distribution of $X$ given $Y = t$ is described as \[
            F_{X\,|\, Y = t}(r) = \int_{-\infty}^s \frac{f_{X, Y}(r, t)}{f_Y(t)}\:dr.
        \] 
    \end{definition}
    \begin{example}
        Consider two continuous random variables $X$ and $Y$ which have a joint
        probability mass function \[
            f_{X, Y}(s, t) = \begin{cases}
                \alpha t, &\text{ if }0 < t < s < 1, \\
                0, &\text{ otherwise.}
            \end{cases}
        \] First normalize, by demanding \[
            \iint_{\R^2} f_{X, Y}(s, t)\:dt\:ds = \int_0^1\int_0^s \alpha t\:dt\:ds = 1,
        \] whence $\alpha = 6$.
        Thus, \[
            \E{Y\,|\,X = s} = \int_\R t \cdot \frac{f_{X, Y}(s, t)}{f_X(s)}\:dt.
        \] Now, \[
            f_X(s) = \int_\R f_{X, Y}(s, t)\:dt = \int_0^s 6t\alpha\:dt = 3s^2
        \] for $0 < s < 1$, and simply $3$ for $s \geq 1$. Thus, \[
            \E{Y\,|\, X = s} = \int_0^s t\cdot \frac{6t}{3s^2}\:dt = \frac{2}{3}s,
        \] in the region $0 < s < 1$. For $s \geq 1$, the expectation becomes $2 /
        3$. 
        Also, \[
            \var{Y\,|\, X = s} = \E{Y^2 \,|\, X = s} - \E{Y \,|\,X = s}^2.
        \] The first term is \[
            \E{Y^2\,|\, X = s} = \int_0^s t^2\cdot \frac{6t}{3s^2}\:dt =
            \frac{1}{2}s^2.
        \] Thus, \[
            \var{Y\,|\, X = s} = \frac{1}{2}s^2 - \frac{4}{9}s^2 = \frac{1}{18}s^2.
        \]
        
        \noindent
        Note that \[
            f_Y(t) = \int_\R f_{X, Y}(s, t)\:ds = \int_t^1 6t\alpha\:ds = 6t(1 - t)
        \] in the region $0 < t < 1$. Thus, \[
            F_Y(t) = \int_0^t 6t'(1 - t')\:dt' = t^2(3 - 2t)
        \] for $0 < t < 1$. $F_Y(t) = 1$ for $t \geq 1$.
    \end{example}

    \begin{theorem}
        For discrete or continuous random variables $X$ and $Y$, \[
            \E{\E{X\,|\,Y}} = \E{X}.
        \] 
    \end{theorem}
    \begin{proof}
        \[
            \E{\E{X\,|\,Y}} = \sum_n \E{X\,|\,Y = n}\,P(Y = n) = \sum_{nm}
            m P(X = m, Y = n).
        \] Reordering the summations, we get \[
            \sum_{m}m\sum_{n}P(X = m, Y = n) = \sum_{m}mP(X = m) = \E{X}.
        \] The proof for discrete random variables is analogous, switching the sums
        for integrals.
    \end{proof}

    \begin{theorem}
        For random variables $X$ and $Y$, \[
            \var{X} = \var{\E{X\,|\,Y}} + \E{\var{X\,|\,Y}}.
        \] 
    \end{theorem}
    \begin{proof}
        Using the previous theorem, \[
            \var{\E{X\,|\,Y}} = \E{\E{X\,|\,Y}^2} - \E{\E{X\,|\,Y}}^2 =
            \E{\E{X\,|\,Y}^2} - \E{X}^2,
        \] and \[
            \E{\var{X\,|\,Y}} = \E{\E{X^2\,|\,Y} - \E{X\,|\,Y}^2} =
            \E{X^2} - \E{\E{X\,|\,Y}^2}.
        \] Adding the above gives the desired result.
    \end{proof}

    \begin{definition}[Order statistics]
        Let $X_1, \dots, X_n$ be discrete independent identically distributed random
        variables, with a common probability mass function. We define \[
            X_{(1)} = \min(X_1, \dots, X_n), \qquad \dots \qquad X_{(n)} =
            \max(X_1, \dots, X_n).
        \] Note that we must have \[
            X_{(1)} \leq X_{(2)} \leq \dots \leq X_{(n - 1)} \leq X_{(n)}.
        \]
    \end{definition}
    \begin{lemma}
        If $X_1, \dots, X_n$ be discrete independent identically distributed random
        variables, with a common probability mass function, for any permutation
        $\sigma$ of $\{1, \dots, n\}$, \[
            P(X_1 = s_1, \dots, X_n = s_n) = P(X_1 = s_{\sigma(1)}, \dots, X_n =
            s_{\sigma(n)}).
        \] 
    \end{lemma}
    \begin{proof}
        The expressions are both equal to $p(s_1) \dots p(s_n)$, where $p$ is the
        common probability mass function.
    \end{proof}

    \begin{theorem}
        Let $X_1, \dots, X_n$ be discrete independent identically distributed random
        variables, and let $g$ denote the joint probability mass function of the
        order statistics. \[
            g(s_1, \dots, s_n) = \begin{cases}
                P(X_{(1)} = s_1, \dots, X_{(n)} = s_n), &\text{ if }s_1 \leq \dots
                \leq s_n, \\
                0, &\text{ otherwise}.
            \end{cases}
        \] Furthermore, let $G_{\tilde{s}}$ denote the group of all permutations of
        $\{s_1, \dots, s_n\}$. Recall that $|G_{\tilde{s}}| = n! / (r_1!\dots r_m!)$
        where $r_i$ of the $s_j$'s are equal to some $t_i$. Thus for increasing
        $s_1, \dots, s_n$, \[
            g(s_1, \dots, s_n) = \sum_{\sigma \in G_{\tilde{s}}} P(X_1 =
            \sigma(s_1), \dots, X_n = \sigma(s_n)) = |G_{\tilde{s}}|P(X_1 = s_1,
            \dots, X_n = s_n).
        \] This can also be written as \[
            g(s_1, \dots, s_n) = \binom{n}{r_1 \dots r_m}p(t_1)^{r_1} \dots
            p(t_m)^{r_m}.
        \] 
    \end{theorem}
    \begin{theorem}
        Let $X_1, \dots, X_n$ be discrete independent identically distributed random
        variables, and let $F$ denote their common cumulative distribution function.
        Then, \[
            P(X_{(n)} \leq s) = P(X_1 \leq s, \dots, X_n \leq s) = F(s)^n.
        \] Now, \[
            P(X_{(n)} = s) = P(X_{(n)} \leq s) - P(X_{(n)} \leq s - 1) = F(s)^n -
            F(s - 1)^n.
        \] Similarly, \[
            P(X_{(1)} \leq s) = 1 - P(X_1 \geq s, \dots, X_n \geq s) = 1 - (1 -
            F(s))^n.
        \] Thus, \[
            P(X_{(1)} = s) = (1 - F(s - 1))^n - (1 - F(s))^n.
        \]  
    \end{theorem}
    
    \begin{theorem}
        Let $X_1, \dots, X_n$ be continuous independent identically distributed random
        variables, let $f$ denote their common probability density function, and let
        $g$ denote their joint probability density function.
        As before, for any permutation of $\{s_1, \dots, s_n\}$, \[
            g(s_{\sigma(1)}, \dots, s_{\sigma(n)}) = f(s_1) \dots f(s_n).
        \] For small $\epsilon > 0$, we can write \[
            P\left(s_{\sigma(1)} - \frac{\epsilon}{2} \leq X_1 \leq s_{\sigma(1)} +
            \frac{\epsilon}{2}, \dots,  s_{\sigma(n)} - \frac{\epsilon}{2} \leq X_n
            \leq s_{\sigma(n)} + \frac{\epsilon}{2}\right) \approx
            \epsilon^nf(s_1)\dots f(s_n).
        \] Therefore, for $s_1 < s_2 < \dots < s_n$, we have \[
            P\left(s_{\sigma(1)} - \frac{\epsilon}{2} \leq X_{(1)} \leq
            s_{\sigma(1)} + \frac{\epsilon}{2}, \dots,  s_{\sigma(n)} -
            \frac{\epsilon}{2} \leq X_{(n)} \leq s_{\sigma(n)} +
            \frac{\epsilon}{2}\right) \approx n!\epsilon^nf(s_1)\dots f(s_n).
        \] Therefore, dividing by $\epsilon^n$ and letting $\epsilon \to 0$, we have
        \[
            g(s_1, \dots, s_n) = n! f(s_1)\dotsf(s_n).
        \] Thus, assuming the continuity of $f$, we have \[
            g(s_1, \dots, s_n) = \begin{cases}
                n!\lim_{\substack{(r_1, \dots, r_n) \to (s_1, \dots, s_n) \\ r_1 <
                \dots < r_n}} f(r_1) \dots f(r_n), &\text{ if }s_1 \leq \dots \leq
                s_n, \\
                0 &\text{ otherwise}.
            \end{cases}
        \] 
    \end{theorem}
    
    \begin{theorem}
        Let $X_1, \dots, X_n$ be continuous independent identically distributed random
        variables, and let $F$ denote their common cumulative distribution function.
        Then, like before, \[
            P(X_{(1)} \leq s) = 1 - (1 - F(s))^n, \qquad
            P(X_{(n)} \leq s) = F(s)^n.
        \] Thus, the probability density functions are given by \begin{align*}
            f_{X_{(1)}}(s) &= \dd{}{s}P(X_{(1)} \leq t) = n (1 - F(s)^{n - 1}) f(s), \\
            f_{X_{(n)}}(s) &= \dd{}{s}P(X_{(n)} \leq t) = n F(s)^{n - 1} f(s).
        \end{align*}
    \end{theorem}
    
\end{document}
% vim: set tabstop=4 shiftwidth=4 softtabstop=4:
