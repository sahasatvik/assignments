\documentclass[11pt]{article}

\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{amsmath, amssymb, amsthm}
\usepackage[%
    hidealllines=true,%
    innerbottommargin=15,%
    nobreak=true,%
]{mdframed}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{lipsum}

\geometry{a4paper, margin=1in, headheight=14pt}

\pagestyle{fancy}
\fancyhf{}
\renewcommand\headrulewidth{0.4pt}
\fancyhead[L]{\scshape MA2201: Probability I}
\fancyhead[R]{\scshape Introduction to probability}
\rfoot{\footnotesize\it Updated on \today}
\cfoot{\thepage}

\def\C{\mathbb{C}}
\def\R{\mathbb{R}}
\def\Q{\mathbb{Q}}
\def\Z{\mathbb{Z}}
\def\N{\mathbb{N}}
\newcommand\ddx[1]{\frac{d #1}{d x}}
\newcommand\ddt[1]{\frac{d #1}{d t}}
\newcommand\dd[3][]{\frac{d^{#1}{#2}}{d {#3}^{#1}}}
\newcommand\ppx[1]{\frac{\partial #1}{\partial x}}
\newcommand\ppt[1]{\frac{\partial #1}{\partial t}}
\newcommand\pp[3][]{\frac{\partial^{#1}{#2}}{\partial {#3}^{#1}}}
\newcommand\norm[1]{\left\lVert#1\right\rVert}
\newcommand\E[1]{E[#1]}

\newcounter{module}
\setcounter{module}{1}

\newmdtheoremenv[%
    backgroundcolor=blue!10!white,%
]{theorem}{Theorem}[module]
\newmdtheoremenv[%
    backgroundcolor=violet!10!white,%
]{corollary}{Corollary}[theorem]
\newmdtheoremenv[%
    backgroundcolor=teal!10!white,%
]{lemma}[theorem]{Lemma}

\theoremstyle{definition}
\newmdtheoremenv[%
    backgroundcolor=green!10!white,%
]{definition}{Definition}[module]
\newmdtheoremenv[%
    backgroundcolor=red!10!white,%
]{exercise}{Exercise}[module]

\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{example}{Example}

\surroundwithmdframed[%
    linecolor=black!20!white,%
    hidealllines=false,%
    innertopmargin=5,%
    innerbottommargin=10,%
    skipabove=0,%
    skipbelow=0,%
]{example}

\numberwithin{equation}{module}

\title{
    \Large\textsc{MA2202: Probability I} \\
    % \vspace{10pt}
    \Huge \textbf{Introduction to probability} \\
    \vspace{5pt}
    \Large{Spring 2021}
}
\author{
    \large Satvik Saha%
    % \thanks{Email: \tt ss19ms154@iiserkol.ac.in}
    \\\textsc{\small 19MS154}
}
\date{\normalsize
    \textit{Indian Institute of Science Education and Research, Kolkata, \\
    Mohanpur, West Bengal, 741246, India.} \\
    % \vspace{10pt}
    % \today
}

\begin{document}
    \maketitle

    \begin{definition}[Experiment]
        An experiment is an act which can be repeated under similar conditions.
    \end{definition}
    \begin{example}
        Tossing a fair coin constitutes an experiment. Here, the possible outcomes
        of the experiment are `heads' or `tails'.
    \end{example}

    \begin{definition}[Random experiment]
        A random experiment is one where there is more than one possible outcome, 
        and the outcome of the experiment cannot be determined beforehand.
    \end{definition}
    \begin{example}
        A coin toss, or the roll of a die is typically regarded as a random
        experiment.
    \end{example}
    
    \begin{definition}[Sample space]
        A sample space $\Omega$ is the set of all outcomes of an experiment.
    \end{definition}
    \begin{example}
        The sample space of rolls of a single die is $\Omega = \{1,2,3,4,5,6\}$.
        Note that this is a finite, discrete sample space.
    \end{example}
    \begin{example}
        In a game of guessing a particular natural number, the sample space is the
        set of all natural numbers $\N$. Note that this is an infinite, discrete
        sample space.
    \end{example}
    \begin{example}
        The temperature in a room may vary continuously. Thus, the sample space of
        temperatures is a continuous sample space.
    \end{example}
    
    \begin{definition}[Events]
        A set of events $\mathcal{E}$ is a collection of measurable subsets of
        a sample space such that $\Omega \in \mathcal{E}$, it is closed under
        complementing, and it is closed under countable unions.
    \begin{remark}
        Formally, the event space $\mathcal{E} \subseteq \mathcal{P}(\Omega)$ forms
        a $\sigma$-algebra. The pair $(\Omega, \mathcal{E})$ is called a measurable
        space.
    \end{remark}
    \end{definition}
    \begin{example}
        We may have $\mathcal{E} = \{\emptyset, \{2, 4, 6\}, \{1, 3, 5\},
        \Omega\}$ as our set of events in the case of rolling a die.
        Obtaining an even number is an event.
    \end{example}

    Note that the set of events is also closed under countable intersections,
    because for a countable set of events $\{E_n\}_n$, we have \[
        \bigcap_{n = 1}^\infty E_n = \left(\bigcup_{n = 1}^\infty E_n^c\right)^c
    \] by De Morgan's Law, and $E_n^c \in \mathcal{E}$.
    
    \begin{definition}[Probability]
        A probability measure is a function $P\colon \mathcal{E} \to [0, 1]$ such
        that $P(\emptyset) = 0$, $P(\Omega) = 1$, and for any countable collection of
        pairwise disjoint events $\{E_n\}_n$, we have \[
            P(E) = \sum_{n = 1}^\infty P(E_n), \qquad E = \bigcup_{n = 1}^\infty E_n.
        \] 
    \end{definition}

    Note that we obtain the relation \[
        P(A^c) = 1 - P(A)
    \] directly by noting that $A \cup A^c = \Omega$ and $P(\Omega) = 1$.

    \begin{definition}[Probability space]
        A probability space $(\Omega, \mathcal{E}, P)$ consists of a sample space
        $\Omega$ together with a set of events $\mathcal{E}$ and a probability
        measure $P$.
    \end{definition}
    \begin{example}
        In the context of a coin toss, set $\Omega = \{H, T\}$, $\mathcal{E} =
        \{\emptyset, \{H\}, \{T\}, \{H, T\}\}$ and define $P\colon \mathcal{E} \to
        [0, 1]$ such that $P(H) = P(T) = 1 /2$. It can be verified that
        $\mathcal{E}$ is a $\sigma$-algebra and that $P$ is a probability measure,
        so the triple $(\Omega, \mathcal{E}, P)$ is indeed a probability space.
    \end{example}

    \begin{definition}[Equally likely events]
        Two events $A, B \in \mathcal{E}$ are said to be equally likely if $P(A) =
        P(B)$.
    \end{definition}

    The classical definition of probability states that if the sample space
    $\Omega$ consists of $N$ equally likely events, then the probability of an
    event $E \in \mathcal{E}$ is given by \[
        P(E) = \frac{|E|}{N}.
    \] 
    Note that this assumes that the notion of equally likely events is known
    beforehand.\\

    The frequency definition of probability involves performing an experiment
    $n$ times, denoting $f_n(E)$ as the frequency of the event $E$ over these
    iterations, and defining \[
        P(E) = \lim_{n \to \infty} \frac{f_n(E)}{n}. 
    \] 
    Note that such a limit may not always be well defined.
    
    \begin{definition}[Mutually exclusive events]
        Two events $A, B \in \mathcal{E}$ are called mutually exclusive if $A \cap B
        = \emptyset$.
    \end{definition}
    \begin{definition}[Exhaustive events]
        A set of events $S \subseteq \mathcal{E}$ is called exhaustive if \[
            \Omega = \bigcup_{E \in S} E.
        \] 
    \end{definition}
    \begin{example}
        For any event $A \in \mathcal{E}$, we see that $A$ and $A^c$ are mutually
        exclusive and exhaustive.
    \end{example}


    \begin{theorem}[Principle of Inclusion and Exclusion]
        For events $A_1, A_2, \dots, A_n \in \mathcal{E}$, we have 
        \begin{align*}
            P(A_1 \cup \dots \cup A_n) \,=\, &\sum P(A_i) - \sum_{i < j} P (A_i \cap
            A_j) + \\ &\sum_{i < j < k} P(A_i \cap A_j \cap A_k) - \dots +
            (-1)^{n-1}P(A_1 \cap \dots A_n).
        \end{align*}
    \end{theorem}
    \begin{proof}
        This follows by induction. The base case of $n = 2$ states \[
            P(A_1 \cup A_2) = P(A_1) + P(A_2) - P(A_1 \cap A_2),
        \] which follows form the fact that the sets $A_1\setminus A_2$, $A_1\cap
        A_2$ and $A_2\setminus A_1$ are pairwise disjoint.

        For the induction step, assume that the expansion holds for $n = m \geq 2$
        and note that \[
            P \left( A_{m + 1} \cap \bigcup_{i = 1}^m A_i \right) = P \left(
            \bigcup_{i = 1}^m A_i \cap A_{m + 1} \right).
        \] Putting the $n = m + 1$ case into the $n = 2$ case and expanding the
        above $n = m$ case, the full expansion will follow. 
    \end{proof}

    \begin{theorem}[Boole's inequality]
        For events $A_1, A_2, \dots, A_n \in \mathcal{E}$, we have \[
            P(A_1 \cup \dots A_n) \leq \sum P(A_i).
        \] 
    \end{theorem}
    \begin{proof}
       This is clearly true for $n = 2$, since \[
            P(A_1 \cup A_2) = P(A_1) + P(A_2) - P(A_1 \cap A_2) \leq P(A_1) +
            P(A_2).
        \] Define \[
            B_i = A_i \setminus \bigcup_{j = 1}^{i - 1} A_j.
        \] Note that $\cup B_i = \cup A_i$, and all $B_i$ are pairwise disjoint.
        In addition, $B_i \subseteq A_i$, so $P(B_i) \leq P(A_i)$. Thus, \[
            P(A_1 \cup \dots \cup A_2) \,=\, \sum P(B_i) \,\leq\, \sum P(A_i). \qedhere
        \] 
    \end{proof}

    \begin{theorem}[Bonferroni's inequality]
        For events $A_1, A_2, \dots, A_n \in \mathcal{E}$, we have \[
            P(A_1 \cap \dots \cap A_n) \geq \sum P(A_i) - (n - 1).
        \] 
    \end{theorem}
    \begin{proof}
        This holds for $n = 2$, since \[
            P(A_1 \cap A_2) = P(A_1) + P(A_2) - P(A_1 \cup A_2) \geq P(A_1) +
            P(A_2) - 1.
        \]
        For the induction step, suppose this holds for $n = m \geq 2$. Thus, \[
            P(A_1 \cap \dots \cap A_m \cap A_{m + 1}) \geq P(A_1 \cap \dots \cap
            A_m) + P(A_m) - 1 \geq \sum P(A_i) - m. \qedhere
        \] 
    \end{proof}

\end{document}
% vim: set tabstop=4 shiftwidth=4 softtabstop=4:
