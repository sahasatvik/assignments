\documentclass[11pt]{article}

\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{amsmath, amssymb, amsthm}
\usepackage[%
    hidealllines=true,%
    innerbottommargin=15,%
]{mdframed}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage[hidelinks]{hyperref}
\usepackage{fancyhdr}
\usepackage{lipsum}
\usepackage{bm}
\usepackage{braket}
\usepackage{siunitx}

\geometry{a4paper, margin=1in, headheight=14pt}

\pagestyle{fancy}
\fancyhf{}
\renewcommand\headrulewidth{0.4pt}
\fancyhead[L]{\scshape PH2201}
\fancyhead[R]{\scshape \leftmark}
\rfoot{\footnotesize\it Updated on \today}
\cfoot{\thepage}

\def\C{\mathbb{C}}
\def\R{\mathbb{R}}
\def\Q{\mathbb{Q}}
\def\Z{\mathbb{Z}}
\def\N{\mathbb{N}}
\newcommand\ve[1]{\boldsymbol{#1}}
\newcommand\ddx[1]{\frac{d #1}{d x}}
\newcommand\ddt[1]{\frac{d #1}{d t}}
\newcommand\dd[3][]{\frac{d^{#1}{#2}}{d {#3}^{#1}}}
\newcommand\ppx[1]{\frac{\partial #1}{\partial x}}
\newcommand\ppt[1]{\frac{\partial #1}{\partial t}}
\newcommand\pp[3][]{\frac{\partial^{#1}{#2}}{\partial {#3}^{#1}}}
\newcommand\norm[1]{\left\lVert#1\right\rVert}
\newcommand\grad[1]{\ve{\nabla}#1}
\newcommand\divg[1]{\ve{\nabla}\cdot#1}
\newcommand\curl[1]{\ve{\nabla}\times#1}
\newcommand\lapl[1]{\nabla^2 #1}
\newcommand\E[1]{\langle #1 \rangle}

\newmdtheoremenv[%
    backgroundcolor=blue!10!white,%
]{theorem}{Proposition}[section]
% \newmdtheoremenv[%
%     backgroundcolor=violet!10!white,%
% ]{corollary}{Corollary}[theorem]
% \newmdtheoremenv[%
%     backgroundcolor=teal!10!white,%
% ]{lemma}[theorem]{Lemma}

\theoremstyle{definition}
\newmdtheoremenv[%
    backgroundcolor=green!10!white,%
]{definition}{Definition}[section]
\newmdtheoremenv[%
    backgroundcolor=red!10!white,%
]{exercise}{Exercise}[section]
\newmdenv[%
    hidealllines=false,%
    linecolor=black!20!white,%,
    % backgroundcolor=cyan!5!white,%
    innertopmargin=10,%
]{equationbox}
\newenvironment{boxedeq}%
    {\begin{equationbox}\begin{equation}}%
    {\end{equation}\end{equationbox}}
\newenvironment{boxedeq*}%
    {\begin{equationbox}\begin{equation*}}%
    {\end{equation*}\end{equationbox}}

\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{example}{Example}

\numberwithin{equation}{section}

\title{
    \Large\textsc{PH2201} \\
    % \vspace{10pt}
    \Huge \textbf{Basic Quantum Mechanics} \\
    \vspace{5pt}
    \Large{Spring 2021}
}
\author{
    \large Satvik Saha%
    % \thanks{Email: \tt ss19ms154@iiserkol.ac.in}
    \\\textsc{\small 19MS154}
}
\date{\normalsize
    \textit{Indian Institute of Science Education and Research, Kolkata, \\
    Mohanpur, West Bengal, 741246, India.} \\
    % \vspace{10pt}
    % \today
}

\begin{document}
    \maketitle
    \tableofcontents

    \section{Introduction}
    The quantum world differs from the classical world in many aspects, most of
    which we seldom encounter in our daily lives and are hence unintuitive.
    \begin{itemize}
        \item The physical world is not deterministic; uncertainty is
        intrinsic to the quantum world.
        This is sometimes illustrated by the Schr\"odinger's cat thought
        experiment.
        \item Both light and matter exhibit characteristics of waves as well
        as those of particles.
        However, a single object cannot exhibit both of these properties 
        simultaneously.
        \item Physical quantities may be quantized -- they may be
        constrained to have discrete values rather than vary continuously.
    \end{itemize}

    \section{Duality of light}
    \subsection{Blackbody radiation}
    A blackbody is an object which absorbs all radiation incident on it, and
    reflects none. It also emits radiation of all frequencies.

    Kirchoff's Law says that the rates of emission and absorption of radiation of a
    body in thermal equilibrium will be equal. By thermal equilibrium, we mean that
    the temperatures of the body and its surroundings are equal.

    \begin{theorem}[Stefan-Boltzmann Law]
        The power emitted by a blackbody is given by \[
            P \,=\, \sigma A T^4.
        \]
        Here, $\sigma \approx 
        \SI{5.67e-8}{\joule\second^{-1}\meter^{-2}\kelvin^{-4}}$ is called the
        Stefan-Boltzmann constant.
    \end{theorem}

    We may break down the total energy density $\rho \propto T^4$ in terms of the
    contributions from each frequency, so \[
        \rho = \int_0^\infty \rho(\nu) \:d\nu.
    \] 
    It turns out that $\rho(\nu)$ is non-monotonic. This cannot be explained by
    classical mechanics (Rayleigh-Jean's Law), which predicts that $\rho(\nu)$
    is unbounded with increasing frequency -- the famous ultraviolet catastrophe.

    \begin{theorem}[Wien's Law]
        The positions of the peaks in $\rho(\nu)$ are described by \[
            \lambda_{peak} \,=\, \frac{w}{T}.
        \]
        Here, $w \approx \SI{2.9e-3}{\milli\kelvin}$.  
    \end{theorem}
    Note that at $T \approx \SI{300}{\kelvin}$, the peak wavelength $\lambda_{peak}$
    is in the infrared range: this is why night vision googles are useful.
    
    Consider a collection of electromagnetic waves in a blackbody cavity, with
    temperature $T$. This can be seen as the superposition of normal modes.
    The classical approach to the blackbody problem is to suppose that the energy
    density at a particular frequency is given by \[
        \rho(\nu) = \E{E} n(\nu),
    \] where $n(\nu)$ is the number density of wave modes with frequency $\nu$, and
    $E$ is the average energy of the radiation.

    The classical law of equipartition of energy gives \[
        \E{E} = k_B T,
    \] where $k_B$ is the Boltzmann constant.

    The wavenumber of for modes within the cavity is given by \[
        \ve{k} = \frac{2\pi}{L}\ve{n},
    \] where $\ve{n} = (n_x, n_y, n_z)$ with integral components. Now, \[
        \nu = \frac{c}{\lambda} = \frac{c}{L}n.
    \] Treating $n$ as a continuous variable and using $dV = 4\pi n^:dn$, we write
    \[
        n(\nu)\:d\nu = \frac{8\pi}{c^3}\nu^2\:d\nu.
    \] This leads to the Rayleigh-Jean Law,
    \begin{boxedeq}
        \rho(\nu)\:d\nu = \E{E} n(\nu) \:d\nu 
            = \frac{8\pi k_B T}{c^3}\nu^2\:d\nu.
    \end{boxedeq}\\~\\

    Planck looked at the probability distribution for the energy, \[
        P(E) = \frac{1}{k_B T}e^{-E /k_B T}.
    \] This is the Boltzmann distribution. It can be shown that \[
        \E{E} = \frac{\int_0^\infty E P(E) \:dE}{\int_0^\infty P(E) \: dE} 
        = k_B T,
    \] which recovers the Rayleigh-Jean Law.

    Planck's idea was to restrict $E$ to discrete values; integral multiples of the
    frequency $\nu$. This leads to \[
        \E{E} = \frac{\sum E P(E)}{\sum P(E)} 
            = \frac{h\nu}{e^{h\nu /k_B T} - 1}.
    \] 
    This gives us the Planck distribution.

    \begin{theorem}[Planck's Law]
        The spectral energy density of radiation emitted by a blackbody in thermal
        equilibrium is described by the distribution \[
            \rho(\nu)\:d\nu 
                = \frac{8\pi h}{c^3} \frac{\nu^3}{e^{h\nu /k_B T} - 1} \:d\nu.
        \] 
        Here, $h \approx \SI{6.626e-34}{\joule\second}$ is called Planck's constant.
    \end{theorem}
    When $h\nu \ll 1$, we recover the Rayleigh-Jean limit. When $h\nu \gg 1$, we get
    the Wien limit.

    Now we calculate, \[
        \rho = \int_0^\infty \rho(\nu)\:d\nu = \frac{8\pi^5k_B^4}{15c^3h^3}T^4,
    \] which recovers the Stefan-Boltzmann Law with \[
        \sigma \,=\, \frac{2\pi^5k_B^4}{15c^2h^3}.
    \] 
    Also, the maxima of the Planck distribution recovers Wien's Law, with \[
        \nu_{max} \,\approx\, 2.8 \frac{k_B T}{h}.
    \] 

    \subsection{Photoelectric effect}
    This reveals the dual nature of light. Classical optics relies on the wave
    nature of light, thus explaining phenomena such as interference and diffraction.
    This culminates in Maxwell's equations, which predict the wave nature of light
    as the propagation of oscillating electric and magnetic fields.

    The photoelectric effect in the phenomenon in which light shining on a metal
    surface ejects electrons from it, thus producing a current. Suppose that the
    incident light has frequency $\nu$, intensity $I$ and this produces a current
    $i$. We can calculate the maximum kinetic energy of the emitted electrons
    $E_{max} = eV_0$ by adjusting an opposing potential $V$.

    It turns out that for a constant intensity $I$, the photocurrent saturates at
    the same value. However, different frequencies $\nu$ produces different stopping
    potentials $V_0$; the greater the frequency, the greater the magnitude of the
    stopping potential. This turns out to have a linear relationship, with
    \begin{boxedeq}
        E_{max} \,=\, eV_0 \,=\, h(\nu - \nu_0) \,=\, h\nu - \phi.
    \end{boxedeq}
    The slope $h$ is universal for all metals, while $\phi = h\nu_0$ varies
    between different metals.
    This shows that below a certain frequency $\nu_0$, we obtain no photocurrent,
    regardless of the intensity! This appears strange from a classical perspective,
    where the energy delivered by an electromagnetic wave is related to its
    intensity, not its frequency.

    Einstein proposed that light strikes the metal in bundles of energy, all
    integral multiples of $h\nu$. There is also a minimum binding energy which must
    be overcome to liberate electrons from the metal surface -- this cannot be paid
    in a continuous manner, since any partial energy given to an electron will be
    lost before the arrival of the next energy bundle. Thus, each energy bundle must
    carry a minimum energy $h\nu_0$ in order to liberate electrons and produce a
    photocurrent.

    This establishes a particle-like mature of light. Each energy bundle, or
    particle of light, is called a photon.

    \section{Duality of matter}
    \subsection{Matter waves}
    Louis de Broglie further hypothesised that matter also has a wave nature, with
    an associated wavelength of 
    \begin{boxedeq}
        \lambda_{matter} \,=\, \frac{h}{p}.
    \end{boxedeq}
    This has been demonstrated by Davisson and Germer, where a stream of electrons
    exhibits diffraction.
    
    This has some amazing implications. In the classical world, knowledge of a
    particle's position and momentum is enough to pinpoint its trajectory with
    arbitrary precision. However, the wavelike nature of matter would imply that we
    can no longer talk of a definite, localized position when we have knowledge of
    the particle's momentum! This uncertainty is inherent to quantum mechanics.

    Another consequence is the phenomenon of quantum tunnelling.

    \subsection{Double slit experiment with pellets}
    Consider Young's double slit experiment, this time with pellets sprayed from a
    gun at a wall with two slits. The other sides has a collector, which is
    moveable. Assume that the pellets do not break, and that they arrive in groups
    independent of the rate of firing. We want to find the probability that a pellet
    lands at a distance $x$ from the centre of the screen.

    Now, this just means that we have to count the number of pellets which reach the
    detector in a given time interval. Also, we expect that the probability
    distribution for two slits ought to be the sum of the distributions for the
    individual slits, obtained by closing one slit at a time. The distribution for a
    single slit looks something like a Gaussian, so their combination should also
    look like a broad Gaussian.

    Note that this says nothing about phenomena such as interference.

    \subsection{Double slit experiment with electrons}
    Instead of using pellets, we now use electrons. These can be fired by heating
    a tungsten wire inside a box with a pinhole. Suppose that the detector at the
    other side produces a `click' whenever an electron strikes it; all such clicks
    are identical and random. The electrons are also fired very slowly, so that
    there is only one electron passing through the slits and hitting the screen at a
    given time. This means that we hear distinct, separate clicks at random
    intervals, which means that an electron must have passed through one of the
    slits at random before impacting the screen.

    What we observe is that over a long time, the probability distribution of the
    electrons is in the form of an interference pattern, just like with light waves.
    The strange thing is that if each electron passes through one slit at a time,
    the other slit ought to be `closed' from its perspective, which means that we
    ought to have obtained the superposition of two Gaussians!
    
    An attempt to explain this might be that the electrons follow some complex
    pathways incorporating both slits. On the other hand, the probability observed
    at the centre is greater than the sum of the probabilities for the single slits,
    and there are regions of zero probability on the screen where the single slit
    probabilities would suggest finite values. This would imply that closing a slit
    somehow increases the probabilities in one region and decreases that in another.

    Thus, the electrons arrive at the screen as particles, but their distribution on
    the screen shows interference patterns just like those of a wave!

    This suggests that a simple sum of the probabilities $P_1$ and $P_2$ from each
    of the slits is not enough. Analogous to the double slit experiment with light,
    we must assign complex probability amplitudes $A_1$ and $A_2$, which we then add
    up. \[
        P_1 = |A_1|^2, \qquad P_2 = |A_2|^2, \qquad P_{12} = |A_1 + A_2|^2.
    \] 
    
    \subsubsection{Spying on the electrons}
    Suppose that we repeat this experiment, but this time we place a detector on the
    slits. Thus, we can be sure which slit a given electron passes through.
    One way to do this is to place a light source in between the slits and the
    screen. When an electron passes through a slit, it will scatter some light which
    we see as a flash in the neighbourhood of the slit.

    What happens is that the interference pattern disappears! The distribution now
    has two peaks, just like a classical particle would behave, i.e.\ we now have
    $P_1 + P_2$.  We do indeed observe a single flash corresponding to each click,
    so we can pinpoint which slit a given electron on the screen corresponds to.

    To see whether the light source is somehow disturbing the paths of the
    electrons, we make it dimmer and dimmer.  Note that this merely changes the
    number of photons hanging around the slits, not their energy (which is $\propto
    h\nu$), so we do not expect the brightness of the flashes to change.  Now, some
    clicks do not have a corresponding flash; some electrons are reaching the screen
    unnoticed by our detector at the slits. The interference pattern at the screen
    gradually reappears, in the form $P_{12}$!

    Now, the momentum imparted by the light photons obeys $p = h / \lambda$. By
    choosing a very large $\lambda$, we can have $p \to 0$, which ought to disturb
    the electrons to a lesser extent. When we do this, we still observe the
    classical pattern. However, when $\lambda$ exceeds the order of the slit
    separation, we lose the ability to resolve which slit the electron passed
    through, i.e.\ the flashes cannot be identified with the correct slit. At this
    point, we get back the interference pattern. \\

    Thus, there is no way to answer which slit each electron passed through while
    retaining the interference pattern!
    This is intrinsic to the quantum system, in the form of the Heisenberg
    uncertainty principle.

    \section{Bohr's atomic model}
    Recall Rutherford's experiment where he fired $\alpha$ particles at a gold
    foil, which established the existence of a very small region of positive charge
    in the centre of every atom (the nucleus), surrounded by negatively charged
    electrons. This raises the problem of the stability of the electron orbits -- an
    accelerating charge must radiate energy, that too over a wide range of
    frequencies. On the other hand, radiation emitted by an atom is always observed
    at discrete frequencies. For example, the wavelengths emitted by hydrogen are 
    given by the Rydberg constant.
    \begin{boxedeq}
        \frac{1}{\lambda} = R\left(\frac{1}{m^2} - \frac{1}{n^2}\right).
    \end{boxedeq}
    Bohr's idea was that the angular momenta of the electrons are quantized. We set
    \[
        L = pr = mvr = n\hbar.
    \] In a sense, an electron forms a standing wave in its orbit, with a
    circumference of $n\lambda$. By balancing the Coulomb and centripetal forces, we
    can write \[
        r = \frac{4\pi\epsilon_0 \hbar^2}{me^2}n^2 = a_0n^2.
    \] Here, $a_0 \approx \SI{5.29e-11}{\meter}$ is called the Bohr radius.
    As a result, \[
        E_n = \frac{1}{2}mv^2 - \frac{e^2}{4\pi\epsilon_0 r} 
            \approx -13.6 \frac{1}{n^2}\,\si{e\volt}.
    \] The energies emitted by electrons are now restricted to differences of these
    energy levels, so \[
        h\nu = E_m - E_n.
    \] 

    \section{Postulates of quantum mechanics}
    \begin{enumerate}
        \itemsep0
        \item Associated with each classical outcome of a quantum experiment is a
        probability amplitude, $\Psi$, which is not directly observable.
        \item The probability distribution is given by $P = |\Psi|^2$, which is
        observable. For any physically interpretable wavefunction $\Psi$, we demand
        that $|\Psi|^2$ is normalizable. Thus, \[
            \int |\Psi|^2 \:dV = 1.
        \] As a consequence, we require $\Psi \to 0$ as we move away from the
        origin. We also demand $\Psi$ be continuous, and $\Psi'$ be continuous
        almost everywhere.
        \item For a system with many classical outcomes, we write \[
            \Psi = \sum_{i = 1}^\infty \Psi_i.
        \] Thus, the probability distribution $P$ carries signatures of the
        probabilities $P_i = |\Psi_i|^2$. For example when we have $n = 2$ 
        outcomes, \[
            P = |\Psi_1 + \Psi_2|^2 = P_1 + P_2 + 2\sqrt{P_1P_2}\cos\delta.
        \] 
    \end{enumerate}

    \subsection{The Schr\"odinger Equation}
    \begin{theorem}
        The wavefunction $\Psi$ must obey the differential equation \[
            \left[-\frac{\hbar^2}{2m}\lapl{} + V(\ve{r}, t)\right]\Psi(\ve{r}, t) =
            i\hbar \ppt{}\Psi(\ve{r}, t).
        \] 
    \end{theorem}
    Consider the simpler case where $V(\ve{r})$ has no time dependence.
    To solve this equation, we often perform separation of variables, \[
        \Psi(\ve{r}, t) = \psi(\ve{r})\, T(t).
    \] As a result, we observe that \[
        -\frac{1}{\psi}\frac{\hbar^2}{2m}\lapl{\psi} + V = \frac{i\hbar}{T}\ddt{T} =
        \text{constant}.
    \] The time part is solved by $T(t) = e^{-i E t/ \hbar}$, where the constant is
    denoted as $E$.  Note that we have shown $\Psi(\ve{r}, t) = \psi(\ve{r}) e^{-iEt
    /\hbar}$, so \[
        |\Psi|^2 = \Psi^*\Psi = \psi^*\psi = |\psi^2|,
    \] The spatial part must now satisfy \[
        -\frac{\hbar^2}{2m} \lapl{\psi} + V\psi = E\psi.
    \] which is called the time independent Schr\"odinger equation.
    The Hamiltonian operator is defined as \[
        \hat{H} = -\frac{\hbar^2}{2m}\lapl{} + V,
    \] so the spatial part of the equation says $\hat{H}\psi = E\psi$.
    In other words, the wavefunction $\Psi$ here is an eigenfunction of the
    Hamiltonian. The full solution can be written as a linear superposition of all
    such eigenfunctions $\Psi_i$.
    For convenience, we deal with only one spatial dimension here on.

    \subsection{Observables and operators}
    \begin{definition}[Expectation value]
        The expectation value of any linear operator $\hat{A}$ is defined as \[
            \E{\hat{A}} = \int_{-\infty}^{+\infty} \psi^* \hat{A} \psi \:dx.
        \]
    \end{definition}
    \begin{example}
        For example, the position, momentum, and energy operators are given by \[
            \hat{x} = x, \qquad \hat{p}_x = -i\hbar \ppx{}, \qquad \hat{E} = i\hbar
            \ppt{}.
        \] 
        This covers all physically observable quantities, and are thus called
        observables. Note that these are all linear operators.
    \end{example}

    What happens if $\psi$ is an eigenstate of $\hat{A}$, with eigenvalue $\lambda$?
    Note that $\hat{A}\psi = \lambda\psi$, so \[
        \E{\hat{A}} = \int_{-\infty}^{+\infty} \lambda \psi^*\psi\:dx = \lambda.
    \]

    \begin{theorem}[Ehrenfest's Theorem]
        The expectation values obey classical laws. For instance, \[
            \ddt{}\E{p} = \E{-\ppx{}V}.
        \] 
    \end{theorem}
    \begin{proof}
        We start by calculating 
        \begin{align*}
            \hat{p}\hat{H} - \hat{H}\hat{p} &= \left(-i\hbar
            \ppx{}\right)\left(\frac{\hbar^2}{2m}\pp[2]{}{x} + V\right)
            - \left(\frac{\hbar^2}{2m}\pp[2]{}{x} + V\right)\left(-i\hbar
            \ppx{}\right) \\
                &= -\frac{i\hbar^3}{2m}\pp[3]{}{x} - i\hbar\ppx{}V +
                \frac{i\hbar^3}{2m}\pp[3]{}{x} + i\hbar V\ppx{} \\
                &= -i\hbar \ppx{}V + i\hbar V\ppx{} \\
                &= -i\hbar \ppx{V}.
        \end{align*}
        Now, \[
            \ddt{\E{p}} = \int \ppt{\psi^*}\hat{p}\psi\:dx + \int \psi^*
            \ppt{\hat{p}}\psi\:dx + \int \psi^*\hat{p}\ppt{\psi}\:dx.
        \] The central term is just $\E{\partial \hat{p} / \partial t}$, which is zero.
        From the Schr\"odinger equation, we can write \[
            \hat{H}\psi = i\hbar\ppt{\psi}, \qquad (\hat{H}\psi)^* = \psi^*\hat{H} =
            -i\hbar \ppt{\psi^*}.
        \] Thus, \[
            \ddt{\E{p}} = \frac{1}{i\hbar} \int \psi^*(\hat{p}\hat{H} -
            \hat{H}\hat{p}) \psi \:dx =  \E{-\ppx{V}}. \qedhere
        \] 
    \end{proof}

    \begin{definition}[Commutator]
        The commutator of two linear operators $\hat{A}$ and $\hat{B}$ is defined as
        \[
            [\hat{A}, \hat{B}] = \hat{A}\hat{B} - \hat{B}\hat{A}.
        \] 
    \end{definition}
    
    In a statistical distribution, the first, second, third and fourth moments deal
    with the mean, standard deviation, skew and kurtosis. Skew is a measure of
    symmetry, and kurtosis is a measure of peakedness.

    \newpage

    \subsection{The uncertainty principle}
    \begin{theorem}[Heisenberg's Uncertainty Principle]
        The standard deviations of the position and momentum are related as \[
            \sigma_x \sigma_p \geq \frac{\hbar}{2}.
        \] More generally, for any two Hermitian operators, \[
            \sigma_A \sigma_B \geq \frac{1}{2}|\E{[\hat{A}, \hat{B}]}|.
        \] 
    \end{theorem}
    \begin{proof}
        We evaluate the following expression using integration by parts. \[
            i\hbar \int \ddx{\psi^*}\psi x\:dx = -i\hbar\int \psi^*\psi\:dx - i\hbar
            \int \ddx{\psi}\psi^* x\:dx = -i\hbar + \left[i\hbar \int \ddx{\psi^*}
            \psi x\:dx\right]^*
        \]
         Thus, \[
            2i\operatorname{Im}\int i\hbar \ddx{\psi^*}\psi x\:dx = -i\hbar. 
        \] Since $z^*z \geq b^2$, where $z = a + ib$, we must have \[
            4\left[\int i\hbar \ddx{\psi^*}\psi x\:dx\right]^2 \geq \hbar^2.
        \] Cauchy Schwarz gives \[
            \int x\psi^*\:x\psi \:dx \;\int
            \left(i\hbar\ppx{\psi^*}\right)\left(-i\hbar\ppx{\psi}\right) \:dx \geq
            \left[\int i\hbar \ddx{\psi^*}\psi x\:dx\right]^2 \geq
            \frac{\hbar^2}{4}.
        \] In other words, \[
            \E{x^2}\E{p^2} \geq \frac{\hbar^2}{4}.
        \] By writing $\sigma_x^2 = \E{x^2} - \E{x}^2$ and $\sigma_p^2 = \E{p^2} -
        \E{p}^2$, and by choosing $\E{x} = \E{p} = 0$, we arrive at the Heisenberg
        uncertainty principle, \[
            \sigma_x\sigma_p \geq \frac{\hbar}{2}. \qedhere
        \]
    \end{proof}

    Another uncertainty relation is the energy-time uncertainty, 
    \begin{boxedeq*}
        \sigma_E \sigma_t \geq \frac{\hbar}{2}.
    \end{boxedeq*}

    \subsection{Eigenstates and eigenvalues}
    Recall that the time independent Schr\"odinger equation is given by $\hat{H}\psi
    = E\psi$, which means that we seek eigenfunctions (or eigenstates) $\psi_n$ of
    the Hamiltonian, and their associated eigenvalues $E_n$. The eigenvalue $E_n$
    captures the time evolution of the associated eigenstate, since $\Psi_n = \psi_n
    e^{-iE_n t/ \hbar}$.

    \begin{definition}[Degenerate eigenstates]
        If two eigenstates $\psi_1$ and $\psi_2$ have the same energy eigenvalue,
        they are said to be degenerate with respect to one another.
    \end{definition}

    \begin{theorem}
        The energy eigenvalues of the Hamiltonian are real.
    \end{theorem}
    \begin{proof}
        This follows from the fact that the Hamiltonian is Hermitian, i.e.\
        $\hat{H}^* = \hat{H}$. Thus, if $E$ is an eigenvalue of $\hat{H}$ with
        the eigenstate $\psi$, then \[
            \hat{H}\psi = E\psi, \qquad \E{\hat{H}} = \int \psi^*\hat{H}\psi\:dx =
            E.
        \] Now, \[
            E^* = \int (\psi^*\hat{H}\psi)^*\:dx = \int \psi^*\hat{H}^*\psi\:dx = E,
        \] which forces $E \in \R$.
    \end{proof}

    \begin{theorem}
        Non-degenerate eigenstates of the Hamiltonian are orthonormal.
    \end{theorem}
    \begin{proof}
        Suppose that $\psi_1$ and $\psi_2$ are two non-degenerate eigenstates, with
        distinct eigenvalues $E_1$ and $E_2$. Then, \[
            \hat{H}\psi_1 = E_1\psi_1, \qquad \hat{H}\psi_2 = E_2\psi_2.
        \] Observe that \[
            \int \psi_1^*\hat{H}\psi_2\:dx = \int (\hat{H}^*\psi_1)^*\psi_2\:dx.
        \] Since the energy eigenvalues are real, we conclude that \[
            E_2 \int \psi_1^*\psi_2\:dx = E_1 \int \psi_1^*\psi_2\:dx.
        \] Since $E_1 \neq E_2$, the inner product must be zero.
    \end{proof}
    \begin{remark}
        If $\{\psi_i\}$ are degenerate, note that all linear combination
        also form degenerate eigenstates. \[
            \hat{H}\sum c_i\psi_i = E\sum c_i\psi_i.
        \] Thus, although $\{\psi_i\}$ may not be orthonormal, we can apply the
        Gram-Schmidt process to obtain an orthonormal basis of that eigenspace.
    \end{remark}

    \begin{theorem}
        If $[\hat{A}, \hat{B}] = 0$, then $\hat{A}$ and $\hat{B}$ share
        non-degenerate eigenstates.
    \end{theorem}
    \begin{proof}
        Let $\psi$ be a non-degenerate eigenstate of $\hat{A}$, so $\hat{A}\psi =
        \lambda \psi$. Since $\hat{A}$ and $\hat{B}$ commute, \[
            \lambda \hat{B}\psi = \hat{B}\hat{A}\psi = \hat{A}\hat{B}\psi,
        \] so $\hat{B}\psi$ is also an eigenstate of $\hat{A}$ with eigenvalue
        $\lambda$. From the non-degeneracy of $\psi$, we must have $\hat{B}\psi =
        \mu\psi$ for some non-zero scalar $\mu$.
    \end{proof}
    
    \begin{theorem}
        The eigenstates of the time independent Schr\"odinger equation form a
        complete set of states, i.e.\ they form a basis of the set of all
        solutions. \[
            \Psi(x, t) = \sum c_n \psi_n(x) e^{-i E_n t/\hbar}.
        \] In order to obtain the coefficients $c_n$, we simply take inner products
        \[
            \int \Psi_n^*\Psi\:dx = c_n.
        \] 
    \end{theorem}
    \begin{proof}
        The first statement can be regarded as a postulate. The second follows from
        the orthonormality of the eigenstates, \[
            \int \psi_n^*\psi_m\:dx = \delta_{nm}.
        \] This means that with $\Psi_n = \psi_n e^{-iE_n t / \hbar}$, we have \[
            \int \psi_n^* \left[\sum c_m\psi_m e^{-i E_m t /\hbar}\right]e^{i E_n t
            / \hbar}\:dx = 
            \sum c_me^{-i (E_m - E_n) t / \hbar}\int \psi_n^*\psi_m\:dx = c_n.
        \] 
    \end{proof}


    \section{Bound state problems}
    Here, we wish to solve the Schr\"dinger equation in one dimension with a time
    independent potential $V(x)$.

    \subsection{Piecewise constant potentials}
    If $V(x)$ is a constant over some interval, then we can write \[
        -\frac{\hbar^2}{2m}\dd[2]{\psi}{x} + (V - E)\psi = 0.
    \] Plugging in the Ansatz $\psi(x) = Ae^{kx}$, we obtain \[
        k = \pm\sqrt{\frac{2m}{\hbar^2}(V - E)}.
    \] Thus, our solution looks like \[
        \psi(x) = Ae^{kx} + Be^{-kx}, \qquad 
        \psi(x) = C\cos{\bar{k}x} + D\sin{\bar{k}x}
    \] depending on whether $V > E$ or $V < E$.
    We can stitch together these solutions over all such intervals by demanding the
    continuity of $\psi$ and $d\psi /dx$.

    \subsection{Infinite square well (particle in a box)}
    Consider a potential of the form \[
        V(x) = \begin{cases}
            0, &\text{ if }0 < x < a \\
            \infty &\text{ otherwise }
        \end{cases}
    \] Note that this forces $\psi(x) = 0$ outside the well where the potential is
    infinite. Now, setting $k = \sqrt{2mE} / \hbar$, we have the solution inside the
    well given by \[
        \psi(x) = A\cos{kx} + B\sin{kx}.
    \] We further demand $\psi(0) = \psi(a) = 0$, which forces $A = 0$ and $k = n\pi
    /a$ for integral values of $n$. Thus, we have \[
        \psi_n(x) = \sqrt{\frac{2}{a}}\sin{\frac{n\pi x}{a}}, \qquad
        E_n = \frac{n^2\pi^2\hbar^2}{2ma^2}.
    \] The factor of $\sqrt{2 / a}$ is required to normalize $\psi_n$.
    
    

\end{document}
% vim: set tabstop=4 shiftwidth=4 softtabstop=4:
