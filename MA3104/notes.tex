\documentclass[11pt]{article}

\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{amsmath, amssymb, amsthm}
\usepackage[scr]{rsfso}
\usepackage[%
    hidealllines=true,%
    innerbottommargin=15,%
    nobreak=true,%
]{mdframed}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{hyperref}

\geometry{a4paper, margin=1in, headheight=14pt}

\pagestyle{fancy}
\fancyhf{}
\renewcommand\headrulewidth{0.4pt}
\fancyhead[L]{\scshape MA3104: Linear Algebra II}
\fancyhead[R]{\scshape \leftmark}
\rfoot{\footnotesize\it Updated on \today}
\cfoot{\thepage}

\newcommand{\C}{\mathbb{C}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\F}{\mathbb{F}}

\newcommand{\I}{I}

\renewcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\vx}{\vec{x}}
\newcommand{\vy}{\vec{y}}
\newcommand{\vu}{\vec{u}}
\newcommand{\vv}{\vec{v}}
\newcommand{\vw}{\vec{w}}

\newcommand{\alg}[1]{\mathscr{#1}}
\newcommand{\algL}{\alg{L}}
\newcommand{\algF}{\alg{F}}

\newcommand{\ip}[2]{\langle #1, #2 \rangle}
\newcommand{\norm}[1]{\Vert #1 \Vert}

\renewcommand{\ker}{\operatorname{ker}}
\newcommand{\spn}{\operatorname{span}}
\newcommand{\im}{\operatorname{im}}
\newcommand{\adj}{\operatorname{adj}}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\trace}{\operatorname{trace}}

\newmdtheoremenv[%
    backgroundcolor=blue!10!white,%
]{theorem}{Theorem}[section]
\newmdtheoremenv[%
    backgroundcolor=violet!10!white,%
]{corollary}{Corollary}[theorem]
\newmdtheoremenv[%
    backgroundcolor=teal!10!white,%
]{lemma}[theorem]{Lemma}

\theoremstyle{definition}
\newmdtheoremenv[%
    backgroundcolor=green!10!white,%
]{definition}{Definition}[section]
\newmdtheoremenv[%
    backgroundcolor=red!10!white,%
]{exercise}{Exercise}[section]

\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{example}{Example}
\newtheorem*{solution}{Solution}

\surroundwithmdframed[%
    linecolor=black!20!white,%
    hidealllines=false,%
    innertopmargin=5,%
    innerbottommargin=10,%
    skipabove=0,%
    skipbelow=0,%
]{example}

\numberwithin{equation}{section}

\title{
    \Large\textsc{MA3104} \\
    \Huge \textbf{Linear Algebra II} \\
    \vspace{5pt}
    \Large{Autumn 2021}
}
\author{
    \large Satvik Saha
    \\\textsc{\small 19MS154}
}
\date{\normalsize
    \textit{Indian Institute of Science Education and Research, Kolkata, \\
    Mohanpur, West Bengal, 741246, India.} \\
}

\begin{document}
    \maketitle

    \tableofcontents

    \section{Linear operators on a vector space}

    \subsection{Preliminaries}
    We discuss finite dimensional vector spaces $V$ over some field $\F$, along with
    linear operators $T\colon V \to V$. We also assume that $V$ has the inner
    product $\ip{\cdot}{\cdot}$.

    \begin{theorem}
        Let $\algL(V)$ be the set of all linear operators on the vector space $V$.
        Then, $\algL(V)$ is a linear algebra over the field $\F$.
    \end{theorem}

    \subsection{Ideals in a ring}
    \begin{definition}
        Let $(R, +, \cdot)$ be a ring, where $(R, +)$ is its additive subgroup. A set
        $I \subseteq R$ is a left ideal of $R$ if $(I, +)$ is a subgroup of $(R, +)$,
        and $rx \in I$ for every $r \in R$, $x \in I$.
    \end{definition}
    \begin{example}
        Let $\Z$ be the ring of integers. For some $n \in \N$, the set $n\Z$ is an
        ideal. In fact, these are the only ideals (along with $\{0\}$).
    \end{example}

    \begin{definition}
        The principal left ideal generated by $x \in R$ is the set \[
            I_x = Rx = \{rx: r \in R\}.
        \] 
    \end{definition}
    \begin{example}
        In the ring of integers $\Z$, every ideal is a principal ideal. This follows
        directly from the fact that $(\Z, +)$ is a cyclic group, thus any subgroup is
        cyclic and generated by a single element. \\

        Let $I \subseteq \Z$ be an ideal. If $I = \{0\}$, we are done. Otherwise, let
        $n$ be the smallest positive integer in $I$ (note that if $a \in I$, then $-a
        \in I$ which means that $I$ must contain positive integers). This immediately
        gives $I \supseteq n\Z$. Now for any $m \in I$, use Euclid's Division Lemma
        to write $m = nq + r$, where $q, r \in \Z$, $0 \leq r < n$. Since $I$ is an
        ideal, $nq \in I$ hence $m - nq = r \in I$.  The minimality of $n$ in $I$
        forces $r = 0$, hence $m = nq$ and $I \subseteq n\Z$. This proves $I = n\Z$.
    \end{example}

    \begin{theorem}
        Let $\F$ be a field and let $\F[x]$ denote the ring of polynomials with
        coefficients from $\F$. Then, every ideal in $\F[x]$ is a principal ideal.
        \begin{remark}
            This is analogous to the theorem which states that every subgroup of a
            cyclic group is cyclic. Both lead to a precise definition of the greatest
            common divisor.
        \end{remark}
    \end{theorem}
    \begin{corollary}
        Let $I$ be a non-trivial ideal in $\F[x]$. Then, there exists a unique monic
        polynomial $p \in \F[x]$ (leading coefficient $1$) such that $I$ is precisely
        the principal ideal generated by $p$.
    \end{corollary}

    \subsection{Eigenvalues and eigenvectors}
    \begin{definition}
        Let $T \in \algL(V)$ and $c \in \F$. We say that $c$ is an eigenvalue or
        characteristic value of $T$ if $T\vv = c\vv$ for some non-zero $\vv \in V$.
        The vector $\vv$ is called an eigenvector of $T$.
    \end{definition}

    \begin{theorem}
        Let $T \in \algL(V)$ and $c \in \F$. The following are equivalent.
        \begin{enumerate}
            \itemsep0em 
            \item $c$ is an eigenvalue of $T$.
            \item $T - c \I$ is singular.
            \item $\det(T - c \I) = 0$.
        \end{enumerate}
    \end{theorem}

    \begin{definition}
        The polynomial $\det(T - x\I)$ is called the characteristic polynomial
        of $T$.
    \end{definition}
    
    \begin{definition}
        Two linear operators $S, T \in \algL(V)$ are similar if there exists an
        invertible operator $X \in \algL(V)$ such that $S = X^{-1} T X$.

        \begin{remark}
            Similarity is an equivalence relation on $\algL(V)$, thus partitioning it
            into similarity classes.
        \end{remark}
    \end{definition}

    \begin{lemma}
        Similar linear operators have the same characteristic polynomial.
    \end{lemma}
    \begin{proof}
        Let $S, T$ be similar with $S = X^{-1}TX$. Then,
        \begin{align*}
            \det(S \,-\, x\I) &= \det(X^{-1}TX \,-\, xX^{-1}X) \\
                &= \det(X^{-1})\,\det(T \,-\, x\I)\,\det(X) \\
                &= \det(T \,-\, x\I). \qedhere
        \end{align*}
    \end{proof}
    
    \begin{definition}
        A linear operator $T \in \algL(V)$ is diagonalizable if there is a basis of
        $V$ consisting of eigenvectors of $T$.
        \begin{remark}
            The matrix of $T$ with respect to such a basis is diagonal.
        \end{remark}
    \end{definition}

    \begin{theorem}
        Let $T \in \algL(V)$ where $V$ is finite dimensional, let $c_1, \dots, c_k$
        be distinct eigenvalues of $T$, and let $W_i = \ker(T - c_iI)$ be the
        corresponding eigenspaces. The following are equivalent.
        \begin{enumerate}
            \itemsep0em
            \item $T$ is diagonalizable.
            \item The characteristic polynomial of $T$ is of the form \[
                f(x) = (x - c_1)^{d_1} \dots (x - c_k)^{d_k}
            \] where each $d_i = \dim{W_i}$.
            \item $\dim{V} = \dim{W_1} + \dots + \dim{W_k}$.
        \end{enumerate}
    \end{theorem}

    \subsection{Annihilating polynomials}
    
    \begin{definition}
        An polynomial $p$ such that $p(T) = 0$ for a given linear operator $T \in
        \algL(V)$ is called an annihilating polynomial of $T$.
    \end{definition}

    \begin{lemma}
        Every linear operator $T\in\algL(V)$, where $V$ is finite dimensional, has
        a non-trivial annihilating polynomial.
    \end{lemma}
    \begin{proof}
        Note that the operators $\I, T, T^2, \dots, T^{n^2} \in \algL(V)$, of
        which there are $n^2 + 1$, are linearly dependent, since $\dim{\algL(V)} =
        n^2$.
    \end{proof}

    \begin{lemma}
        The annihilating polynomials of $T$ form an ideal in $\F[x]$.
    \end{lemma}

    \begin{definition}
        The minimal polynomial of $T$ is the unique monic generator of the
        annihilating polynomials of $T$.
        \begin{remark}
            The minimal polynomial of $T$ divides all its annihilating polynomials.
        \end{remark}
    \end{definition}

    \begin{theorem}
        The minimal polynomial and characteristic polynomial of $T$ share the same
        roots, except for multiplicities.
    \end{theorem}
    \begin{proof}
        Let $p$ be the minimal polynomial of $T$ and let $f$ be its characteristic
        polynomial.

        First, let $c \in \F$ be a root of the minimal polynomial, i.e.\ $p(c) = 0$.
        The Division Algorithm guarantees \[
            p(x) = (x - c)\, q(x)
        \] for some monic polynomial $q$. By the minimality of the degree of $p$, we
        have $q(T) \neq 0$, hence there exists non-zero $\vv \in V$ such that $\vw =
        q(T)\,\vv \neq \vec{0}$. Thus, $p(T)\,\vv = \vec{0}$ gives \[
            (T - c \I)\, q(T)\, \vv = \vec{0}, \qquad T\vw = c\vw,
        \] which shows that $c$ is an eigenvalue, i.e.\ a root of the characteristic
        polynomial $f$.

        Next, suppose that $c$ is a root of the characteristic polynomial, i.e.\
        $f(c) = 0$. Thus, $c$ is an eigenvalue of $T$, hence there exists non-zero
        $\vv \in V$ such that $T\vv = c\vv$. This gives $p(T)\,\vv = p(c)\,\vv$, but
        $p(T) = 0$ identically, forcing $p(c) = 0$.
    \end{proof}

    \begin{theorem}[Cayley-Hamilton]
        The characteristic polynomial of $T$ annihilates $T$.
    \end{theorem}
    \begin{proof}
        Set $S = \adj(T - xI)$. This is a matrix with polynomial entries, satisfying
        \[
            (T - xI)S = \det(T - xI)I = f(x) I,
        \] where $f$ is the characteristic polynomial of $T$. Now, we can also
        collect the powers $x^n$ from $S$ and write \[
            S = \sum_{k = 0}^{n - 1} x^k S_k
        \] for matrices $S_k$. Now, calculate
        \begin{align*}
            f(x)I &= (T - xI)S \\
            &= (T - xI)\sum_{k = 0}^{n - 1}x^k S_k \\
            &= -x^{k}S_{k - 1} + \sum_{k = 1}^{n - 1} x^k(TS_k - S_{k - 1}) + TS_0.
        \end{align*}
        Compare coefficients with \[
            f(x)I = x^nI + a_{n - 1}x^{n - 1} + \dots + a_0I
        \] to get \[
            S_{n - 1} = -I, \qquad TS_0 = a_0I, \qquad TS_k - S_{k - 1} = a_kI \text{
            for } 1 \leq k \leq n - 1.
        \] Thus,
        \begin{align*}
            f(T) &= \sum_{k = 0}^n a_k T^k \\
            &= -T^nS_{n - 1} + \sum_{k = 1}^{n - 1} (TS_k - S_{k - 1})T^k + TS_0 \\
            &= 0. \qedhere
        \end{align*}
    \end{proof}

    \begin{corollary}
        The minimal polynomial of $T$ divides its characteristic polynomial.
    \end{corollary}
    \begin{corollary}
        The minimal polynomial of $T$ in a finite-dimensional vector space $V$ is
        at most $\dim{V}$.
    \end{corollary}

    \begin{theorem}
        The minimal polynomial for a diagonalizable linear operator $T$ in a
        finite-dimensional vector space is \[
            p(x) = (x - c_1) \dots (x - c_k),
        \] where $c_1, \dots, c_k$ are distinct eigenvalues of $T$.
    \end{theorem}
    \begin{proof}
        The diagonalizability of $T$ implies that $V$ admits a basis of eigenvectors
        of $T$. Thus, for any such eigenvector $\vv_i$, the operator $T - c_iI$ kills
        it where $c_i$ is the corresponding eigenvalue. Thus, $p(T)\vv_i$ vanishes
        for every basis vector $\vv_i$
    \end{proof}
    \begin{remark}
        The converse is also true, i.e.\ $T$ is diagonalizable if and only if the
        minimal polynomial is the product of distinct linear factors.
    \end{remark}

    \subsection{Invariant subspaces}

    \begin{definition}
        Let $T \in \algL(V)$ where $V$ is finite-dimensional, and let $W \subseteq V$
        be a subspace. We say that $W$ is invariant under $T$ if $T(W) \subseteq W$.

        If a subspace $W$ is invariant under $T$, we define the linear map
        $T_W \in\algL(W)$ as the restriction of $T$ to $W$ in the natural way, by
        setting $T_W(\vw) = T(\vw)$ for all $\vw \in W$.
    \end{definition}

    \begin{lemma}
        If $W$ is an invariant subspace under $T \in \algL(V)$, then there is a basis of
        $V$ in which $T$ has the block triangular form \[
            [T]_\beta = \begin{bmatrix}
                A & B \\ 0 & C
            \end{bmatrix},
        \] where $A$ is an $r \times r$ matrix, $r = \dim{W}$.
    \end{lemma}
    \begin{proof}
        Let $\beta_W = \{\vv_1, \dots, \vv_r\}$ be an ordered basis of $W$, and
        extend it to an ordered basis $\beta = \{\vv_1, \dots, \vv_n\}$ of $V$. Thus,
        the matrix $[T]_\beta$ has coefficients $a_{ij}$ such that \[
            T\vv_j = a_{1j}\vv_1 + \dots  + a_{rj}\vv_r + \dots + a_{nj}\vv_n.
        \] However for all $j \leq r$, $T\vv_j \in W$ by the invariance of $W$, so
        the coefficients of $\vv_{i > r}$ in the expansion of $T\vv_j$ must vanish.
        Thus, all $a_{ij} = 0$ where $i > r$, $j \leq r$.
    \end{proof}

    \begin{lemma}
        If $W$ is an invariant subspace under $T \in \algL(V)$, the characteristic
        polynomial of $T_W$ divides the characteristic polynomial of $T$, and the
        minimal polynomial of $T_W$ divides the minimal polynomial of $T$.
    \end{lemma}
    \begin{proof}
        Choose an ordered basis $\beta$ of $V$ such that \[
            [T]_\beta = \begin{bmatrix}
                A & B \\ 0 & C
            \end{bmatrix} = D.
        \] Note that the matrix of $T_W$ in the restricted basis $\beta_W$ is just
        $A$. It can be shown that \[
            \det(xI - D) = \det(xI - A)\,\det(xI - C),
        \] which immediately gives the first result.

        Now, it can also be shown that the powers of $D$ are of the form \[
            [T^k]_\beta = \begin{bmatrix}
                A^k & B_k \\ 0 & C^k
            \end{bmatrix} = D^k.
        \] Now, $T^k\vv = \vec{0}$ implies $T_W^k\vv = \vec{0}$, hence any
        polynomial which annihilates $T$ also annihilates $T_W$. This gives the
        second result.
    \end{proof}

    \begin{definition}
        Let $W$ be an invariant subspace under $T \in \algL(V)$, and let $\vv \in V$.
        We define the $T$-conductor of $\vv$ into $W$ as the set $S_T(\vv; W)$ of all
        polynomials $g$ such that $g(T)\vv \in W$.

        When $W = \{0\}$, $S_T(\vv, \{0\})$ is called the $T$-annihilator of
        $\vv$.
    \end{definition}

    \begin{lemma}
        If $W$ is invariant under $T$, then it is invariant under all polynomials of
        $T$. Thus, the conductor $S_T(\vv, W)$ is an ideal in the ring of polynomials
        $\F[x]$.
    \end{lemma}

    \begin{definition}
        If $W$ is an invariant subspace under $T \in \algL(V)$, and $\vv \in V$, then
        the unique monic generator of $S_T(\vv, W)$ is also called the $T$-conductor
        of $\vv$ into $W$.

        The unique monic generator of $S_T(\vv, \{0\})$ is also called the
        $T$-annihilator of $\vv$.
        \begin{remark}
            The $T$-annihilator of $\vv$ is the unique monic polynomial $g$ of least
            degree such that $g(T)\vv = \vec{0}$.
        \end{remark}
        \begin{remark}
            The minimal polynomial is a $T$-conductor for every $\vv \in V$, thus
            every $T$-conductor divides the minimal polynomial of $T$.
        \end{remark}
    \end{definition}

    \begin{lemma} \label{lemma:tri_span}
        Let $T \in \alg{L}(V)$ for finite-dimensional $V$, where the minimal
        polynomial of $T$ is a product of linear operators \[
            p(x) = (x - c_1)^{r_1} \dots (x - c_k)^{r_k}.
        \] Let $W$ be a proper subspace of $V$ which is invariant under $T$. Then,
        there exists a vector $\vv \in V$ such that $\vv \notin W$, and $(T - cI)\vv
        \in W$ for some eigenvalue $c$.
    \end{lemma}
    \begin{proof}
        What we must show is that the $T$-conductor of $\vv$ into $W$ is a linear
        polynomial. Choose arbitrary $\vw \in V\setminus W$, and let $g$ be the
        $T$-conductor of $\vw$ into $W$. Thus, $g$ divides the minimal polynomial of
        $T$, and hence is a product of linear factors of the form $x - c_i$ for
        eigenvalues $c_i$. Thus write \[
            g = (x - c_i)h.
        \] The minimality of $g$ ensures that $\vv = h(T)\vw \notin W$. Finally, note
        that \[
            (T - c_iI)\vv = (T - c_iI)h(T)\vw = g(T)\vw \in W. \qedhere
        \] 
    \end{proof}

    \subsection{Triangulability and diagonalizability}
    

    \begin{theorem} \label{theorem:triangulation}
        Let $T \in \alg{L}(V)$ for finite-dimensional $V$. Then, $T$ is triangulable
        if and only if the minimal polynomial is a product of linear polynomials.
    \end{theorem}
    \begin{proof}
        First suppose that the minimal polynomial is of the form \[
            p(x) = (x - c_1)^{r_1} \dots (x - c_k)^{r_k}.
        \] We want to find an ordered basis $\beta = \{\vv_1, \dots, \vv_n\}$ in
        which \[
            [T]_\beta = \begin{bmatrix}
                a_{11} & a_{12} & \cdots & a_{1n} \\
                0      & a_{22} & \cdots & a_{2n} \\
                \vdots & \vdots & \ddots & \vdots \\
                0      & 0      & \cdots & a_{nn}
            \end{bmatrix}.
        \] Thus, we demand \[
            T\vv_j = a_{1j}\vv_1 + \dots + a_{jj}\vv_j,
        \] i.e.\ each $T\vv_j$ is in the span of $\vv_1, \dots, \vv_j$.

        Apply the previous lemma on $W = \{\vec{0}\}$ to obtain $\vv_1$. Next, let
        $W_1$ be the subspace spanned by $\vv_1$ and use the lemma to obtain
        $\vv_2$. Then let $W_2$ be the subspace spanned by $\vv_1, \vv_2$ and
        use the lemma to obtain $\vv_3$, and so on. Note that at each step, the newly
        generated vector $\vv_j$ satisfies $\vv_j \notin W_{j - 1}$ and $(T -
        c_iI)\vv_j \in W_{j - 1}$, hence \[
            T\vv_j = a_{ij}\vv_1 + \dots + a_{(j-1)j}\vv_{j - 1} + c_i\vv_j
        \] as desired.

        Next, suppose that $T$ is triangulable. Thus, there is a basis in which the
        matrix of $T$ is diagonal, which immediately means that the characteristic
        polynomial is the product of linear factors $x - a_{ii}$. Furthermore, the
        diagonal elements are precisely the eigenvalues of $T$. Since the minimal
        polynomial divides the characteristic polynomial, it too is a product of
        linear polynomials.
    \end{proof}

    \begin{corollary}
        In an algebraically closed field $\F$, any $n\times n$ matrix over $\F$ is
        triangulable.
    \end{corollary}

    \begin{theorem}
        Let $T \in \alg{L}(V)$ for finite-dimensional $V$. Then, $T$ is diagonalizable
        if and only if the minimal polynomial is a product of distinct linear
        factors, i.e.\ \[
            p(x) = (x - c_1) \dots (x - c_k)
        \] where $c_i$ are distinct eigenvalues of $T$.
    \end{theorem}
    \begin{proof}
        We have already shown that if $T$ is diagonalizable, then its minimal
        polynomial must have the given form.

        Next, let the minimal polynomial of $T$ have the given form. Let $W$ be the
        subspace spanned by all eigenvectors of $V$. Suppose that $W \neq V$. Using
        the fact that $W$ is an invariant subspace under $T$ and the previous lemma,
        we find $\vv \notin W$ and an eigenvalue $c_j$ such that $\vw = (T - c_jI)\vv
        \in W$. Now, $\vw$ can be written as the sum of eigenvectors \[
            \vw = \vw_1 + \dots + \vw_k
        \] where each $T\vw_i = c_i\vw_i$. Thus for every polynomial $h$, we have \[
            h(T)\vw = h(c_1)\vw_1 + \dots + h(c_k)\vw_k \in W.
        \] Since $c_j$ is an eigenvalue of $T$, write $p = (x - c_j)q$ for some
        polynomial $q$. Further write $q - q(c_j) = (x - c_j)h$ using the Remainder
        Theorem. Thus, \[
            q(T)\vv - q(c_j)\vv = h(T)(T - c_jI)\vv = h(T)\vw \in W.
        \] Since \[
            \vec{0} = p(T)\vv = (T - c_jI)q(T)\vv,
        \] the vector $q(T)\vv$ is an eigenvector and hence in $W$. However, $\vv
        \notin W$, forcing $q(c_j) = 0$. This contradicts the fact that the factor
        $x - c_j$ appears only once in the minimal polynomial.
    \end{proof}


    \subsection{Simultaneous triangulation and diagonalization}

    \begin{definition}
        Let $V$ be a finite-dimensional vector space, and let $\algF$ be a
        family of linear operators on $V$. The family $\algF$ is said to be
        simultaneously triangulable if there exists a basis of $V$ in which every
        operator in $\algF$ is represented by an upper triangular matrix.

        An analogous definition holds for simultaneous diagonalizability.
    \end{definition}

    \begin{lemma}
        Let $\algF$ be a simultaneously diagonalizable family of linear
        operators. Then, every pair of operators from $\algF$ commute.
    \end{lemma}
    \begin{proof}
        This follows trivially from the fact that diagonal matrices commute.
    \end{proof}

    \begin{definition}
        A subspace $W$ is invariant under a family of linear operators $\algF$
        if it is invariant under every operator $T \in \algF$.
    \end{definition}

    \begin{lemma}
        Let $\algF$ be a commuting family of triangulable linear operators on
        $V$, and let $W \subset V$ be a proper subspace invariant under
        $\algF$. Then, there exists a vector $\vv \in V$ such that $\vv \notin
        W$ and $T\vv \in \spn\{\vv, W\}$ for each $T \in \algF$.
    \end{lemma}
    \begin{proof}
        We observe that we can assume that $\algF$ contains only finitely many
        operators, without loss of generality. This is because of the finite
        dimensionality of $V$, which enables us to pick a finite basis of $\algL(V)$.

        Using Lemma~\ref{lemma:tri_span}, we can find vectors $\vv_1 \notin W$ and
        $c_1$ such that $(T_1 - c_1I)\vv_1 \in W$, for $T_1 \in \algF$. Define
        \[
            V_1 = \{\vv \in V: (T_1 - c_1I)\vv \in W\}.
        \] Note that $V_1$ is a subspace which properly contains $W$. Furthermore,
        $V_1$ is invariant under $\algF$ -- this uses the fact that the operators
        from $\algF$ commute. Now, let $U_2$ be the restriction of $T_2$ to $V_1$.
        Apply the lemma the find to $U_2$, $W$, $V_1$ to obtain $\vv_2 \in V_1$,
        $\vv_2 \notin W$ such that $(U_2 - c_2I)\vv_2 \in W$. Note that $(T_i - c_i
        I) \vv_2 \in W$ for $i = 1, 2$. Construct $V_2$ as before, and repeat this
        process until we have exhausted all linear operators in $\algF$.  The final
        vector $\vv_j$ satisfies the desired properties.
    \end{proof}

    \begin{theorem}
        Let $\algF$ be a commuting family of triangulable linear operators on
        $V$. There exists an ordered basis of $V$ which simultaneously triangulates
        $\algF$.
    \end{theorem}
    \begin{proof}
        The proof is identical to that of Theorem~\ref{theorem:triangulation}.
    \end{proof}
    
    \begin{theorem}
        Let $\algF$ be a commuting family of diagonalizable linear operators on $V$.
        There exists an ordered basis of $V$ which simultaneously diagonalizes
        $\algF$.
    \end{theorem}
    \begin{proof}
        We perform induction on the dimension of $V$. The theorem is trivial when
        $\dim{V} = 1$; suppose that it holds for vector spaces of dimension less than
        $n$, and let $\dim{V} = n$. Pick $T \in \algF$ such that $T$ is not a scalar
        multiple of $I_n$. Let $c_1, \dots, c_k$ be distinct eigenvalues of $T$, and
        let $W_i$ be the corresponding eigenspaces. Each $W_i$ is invariant under all
        operators which commute with $T$. Now let $\algF_i$ be the family of
        operators from $\algF$, restricted to the invariant subspace $W_i$. Note that
        each operator in $\algF_i$ is diagonalizable. Furthermore, $\dim{W_i} <
        \dim{V}$, so the induction hypothesis says that $\algF_i$ is simultaneously
        diagonalizable; let $\beta_i$ be the corresponding basis. Each vector in
        $\beta_i$ is an eigenvector for every operator in $\algF_i$. Let $\beta$
        consist of the such vectors from all $\beta_i$ generated in this way. Since
        $T$ is diagonal, this is indeed an basis of $V$, as desired.
    \end{proof}

    \subsection{Direct sum decompositions}
    \begin{definition}
        Let $W_1, \dots, W_k$ be subspaces of $V$. We say that these $W_i$ are
        independent if \[
            \vw_1 + \dots + \vw_k = \vec{0}
        \] where $\vw_i \in W_i$ implies that each $\vw_i = \vec{0}$.
    \end{definition}

    \begin{lemma}
        If $W_1, \dots, W_k$ are independent, then each vector $\vw \in W_1 + ... +
        W_k$ has a unique representation \[
            \vw = \vw_1 + \dots + \vw_k
        \] where each $\vw_i \in W_i$.
    \end{lemma}

    \begin{definition}
        The sum of independent subspaces $W_1 + \dots + W_k$ is called a direct sum,
        denoted \[
            W_1 \oplus \dots \oplus W_k.
        \] 
    \end{definition}

    \begin{lemma}
        Let $V$ be a finite-dimensional vector space, let $W_1, \dots, W_k$ be
        subspaces of $V$, and let $W = W_1 + \dots + W_k$. Then, the following are
        equivalent. \begin{enumerate}
            \itemsep0em
            \item $W_1, \dots, W_k$ are independent.
            \item For each $2 \leq j \leq k$, \[
                W_j \cap (W_1 + \dots + W_{j - 1}) = \{\vec{0}\}.
            \]
            \item If $\beta_i$ are bases of $W_i$, then the set $\beta$ consisting of
            all these vectors is a basis of $W$.
        \end{enumerate}
    \end{lemma}

    \subsection{Projections maps}
    \begin{definition}
        A projection map on a vector space $V$ is a linear operator $E$ such that
        $E^2 = E$. In other words, $E$ is idempotent.
    \end{definition}

    \begin{lemma}
        Let $E$ be a projection map on $V$, and let $R = \im{E}$, $N = \ker{E}$.
        \begin{enumerate}
            \itemsep0em
            \item A vector $\vv \in R$ if and only if $E\vv = \vv$.
            \item Any vector $\vv \in V$ has the unique representation $\vv =
            E\vv + (\vv - E\vv)$, with $E\vv \in R$ and $\vv - E\vv \in N$.
            \item $V = R\oplus N$.
        \end{enumerate}
        \begin{remark}
            If $R$ and $N$ are two subspaces of $V$ such that $V = R\oplus N$, then
            there is exactly one projection map $E$ such that $R = \im{E}$ and $N =
            \ker{E}$. Namely, send $\vv \mapsto \vv_R$ where $\vv = \vv_R + \vv_N$ is
            the unique decomposition of $\vv$.
        \end{remark}
    \end{lemma}

    \begin{lemma}
        A projection map is trivially diagonalizable.
    \end{lemma}
    \begin{proof}
        Note that $x^2 - x = x(x -1)$ annihilates any projection map. Also note that
        any projection map restricted to its range is the identity map. Thus,
        $\trace{E} = \rank{E}$.
    \end{proof}

    \begin{lemma}
        Let $V = W_1 \oplus \dots \oplus W_k$, and let $\vv = \vv_1 + \dots + \vv_k$
        with $\vv_i \in W_i$. Define the maps $E_i$ such that $E_i\vv = \vv_i$. Then,
        each $E_i$ is the projection map along $W_i$.
        \begin{remark}
            Observe that \[
                I = E_1 + \dots + E_k.
            \] Furthermore, we have $E_iE_j = 0$ for all $i \neq j$, which means that
            $\im{E_j} \subseteq \ker{E_i}$.
        \end{remark}
    \end{lemma}

    \begin{theorem}\label{thm:direct_sum_proj}
        If $V = W_1 + \dots + W_k$, then there exist $k$ linear operators $E_1,
        \dots, E_k$ on $V$ such that 
        \begin{enumerate}
            \itemsep0em
            \item $E_i^2 = E_i$.
            \item $E_iE_j = 0$ for all $i \neq j$.
            \item $I = E_1 + \dots + E_k$.
            \item $\im{E_i} = W_i$.
        \end{enumerate}
        Conversely, if there exist linear $k$ linear operators which satisfy
        properties 1, 2, 3 and label $\im{E_i} = W_i$, then $V = W_1 \oplus \dots
        \oplus W_k$.
    \end{theorem}
    \begin{proof}
        We only need to prove the converse. Let $E_i, \dots, E_k$ satisfy the
        properties 1, 2, 3 and let $\im{E_i} = W_i$. Pick $\vv\in V$, hence \[
            \vv = I_k\vv = E_1\vv + \dots + E_k\vv \in W_1 + \dots + W_k,
        \] which shows that $V = W_1 + \dots + W_k$. We claim that this
        representation of $\vv$ is unique. In other words, suppose that \[
            \vv = \vv_1 + \dots + \vv_k
        \] where each $\vv_i \in W_i$; we claim that $\vv_i = E_i\vv$ is the only
        choice. Since $\vv_i \in W_i$, write $\vv_i = E_i\vw_i$. Then, \[
            E_j\vv = \sum_{i = 1}^k E_j \vv_i = \sum_{i = 1}^k E_j E_i\vw_i =
            E_j^2\vw_j = E_j\vw_j = \vv_j. \qedhere
        \] 
    \end{proof}

    \begin{definition}
        Let $V = W_1 \oplus \dots \oplus W_k$, and let $T \in \algL(V)$.
        Additionally, let each $W_i$ be invariant under $T$, hence $T\vv_i \in W_i$.
        Define the linear operators $T_i \in \algL(W_i)$, which are the restrictions
        of $T$ to $W_i$. Then, given any $\vv \in V$, there is a unique
        representation $\vv = \vv_1 + \dots + \vv_k$ where $\vv_i \in W_i$, so \[
            T\vv = T\vv_1 + \dots + T\vv_k = T_1\vv_1 + \dots + T_k\vv_k = \vv_1 +
            \dots + \vv_k.
        \] This representation must be unique. We say that $T$ is the direct sum of
        the linear operators $T_1, \dots, T_k$.
    \end{definition}

    \begin{lemma}
        Let $V = W_1 \oplus \dots \oplus W_k$, let $\beta_i$ be ordered basses of
        $W_i$, and let $\beta$ be the basis formed by combining all these vectors.
        Let $T \in \algL(V)$ and suppose that each $W_i$ is invariant under $T$.
        Then, by setting $[T_i]_{\beta_i} = A_i$, we have the block diagonal form \[
            [T]_\beta = \begin{bmatrix}
                A_1 & 0 & \cdots & 0 \\
                0 & A_1 & \cdots & 0 \\
                \vdots & \vdots & \ddots & \vdots \\
                0 & 0 & \cdots & A_k
            \end{bmatrix}.
        \] 
    \end{lemma}

    \begin{theorem}
        Let $V = W_1 \oplus \dots \oplus W_k$, let $E_i$ be the projections along
        $W_i$, and $T \in \algL(V)$. Then, each $W_i$ is invariant under $T$ if and
        only if $T$ commutes with each of the projections $E_i$.
    \end{theorem}
    \begin{proof}
        Suppose that $T$ commutes with each $E_i$, i.e.\ $TE_i = E_iT$. We want to
        show that each $W_i = \im{E_i}$ is invariant under $T$. Let $\vv \in W_i$,
        hence $\vv = E_i\vv$ and \[
            T\vv = TE_i\vv = E_iT\vv.
        \] Thus, $T\vv \in W_i$ as desired.

        Conversely, suppose that each $W_i$ is invariant under $T$. Pick $\vv = \vv_1
        + \dots + \vv_k \in V$ where $\vv_i \in W_i$. Set $\vw_i = T\vv_i \in W_i$,
        and compute \[
            E_iT\vv = E_iT(\vv_1 + \dots + \vv_k) = E_i(\vw_1 + \dots + \vw_k) =
            \vw_i = T\vv_i = TE_i\vv. \qedhere
        \] 
    \end{proof}

    \begin{theorem}
        Let $T\in\algL(V)$ where $V$ is a finite-dimensional vector space. If $T$ is
        diagonalizable and $c_1, \dots, c_k$ are the distinct eigenvalues of $T$,
        then there are non-zero linear operators $E_1, \dots, E_k$ on $V$ which
        satisfy the following.
        \begin{enumerate}
            \itemsep0em
            \item $T = c_1E_1 + \dots + c_kE_k$.
            \item $I = E_1 + \dots + E_k$.
            \item $E_iE_j = 0$ for all $i \neq j$.
            \item $E_i^2 = E_i$.
            \item $\im{E_i} = \ker(T - c_iI)$.
        \end{enumerate}
        Conversely, if there exist $k$ distinct scalars $c_1, \dots, c_k$ and $k$
        non-zero linear operators which satisfy properties 1, 2, 3, then $T$ is
        diagonalizable, $c_1, \dots, c_k$ are the eigenvalues of $T$, and properties
        $4, 5$ are also satisfied.
    \end{theorem}
    \begin{proof}
        Suppose that $T$ is diagonalizable, with distinct eigenvalues $c_1, \dots,
        c_k$. Let $W_i = \ker(T - c_iI)$, and note that $V = W_1 \oplus \dots \oplus
        W_k$. Let $E_1, \dots, E_k$ be the projections associated with this
        decomposition. This immediately gives us the properties 2, 3, 4, 5. To show
        that property 1 holds, pick arbitrary $\vv \in V$ and write $\vv = E_1\vv +
        \dots + E_k\vv$. Then, note that $E_i\vv$ are eigenvectors, hence \[
            T\vv = TE_1\vv + \dots + TE_k\vv = c_1E_1\vv + \dots + c_kE_k\vv.
        \] 

        Conversely, let $T \in \algL(V)$ and suppose that $c_1, \dots, c_k$ and
        non-zero $E_1, \dots, E_k$ satisfy properties 1, 2, 3. Then, note that \[
            E_i = E_iI = E_i(E_1 + \dots + E_k) = E_i^2,
        \] giving property 4. Also, \[
            TE_i = (c_1E_1 + \dots + c_kE_k)E_i = c_iE_i^2 = c_iE_i,
        \] hence $\im{E_i} \neq \{\vec{0}\}$ is an eigenspace of $T$ corresponding to
        the eigenvalue $c_i$, i.e.\ $\im{E_i} \subseteq \ker(T - c_iI)$. We claim
        that there are no other eigenvalues; suppose that $\ker(T - cI)$ is non-zero.
        Write \[
            T - cI = c_1E_1 + \dots + c_kE_k - cI = (c_1 - c)E_1 + \dots + (c_k -
            c)E_k.
        \] Pick non-zero $\vv \in V$ such that $(T - cI)\vv = 0$. Then, some $E_i\vv
        \neq \vec{0}$ (this is because the images of the projection operators are
        independent, and $I = E_1 + \dots + E_k$). On the other hand, we must have
        each $(c_i - c)E_i\vv = \vec{0}$, forcing $c = c_i$. Finally, $I = E_1 +
        \dots + E_k$ says that $V$ is the direct sum of the $\im{E_i}$, which are
        are contained within the eigenspaces of T. This means that $T$ is
        diagonalizable.

        We finally show that $\im{E_i} = \ker(T - c_iI)$. Pick $\vv \in \ker(T -
        c_iI)$, which means that \[
            (c_1 - c_i)E_1\vv + \dots + (c_k - c_i)E_k\vv = 0.
        \] By the independence of each $\im{E_i}$, each $(c_j - c_i)E_j\vv =
        \vec{0}$, or $E_j\vv = \vec{0}$ for $j \neq i$. Thus, \[
            \vv = E_1\vv + \dots + E_k\vv = E_i\vv,
        \] so $\vv \in \im{E_i}$. This proves that $\im{E_i} = \ker(T - c_iI)$.
    \end{proof}

    \begin{lemma}
        The Lagrange polynomials $p_i$ of degree $n$ form a basis of the vector space
        of polynomials of degree at most $n$. If we have $p_i(t_j) = \delta_{ij}$,
        then for any polynomial $f$ of degree $n$, we have \[
            f = \sum f(t_i)p_i.
        \] 
    \end{lemma}
    
    \begin{lemma}
        If $T$ is diagonalizable with $T = c_1E_1 + \dots + c_kE_k$ where $E_i$ are
        projections as discussed earlier, Then, for any polynomial $g$, we have \[
            g(T) = g(c_1)E_1 + \dots + g(c_k)E_k.
        \] Thus, if $p_1, \dots, p_k$ are the Lagrange polynomials corresponding to
        the points $c_1, \dots, c_k$ and we put $g = c_i$, then each $p_i(T) = E_i$.
        Thus, each $E_i$ is a polynomial in $T$.
    \end{lemma}

    \begin{theorem}[Primary Decomposition Theorem]
        Let $T \in \algL(V)$ where $V$ is finite-dimensional, and let $p$ be the
        minimal polynomial of $T$, where \[
            p = p_1^{r_1} \dots p_k^{r_k}
        \] where $p_i$ are distinct, irreducible polynomials. Let $W_i =
        \ker{p_i(T)^{r_i}}$, then 
        \begin{enumerate}
            \itemsep0em
            \item $V = W_1 \oplus \dots \oplus W_k$.
            \item Each $W_i$ is invariant under $T$.
            \item If $T_i$ is the restriction of $T$ to $W_i$, then the minimal
            polynomial of $T_i$ is $p_i^{r_i}$.
        \end{enumerate}
    \end{theorem}
    \begin{proof}
        Set \[
            f_i = \frac{p}{p_i^{r_i}} = \prod_{j \neq i} p_j^{r_j}.
        \] Since the polynomials $f_i$ are relatively prime, we can pick polynomials
        $g_i$ such that \[
            f_1g_1 + \dots + f_kg_k = 1.
        \] Note that when $i \neq j$, we have $p | f_if_j$. Set $h_i = f_ig_i$, and
        let $E_i = h_i(T)$. We have $E_1 + \dots + E_k = I$, and $E_iE_j = 0$ for $i
        \neq j$ (the $f_if_j(T)$ term contains $p(T) = 0$). This shows that $E_i$ are
        projections corresponding to some direct sum decomposition of $V$. We claim
        that $\im{E_i} = W_i$. To see this, first let $\vv \in \im{E_i}$, whence $\vv
        = E_i\vv$ so \[
            p_i(T)^{r_i}\vv = p_i(T)^{r_i}E_i\vv = p_i(T)^{r_i}f_i(T)g_i(T)\vv =
            \vec{0}.
        \] Conversely, if $\vv \in W_i$, when $p_i(T)^{r_i}\vv = \vec{0}$. Now, for
        $i \neq j$, we have $p_i^{r_i} | f_jg_j$ hence $E_j\vv = f_jg_j(T)\vv =
        \vec{0}$ for $i \neq j$. This leaves \[
            \vv = I\vv = (E_1 + \dots + E_k)\vv = E_i\vv,
        \] hence $\vv \in \im{E_i}$. This proves 1.

        It is clear that $W_i$ is invariant under $T$. Pick arbitrary $\vv \in W_i$,
        whence $\vv = E_i\vv$ so $T\vv = TE_i\vv = E_iT\vv \in W_i$. This proves
        2.

        Since $p_i(T)^{r_i} = 0$ on $W_i$, we have $p_i(T_i)^{r_i} = 0$, hence the
        minimal polynomial of $T_i$ divides $p_i^{r_i}$. Conversely, if $g(T_i) = 0$
        for some polynomial $g$, then $g(T)f_i(T) = 0$ ($g$ kills everything in
        $W_i$, while $f_i$ kills everything in the other $W_j \neq W_i$). Thus, $p =
        p_i^{r_i}f_i$ divides $gf_i$, or $p_i^{r_i}$ divides $g$. Hence, the minimal
        polynomial of $T_i$ is precisely $p_i^{r_i}$. This proves 3.
    \end{proof}

    \begin{corollary}
        Let $E_1, \dots, E_k$ be the projections associated with the primary
        decomposition of $T$. Then, each $E_i$ is a polynomial in $T$, so any
        operator which commutes with $T$ must also commute with each $E_i$. The
        subspaces $W_i$ are thus invariant under any any operator which commutes with
        $T$.
    \end{corollary}


    \begin{theorem}
        Let $T \in \algL(V)$ where $V$ is finite-dimensional, and let the minimal
        polynomial of $p$ be of the form \[
            p = (x - c_1)^{r_1} \cdots (x - c_k)^{r_k}.
        \] Then, there is a unique diagonalizable operator $D$ and a unique nilpotent
        operator $N$ such that $T = D + N$, $DN = ND$, and both are polynomials in
        $T$.
    \end{theorem}
    \begin{proof}
        Set $D = c_1E_1 + \dots + c_kE_k$, $N = T - D$. Note that $D$ is
        diagonalizable, and \[
            N = (T - c_1I)E_i + \dots + (T - c_kI)E_k.
        \] It can be shown that \[
            N^r = (T - c_1I)^rE_i + \dots + (T - c_kI)^rE_k,
        \] hence $N^r = 0$ when $r$ is equal to the maximum of the $r_i$.

        We now claim that this choice of $D$ and $N$ is unique. Let $D'$ and $N'$
        also satisfy the above properties; since $D'$ and $N'$ commute and $T = D' +
        N'$, all the operators $T, D, N, D', N'$ commute. Write $D + N = D' + N'$,
        hence \[
            D - D' = N' - N.
        \] Since $D$ and $D'$ commute, they are simultaneously diagonalizable, hence
        $D - D'$ is diagonalizable. Now, note that \[
            (N' - N)^r = \sum_{j = 0}^r \binom{r}{j} (N')^{r - j}(-N)^{j}.
        \] Since $N'$ and $N$ are both nilpotent, the right hand side is zero for
        sufficiently high $r$. In other words, $N' - N$ is nilpotent, hence so is $D
        - D'$. This forces $D = D'$, since the only nilpotent diagonalizable operator
        is the zero operator.
    \end{proof}

    \subsection{Cyclic subspaces and the Rational form}
    \begin{lemma}
        Let $T \in \algL(V)$ where $V$ is finite-dimensional, and let $\vv \in V$.
        There is a smallest invariant subspace $W$ containing $\vv$, namely the
        intersection of all invariant subspaces containing $\vv$. Then, $W$ is the
        collection of $g(T)\vv$, for all polynomials $g$.
    \end{lemma}
    \begin{proof}
        It is clear that the collection $\{g(T) \vv\}$ is a $T$-invariant
        subspace containing $\vv$. We now show that this is contained within every
        $T$-invariant subspace containing $\vv$. Let $W'$ be a $T$-invariant subspace
        containing $\vv$. Then, $T\vv \in W'$, hence all $T^k\vv \in W'$. This means
        that all polynomials $g(T)\vv \in W'$, as desired.
    \end{proof}

    \begin{definition}
        Let $T \in \algL(V)$, and $\vv \in V$. We define the $T$-cyclic subspace
        generated by $\vv$ as \[
            Z(\vv, T) = \{g(T)\vv: g\in \F[x]\}.
        \]
        If $V = Z(\vv, T)$, then $\vv$ is called a cyclic vector for $T$.
    \end{definition}

    \begin{theorem}
        Let $T \in \algL(V)$, let $\vv \in V$ be non-zero, and let $p_{\vv}$ be the
        $T$-annihilator of $\vv$. Then,
        \begin{enumerate}
            \itemsep0em
            \item $\dim{Z(\vv, T)} = \deg{p_{\vv}}$.
            \item If $\deg{p_{\vv}} = k$, then $\vv, T\vv, \dots, T^{k - 1}\vv$ forms a
            basis of $Z(\vv, T)$.
            \item If $U$ is the restriction of $T$ to $Z(\vv, T)$, then $p_{\vv}$ is
            the minimal polynomial of $U$.
        \end{enumerate}
        \begin{remark}
            If $V$ contains a $T$-cyclic vector $\vv$, then $Z(\vv, T)$, then the
            minimal polynomial of $T$ is precisely its characteristic polynomial. The
            converse of this is also true.
        \end{remark}
    \end{theorem}
    \begin{proof}
        First note that \[
            \vec{0} = p_{\vv}(T)\vv = a_kT^k\vv + a_{k - 1}T^{k - 1}\vv + \dots +
            a_0\vv.
        \] Since $a_k \neq 0$, this immediately gives $T^k\vv$ as a linear combination
        of $\vv, \dots, T^{k - 1}\vv$. Thus, $Z(\vv, T)$ is spanned by $\vv, \dots,
        T^{k - 1}\vv$. The same thing can be shown by using the Division Lemma to
        write $g = p_{\vv}q + r$ where $0 \leq \deg{r} < k$.

        We now show that $\vv, \dots, T^{k - 1}\vv$ are linearly independent. If not,
        then \[
            a_0\vv + \dots + a_{k - 1}T^{k - 1}\vv = \vec{0}
        \] for at least one $a_i \neq 0$. This contradicts the minimality of the
        degree of the $T$-annihilator of $\vv$. Thus, we have properties 1, 2.

        Note that $p_{\vv}(U) = 0$. Any polynomial of lower degree such that $p(U)\vv
        = 0$ must be the zero polynomial by the linear independence of $\vv, \dots,
        T^{k - 1}\vv$. This means that $p_{\vv}$ must be the minimal polynomial of
        $Z(\vv, T)$, proving 3.
    \end{proof}

    \begin{definition}
        Let $p$ be the following monic polynomial. \[
            p(x) = a_0 + a_1x + \dots + a_{k - 1}x^{k - 1} + x^k.
        \] The following matrix is called its companion matrix. \[
            \begin{bmatrix}
                0 & 0 &  \cdots & 0 & -a_0 \\
                1 & 0 &  \cdots & 0 & -a_1 \\
                0 & 1 &  \cdots & 0 & -a_2 \\
                \vdots & \vdots & \ddots & \vdots & \vdots \\
                0 & 0 & \cdots & 1 & -a_{k - 1}
            \end{bmatrix}.
        \] 
    \end{definition}

    \begin{lemma}
        Let $T\in \algL(V)$ such that $\vv \in V$ is a cyclic vector of $T$. Then,
        the matrix representation of $T$ in the basis $\vv, T\vv, \dots, T^{n -
        1}\vv$ is the companion matrix of the characteristic/minimal polynomial of
        $T$.
        \begin{remark}
            If $T$ is also nilpotent, then $T^n = 0$ hence the last column in our
            matrix vanishes.
        \end{remark}
    \end{lemma}

    \begin{theorem}
        Let $T \in \algL(V)$. Then, $T$ admits a cyclic vector if and only if there
        is an ordered basis of $V$ in which the matrix representation of $T$ is the
        companion matrix of its characteristic polynomial.
    \end{theorem}
    \begin{proof}
        If $T$ admits a cyclic vector $\vv$, we have already shown that the desired
        basis is $\{\vv, T\vv,$ $\dots, T^{n - 1}\vv\}$.

        Conversely, suppose that in the basis $\{\vv_0, \vv_1, \dots, \vv_{k - 1}\}$,
        the matrix representation of $T$ is the companion matrix of its
        characteristic polynomial. Then we immediately have $T\vv_0 = \vv_1$, $T\vv_1
        = \vv_2$, \dots, $T^{n - 2}\vv_{n - 2} = \vv_{n - 1}$. This immediately shows
        that $\vv_0$ is a cyclic vector of $T$.
    \end{proof}

    \begin{corollary}
        If $A$ is companion matrix of a monic polynomial $p$, then $p$ is both the
        minimal and characteristic polynomial of $A$.
    \end{corollary}

    \begin{corollary}
        If $S, T \in \algL(V)$ both have cyclic vectors in $V$, then they are similar
        if and only if they have the same characteristic polynomial.
    \end{corollary}

    \begin{definition}
        Let $T \in \algL(V)$ and let $W \subseteq V$ be a $T$-invariant subspace. We
        say that $W$ is $T$-admissible if the following condition holds: if $f(T) \vv
        \in W$ for some polynomial $f$, then there exists $\vw \in W$ such that
        $f(T)\vv = f(T)\vw$.
    \end{definition}

    \begin{theorem}[Cyclic Decomposition Theorem]
        Let $T \in \algL(V)$, and let $W_0 \subset V$ be a proper $T$-admissible
        subspace. Then, there exist non-zero vectors $\vv_1, \dots, \vv_1$, with
        respective $T$-annihilators $p_1, \dots, p_r$ such that \begin{enumerate}
            \itemsep0em
            \item $V = W_0 \oplus Z(\vv_1, T) \oplus \dots \oplus Z(\vv_r, T)$.
            \item $p_k$ divides $p_{k - 1}$.
        \end{enumerate}
        The integer $r$ and the annihilators $p_1, \dots, p_r$ are uniquely
        determined by 1 and 2.
    \end{theorem}

    \begin{corollary}
        Every $T$-admissible subspace of $V$ has a complementary $T$-invariant
        subspace.
    \end{corollary}

    \begin{corollary}
        The annihilator $p_1$ is the minimal polynomial of $T$.
    \end{corollary}
    \begin{proof}
        Choose $W_0 = \{\vec{0}\}$, hence $V$ is the direct sum of $T$-cyclic
        subspaces. Since each $p_k$ divides $p_{k - 1}$, we see that $p_1$
        annihilates every vector in $V$. Its minimality is guaranteed by the fact
        that it is the minimal polynomial of $Z(\vv_1, T)$.
    \end{proof}

    \begin{corollary}
        Given any $T \in \algL(V)$, there exists $\vv \in V$ such that its
        $T$-annihilator is the minimal polynomial of $T$.
    \end{corollary}

    \begin{corollary}
        Given, $T \in \algL(V)$, $T$ has a cyclic vector if and only if its minimal
        and characteristic polynomials are identical.
    \end{corollary}

    \begin{definition}
        Let $T \in \algL(V)$, and let $V$ be written as the direct sum of $T$-cyclic
        subspaces as described by the Cyclic Decomposition Theorem. Then, there is a
        basis of $V$ in which $T$ is represented in a block diagonal form, with each
        block being a companion matrix, with the sizes of the blocks being weakly
        decreasing. This matrix is called the rational form of $T$.
    \end{definition}

    \begin{theorem}
        Each matrix is similar to exactly one matrix in the rational form.
    \end{theorem}
    \begin{proof}
        This is guaranteed by the uniqueness of the polynomials $p_1, \dots, p_r$
        generated by the Cyclic Decomposition Theorem. Note that if two blocks happen
        to be of equal size, the divisibility property forces $p_i = p_j$ for the
        corresponding blocks, so these blocks are exactly equal.
    \end{proof}

    \begin{theorem}[Generalized Cayley-Hamilton Theorem]
        Let $T \in \algL(V)$, let $p$ be its minimal polynomial, and let $f$ be its
        characteristic polynomial. Then $p$ divides $f$, $p$ and $f$ have the same
        prime factors except for multiplicities, and if the prime factorization of
        $p$ is \[
            p = f_1^{r_1}\dots f_k^{r_k},
        \] then the prime factorization of $f$ is of the form \[
            f = f_1^{d_1}\dots f_k^{d_k}
        \] with $d_i = \dim{\ker{(f_i^{r_i})}} / \deg{f_i}$.
    \end{theorem}

    \subsection{Jordan form}
    \begin{lemma}
        The rational form of a nilpotent matrix contains only $1$'s and $0$'s on the
        lower off-diagonal. Each choice of $k_1\geq k_2\geq \dots\geq k_r \geq 1$
        with $k_1 + \dots + k_r = n$, i.e.\ each partition of $n$ completely
        determines a similarity class of nilpotent $n \times n$ matrices.

        \begin{remark}
            Note that $r = \dim{\ker{N}}$.
        \end{remark}
    \end{lemma}

    \begin{definition}
        Let $T \in \algL(V)$ such that its minimal polynomial is a product of
        linear factors, \[
            p = (x - c_1)^{r_1} \dots (x - x_k)^{r_k}.
        \] The Primary Decomposition Theorem guarantees that by defining $W_i =
        \ker{(T - c_iI)^{r_i}}$, we have $V = W_1 \oplus \dots \oplus W_k$.
        Furthermore, if $T_i$ are the restrictions of $T$ to $W_i$, the minimal
        polynomials for $T_i$ are $(x - c_i)^{r_i}$, hence $T_i = N_i + c_iI$ for
        nilpotent operators $N_i$. In a cyclic basis, each $T_i$ is the direct sum of
        matrices \[
            \begin{bmatrix}
                c_i & 0 & \dots & 0 & 0 \\
                1 & c_i & \dots & 0 & 0 \\
                \vdots & \vdots & \ddots & \vdots & \vdots \\
                0 & 0 & \dots & c_i & 0 \\
                0 & 0 & \dots & 1 & c_i
            \end{bmatrix},
        \] descending in size. These are called elementary Jordan matrices with
        characteristic value $c_i$. Since $T$ is a direct sum of each $W_i$, the
        matrix representation of $T$ in a appropriate basis is in a block diagonal
        form with eigenvalues along the diagonal, and 1's and 0's along the
        off-diagonal. This is called the Jordan form of $T$.
    \end{definition}

    \begin{theorem}
        The Jordan form of a linear operator is unique, up to permutation of the blocks.
    \end{theorem}




    \section{Inner product spaces}

    \subsection{Preliminaries}
    \begin{definition}
        Let $\F$ either the field of real or complex numbers, and let $V$ be a vector
        space over $\F$. An inner product on $V$ is a function
        $\ip{\cdot}{\cdot}\colon V \times V \to \F$ satisfying the following
        conditions. \begin{enumerate}
            \itemsep0em
            \item $\ip{\vu + \vv}{\vw} = \ip{\vu}{\vw} + \ip{\vv}{\vw}$.
            \item $\ip{\alpha\vv}{\vw} = \alpha\ip{\vv}{\vw}$.
            \item $\ip{\vv}{\vw} = \overline{\ip{\vw}{\vv}}$.
            \item $\ip{\vv}{\vv} > 0$ for all $\vv \neq \vec{0}$.
        \end{enumerate}
        \begin{remark}
            An inner product is completely determined by its real part.
        \end{remark}
    \end{definition}

    \begin{definition}
        The standard inner product on the vector space $\F^n$ is defined as \[
            \ip{\vv}{\vw} = \sum_{i = 1}^n v_i\overline{w_i}.
        \] 
    \end{definition}

    \begin{definition}
        A norm is a function $\norm{\cdot}\colon V \to \R$ if \begin{enumerate}
            \itemsep0em
            \item $\norm{\vv} \geq 0$, and $\norm{\vv} = 0$ implies $\vv = \vec{0}$.
            \item $\norm{\alpha\vv} = |\alpha|\norm{\vv}$.
            \item $\norm{\vv + \vw} \leq \norm{\vv} + \norm{\vw}$.
        \end{enumerate}
        \begin{remark}
            Any inner product induces a norm, via $\norm{\vv} =
            \sqrt{\ip{\vv}{\vv}}$.
        \end{remark}
    \end{definition}

    \begin{lemma}[Polarization identity]
        \[
            4\ip{\vv}{\vw} = \sum_{k = 1}^4 i^k\norm{\vv + i^k\vw}^2.
        \] 
    \end{lemma}

    \begin{lemma}
        A norm arises from an inner product if and only if it satisfies the
        parallelogram identity, \[
            \norm{\vv + \vw}^2 + \norm{\vv - \vw}^2 = 2(\norm{\vv}^2 + \norm{\vw}^2).
        \] 
    \end{lemma}

    \begin{lemma}
        Let $V$ be finite dimensional, with an ordered basis $\beta = \{\vv_1, \dots,
        \vv_n\}$. Then for any $\vu, \vw \in V$, we have \[
            \ip{\vu}{\vw} = \sum_{i = 1}^n \sum_{j = 1}^n u_i\overline{w_j}
            \ip{\vv_i}{\vv_j}.  \] By setting $a_{ij} = \ip{\vv_j}{\vv_i}$ and
            letting $A = [a_{ij}]$, we can
        write \[
            \ip{\vu}{\vw} = \vw^* A \vu.
        \] Note that the $\vu, \vw$ on the right hand side denote the coordinate
        column vectors in the basis $\beta$. We see that $A$ is a Hermitian matrix,
        satisfying $A^* = A$. Furthermore, $A$ is invertible because $\ip{\vu}{\vu} =
        \vu^*A\vu > 0$ for all $\vu \neq \vec{0}$. Conversely, any such matrix
        defined an inner product.
    \end{lemma}

    \begin{definition}
        A positive linear operator satisfies $\ip{T\vv}{\vv} > 0$ for all $\vv \in
        V$.
        \begin{remark}
            We will see that this conditions on $T$ implies that it is Hermitian.
        \end{remark}
    \end{definition}

    \begin{lemma}
        If $\ip{T\vv}{\vv} = 0$ for all $\vv\in V$ where $V$ is a complex inner
        product space, then $T = 0$.
    \end{lemma}
    \begin{proof}
        Expand $\ip{T(\vv + \vw)}{\vv + \vw} = 0$ and $\ip{T(\vv + i\vw)}{\vv + i\vw}
        = 0$ to conclude that $\ip{T\vv}{\vw} = 0$ for all $\vv, \vw \in V$. Setting
        $\vw = T\vv$ immediately gives the result.
    \end{proof}


    \subsection{Orthogonality}
    \begin{definition}
        Two vectors $\vv, \vw \in V$ are orthogonal if $\ip{\vv}{\vw} = 0$.
    \end{definition}

    \begin{definition}
        If $W \subset V$, then $W^\perp$ is the set of vectors which are orthogonal to
        all vectors in $W$.
    \end{definition}

    \begin{lemma}
        Let $W\subset V$ be a subspace. Then, $V = W \oplus W^\perp$.
    \end{lemma}

    \begin{theorem}
        An orthogonal set of non-zero vectors is linearly independent.
    \end{theorem}

    \begin{theorem}[Gram-Schmidt]
        Every finite-dimensional inner product space admits an orthonormal basis.
    \end{theorem}


    \subsection{Dual spaces and adjoints}
    
    \begin{lemma}
        Every member of the dual space $V^*$ is of the form $\vv \mapsto
        \ip{\vv}{\vw}$ for unique $\vw \in V$. This gives a canonical identification
        between $V$ and $V^*$.
    \end{lemma}

    \begin{definition}
        The adjoint of a linear operator $T$ on a finite-dimensional inner product
        space $V$ is the unique linear operator $T^*$ which satisfies \[
            \ip{T\vv}{\vw} = \ip{\vv}{T^*\vw}
        \] for all $\vv, \vw \in V$.
        \begin{remark}
            The matrix of $T^*$ is the conjugate transpose of the matrix of $T$,
            given an orthonormal basis.
        \end{remark}
        \begin{remark}
            An operator such that $T = T^*$ is called self-adjoint, or Hermitian.
        \end{remark}
    \end{definition}
    \begin{example}
        Given any operator $T$, we can write \[
            T = \frac{1}{2}(T + T^*) + i\cdot\frac{1}{2i}(T - T^*)
        \] where both $(T + T^2) / 2$ and $(T - T^*) / 2i$ are Hermitian.
    \end{example}
    
    \begin{lemma}
        Let $T \in \alg{L}$. Then, the subspace $W$ is invariant under $T$ if and
        only if $W^\perp$ is invariant under $T^*$.
    \end{lemma}

    \begin{definition}
        A reducing subspace of $T \in \alg{L}$ is invariant under both $T$ and $T^*$.
        \begin{remark}
            If $W$ is a reducing subspace of $T$, then $W$ and $W^\perp$ are both
            invariant under $T$,
        \end{remark}
    \end{definition}

    \begin{lemma}
        Let $V = R \oplus N$; then, there is a unique projection $E$ with range $R$
        and kernel $N$. When $N = R^\perp$, we call $E$ the orthogonal projection into
        $R$. Here, we have $E = E^*$.
    \end{lemma}
    \begin{proof}
        We prove the latter, i.e\ for all $\vv, \vw \in V$, we have \[
            \ip{E\vv}{\vw} = \ip{\vv}{E\vw}.
        \] Note that we can expand \[
            \vv = E\vv + (I - E)\vv, \qquad
            \vw = E\vw + (I - E)\vw,
        \] hence \[
            \ip{E\vv}{\vw} = \ip{E\vv}{E\vw} + \ip{E\vv}{(I - E)\vw} =
            \ip{E\vv}{E\vw},
        \] \[
            \ip{\vv}{E\vw} = \ip{E\vv}{E\vw} + \ip{(I - E)\vv}{E\vw} =
            \ip{E\vv}{E\vw}. \qedhere
        \] 
    \end{proof}
    
    \begin{lemma}
        A linear operator $T$ is Hermitian if and only if $\ip{T\vv}{\vv}$ is real
        for all $\vv \in V$.
    \end{lemma}
    \begin{corollary}
        Every eigenvalue of a Hermitian operator is real.
    \end{corollary}

    \begin{theorem}
        Hermitian operators are diagonalizable, with an orthonormal basis of
        eigenvectors.
    \end{theorem}

    \begin{definition}
        A normal operator is one which commutes with its adjoint.
    \end{definition}

    \begin{lemma}
        A linear operator $T$ is normal if and only if $\norm{T\vv} = \norm{T^*\vv}$
        for all $\vv \in V$.
    \end{lemma}
    \begin{corollary}
        Given two distinct eigenvalues of a normal operator, the corresponding
        eigenspaces are orthogonal.
    \end{corollary}

    \begin{theorem}
        Normal operators are diagonalizable, with an orthonormal basis of
        eigenvectors. Conversely, any diagonalizable operator with an orthonormal
        basis is normal.
    \end{theorem}


    \subsection{Inner product space isomorphisms}
    \begin{definition}
        A linear map $T\colon V \to W$ is an inner product space isomorphism if it is
        bijective and preserves the inner products of $V$ and $W$.
    \end{definition}

    \begin{theorem}
        Let $T\colon V \to W$, where $V$ and $W$ are finite dimensional inner product
        spaces of the same dimension. The following are equivalent. \begin{enumerate}
            \itemsep0em
            \item $T$ preserves the inner product.
            \item $T$ is an inner product space isomorphism.
            \item $T$ maps every orthonormal basis of $V$ to an orthonormal basis of
            $W$.
            \item $T$ maps some orthonormal basis of $V$ to an orthonormal basis of
            $W$.
        \end{enumerate}
    \end{theorem}
    \begin{proof}
        The implications 1 $\Rightarrow$ 2 $\Rightarrow$ 3 $\Rightarrow$ 4 are
        trivial. To show 4 $\Rightarrow$ 1, suppose that $T$ maps the orthonormal
        basis $\vv_1, \dots, \vv_n$ to $\vw_1, \dots, \vw_n$. Then given any $\vv =
        a_1\vv_1 + \dots + a_n\vv_n$, $\vu = b_1\vv_1 + \dots + b_n\vv_n$, we can
        calculate \[
            \ip{\vv}{\vu} = \sum_{i = 1}^n\sum_{j = 1}^n
            a_i\overline{b_j}\ip{\vv_i}{\vv_j} = \sum_{i = 1}^n
            a_i\overline{b_i},
        \] \[
            \ip{T\vv}{T\vu} = \sum_{i = 1}^n\sum_{j = 1}^n
            a_i\overline{b_j}\ip{T\vv_i}{T\vv_j} = \sum_{i = 1}^n\sum_{j = 1}^n
            a_i\overline{b_j}\ip{\vw_i}{\vw_j} = \sum_{i = 1}^n
            a_i\overline{b_i}. \qedhere
        \] 
    \end{proof}

    \begin{corollary}
        Two finite dimensional inner product spaces over the same field are
        isomorphic if and only if they have the same dimension.
    \end{corollary}

    \begin{lemma}
        A linear isomorphism $T\colon V \to W$ is inner product preserving if and
        only if it is norm preserving.
    \end{lemma}


    \begin{definition}
        A unitary operator on an inner product space is an isomorphism of the space
        to itself.
        \begin{remark}
            The products and inverses of unitary operators are also unitary. Thus,
            the unitary operators on an inner product space form a group.
        \end{remark}
    \end{definition}

    \begin{theorem}
        A linear operator $U$ is unitary if and only if its adjoint $U^*$ exists and
        $U^*U = UU^* = I$.
    \end{theorem}
    \begin{proof}
        We have $U^* = U^{-1}$, since \[
            \ip{U\vv}{\vw} = \ip{U\vv}{UU^{-1}\vw} = \ip{\vv}{U^{-1}\vw}.
        \] Conversely if we know that $U^*U = UU^* = I$, then we immediately get
        $U^{-1} = U^*$. To show that it is inner product preserving, write \[
            \ip{U\vv}{U\vw} = \ip{\vv}{U^*U\vw} = \ip{\vv}{\vw}. \qedhere
        \] 
    \end{proof}

    \begin{theorem}
        A linear operator is unitary if and only if its matrix representation in some
        orthonormal basis is unitary, i.e.\ satisfies $A^*A = I$.
    \end{theorem}

    \begin{definition}
        A square matrix $A$ is called orthogonal if  it satisfies $A^\top A = I$.
        \begin{remark}
            A unitary matrix is orthogonal if and only if it is real.
        \end{remark}
    \end{definition}


    \begin{theorem}
        For every complex, invertible $n \times n$ matrix $B$, there exists a unique
        lower triangular $M$ with positive real entries on the main diagonal such
        that $MB$ is unitary.
    \end{theorem}
    \begin{proof}
        The rows of $B$ are linearly independent, and hence form a basis of $\C^n$.
        Perform Gram-Schmidt orthonormalization on $B$, and simply let $M$ be the
        matrix of this transformation.
    \end{proof}


    \begin{definition}
        Two complex matrices are said to be unitarily equivalent if they are
        conjugate via a unitary matrix.
        \begin{remark}
            The analogous definition for orthogonally equivalent matrices applies
            for real matrices.
        \end{remark}
    \end{definition}

    



\end{document}
