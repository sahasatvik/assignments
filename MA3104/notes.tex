\documentclass[11pt]{article}

\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{amsmath, amssymb, amsthm}
\usepackage[scr]{rsfso}
\usepackage[%
    hidealllines=true,%
    innerbottommargin=15,%
    nobreak=true,%
]{mdframed}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{hyperref}

\geometry{a4paper, margin=1in, headheight=14pt}

\pagestyle{fancy}
\fancyhf{}
\renewcommand\headrulewidth{0.4pt}
\fancyhead[L]{\scshape MA3104: Linear Algebra II}
\fancyhead[R]{\scshape \leftmark}
\rfoot{\footnotesize\it Updated on \today}
\cfoot{\thepage}

\newcommand{\C}{\mathbb{C}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\F}{\mathbb{F}}

\newcommand{\I}{I}

\renewcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\vx}{\vec{x}}
\newcommand{\vy}{\vec{y}}
\newcommand{\vv}{\vec{v}}
\newcommand{\vw}{\vec{w}}

\newcommand{\alg}[1]{\mathscr{#1}}
\newcommand{\algL}{\alg{L}}
\newcommand{\algF}{\alg{F}}

\newcommand{\ip}[2]{\langle #1, #2 \rangle}

\renewcommand{\ker}{\operatorname{ker}}
\newcommand{\span}{\operatorname{span}}
\newcommand{\im}{\operatorname{im}}
\newcommand{\dim}{\operatorname{dim}}
\newcommand{\adj}{\operatorname{adj}}

\newmdtheoremenv[%
    backgroundcolor=blue!10!white,%
]{theorem}{Theorem}[section]
\newmdtheoremenv[%
    backgroundcolor=violet!10!white,%
]{corollary}{Corollary}[theorem]
\newmdtheoremenv[%
    backgroundcolor=teal!10!white,%
]{lemma}[theorem]{Lemma}

\theoremstyle{definition}
\newmdtheoremenv[%
    backgroundcolor=green!10!white,%
]{definition}{Definition}[section]
\newmdtheoremenv[%
    backgroundcolor=red!10!white,%
]{exercise}{Exercise}[section]

\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{example}{Example}
\newtheorem*{solution}{Solution}

\surroundwithmdframed[%
    linecolor=black!20!white,%
    hidealllines=false,%
    innertopmargin=5,%
    innerbottommargin=10,%
    skipabove=0,%
    skipbelow=0,%
]{example}

\numberwithin{equation}{section}

\title{
    \Large\textsc{MA3104} \\
    \Huge \textbf{Linear Algebra II} \\
    \vspace{5pt}
    \Large{Autumn 2021}
}
\author{
    \large Satvik Saha
    \\\textsc{\small 19MS154}
}
\date{\normalsize
    \textit{Indian Institute of Science Education and Research, Kolkata, \\
    Mohanpur, West Bengal, 741246, India.} \\
}

\begin{document}
    \maketitle

    \tableofcontents

    \section{Linear operators on a vector space}

    \subsection{Preliminaries}
    We discuss finite dimensional vector spaces $V$ over some field $\F$, along with
    linear operators $T\colon V \to V$. We also assume that $V$ has the inner
    product $\ip{\cdot}{\cdot}$.

    \begin{theorem}
        Let $\algL(V)$ be the set of all linear operators on the vector space $V$.
        Then, $\algL(V)$ is a linear algebra over the field $\F$.
    \end{theorem}

    \subsection{Ideals in a ring}
    \begin{definition}
        Let $(R, +, \cdot)$ be a ring, where $(R, +)$ is its additive subgroup. A set
        $I \subseteq R$ is a left ideal of $R$ if $(I, +)$ is a subgroup of $(R, +)$,
        and $rx \in I$ for every $r \in R$, $x \in I$.
    \end{definition}
    \begin{example}
        Let $\Z$ be the ring of integers. For some $n \in \N$, the set $n\Z$ is an
        ideal. In fact, these are the only ideals (along with $\{0\}$).
    \end{example}

    \begin{definition}
        The principal left ideal generated by $x \in R$ is the set \[
            I_x = Rx = \{rx: r \in R\}.
        \] 
    \end{definition}
    \begin{example}
        In the ring of integers $\Z$, every ideal is a principal ideal. This follows
        directly from the fact that $(\Z, +)$ is a cyclic group, thus any subgroup is
        cyclic and generated by a single element. \\

        Let $I \subseteq \Z$ be an ideal. If $I = \{0\}$, we are done. Otherwise, let
        $n$ be the smallest positive integer in $I$ (note that if $a \in I$, then $-a
        \in I$ which means that $I$ must contain positive integers). This immediately
        gives $I \supseteq n\Z$. Now for any $m \in I$, use Euclid's Division Lemma
        to write $m = nq + r$, where $q, r \in \Z$, $0 \leq r < n$. Since $I$ is an
        ideal, $nq \in I$ hence $m - nq = r \in I$.  The minimality of $n$ in $I$
        forces $r = 0$, hence $m = nq$ and $I \subseteq n\Z$. This proves $I = n\Z$.
    \end{example}

    \begin{theorem}
        Let $\F$ be a field and let $\F[x]$ denote the ring of polynomials with
        coefficients from $\F$. Then, every ideal in $\F[x]$ is a principal ideal.
        \begin{remark}
            This is analogous to the theorem which states that every subgroup of a
            cyclic group is cyclic. Both lead to a precise definition of the greatest
            common divisor.
        \end{remark}
    \end{theorem}
    \begin{corollary}
        Let $I$ be a non-trivial ideal in $\F[x]$. Then, there exists a unique monic
        polynomial $p \in \F[x]$ (leading coefficient $1$) such that $I$ is precisely
        the principal ideal generated by $p$.
    \end{corollary}

    \subsection{Eigenvalues and eigenvectors}
    \begin{definition}
        Let $T \in \algL(V)$ and $c \in \F$. We say that $c$ is an eigenvalue or
        characteristic value of $T$ if $T\vv = c\vv$ for some non-zero $\vv \in V$.
        The vector $\vv$ is called an eigenvector of $T$.
    \end{definition}

    \begin{theorem}
        Let $T \in \algL(V)$ and $c \in \F$. The following are equivalent.
        \begin{enumerate}
            \itemsep0em 
            \item $c$ is an eigenvalue of $T$.
            \item $T - c \I$ is singular.
            \item $\det(T - c \I) = 0$.
        \end{enumerate}
    \end{theorem}

    \begin{definition}
        The polynomial $\det(T - x\I)$ is called the characteristic polynomial
        of $T$.
    \end{definition}
    
    \begin{definition}
        Two linear operators $S, T \in \algL(V)$ are similar if there exists an
        invertible operator $X \in \algL(V)$ such that $S = X^{-1} T X$.

        \begin{remark}
            Similarity is an equivalence relation on $\algL(V)$, thus partitioning it
            into similarity classes.
        \end{remark}
    \end{definition}

    \begin{lemma}
        Similar linear operators have the same characteristic polynomial.
    \end{lemma}
    \begin{proof}
        Let $S, T$ be similar with $S = X^{-1}TX$. Then,
        \begin{align*}
            \det(S \,-\, x\I) &= \det(X^{-1}TX \,-\, xX^{-1}X) \\
                &= \det(X^{-1})\,\det(T \,-\, x\I)\,\det(X) \\
                &= \det(T \,-\, x\I). \qedhere
        \end{align*}
    \end{proof}
    
    \begin{definition}
        A linear operator $T \in \algL(V)$ is diagonalizable if there is a basis of
        $V$ consisting of eigenvectors of $T$.
        \begin{remark}
            The matrix of $T$ with respect to such a basis is diagonal.
        \end{remark}
    \end{definition}

    \begin{theorem}
        Let $T \in \algL(V)$ where $V$ is finite dimensional, let $c_1, \dots, c_k$
        be distinct eigenvalues of $T$, and let $W_i = \ker(T - c_iI)$ be the
        corresponding eigenspaces. The following are equivalent.
        \begin{enumerate}
            \itemsep0em
            \item $T$ is diagonalizable.
            \item The characteristic polynomial of $T$ is of the form \[
                f(x) = (x - c_1)^{d_1} \dots (x - c_k)^{d_k}
            \] where each $d_i = \dim{W_i}$.
            \item $\dim{V} = \dim{W_1} + \dots + \dim{W_k}$.
        \end{enumerate}
    \end{theorem}

    \subsection{Annihilating polynomials}
    
    \begin{definition}
        An polynomial $p$ such that $p(T) = 0$ for a given linear operator $T \in
        \algL(V)$ is called an annihilating polynomial of $T$.
    \end{definition}

    \begin{lemma}
        Every linear operator $T\in\algL(V)$, where $V$ is finite dimensional, has
        a non-trivial annihilating polynomial.
    \end{lemma}
    \begin{proof}
        Note that the operators $\I, T, T^2, \dots, T^{n^2} \in \algL(V)$, of
        which there are $n^2 + 1$, are linearly dependent, since $\dim{\algL(V)} =
        n^2$.
    \end{proof}

    \begin{lemma}
        The annihilating polynomials of $T$ form an ideal in $\F[x]$.
    \end{lemma}

    \begin{definition}
        The minimal polynomial of $T$ is the unique monic generator of the
        annihilating polynomials of $T$.
        \begin{remark}
            The minimal polynomial of $T$ divides all its annihilating polynomials.
        \end{remark}
    \end{definition}

    \begin{theorem}
        The minimal polynomial and characteristic polynomial of $T$ share the same
        roots, except for multiplicities.
    \end{theorem}
    \begin{proof}
        Let $p$ be the minimal polynomial of $T$ and let $f$ be its characteristic
        polynomial.

        First, let $c \in \F$ be a root of the minimal polynomial, i.e.\ $p(c) = 0$.
        The Division Algorithm guarantees \[
            p(x) = (x - c)\, q(x)
        \] for some monic polynomial $q$. By the minimality of the degree of $p$, we
        have $q(T) \neq 0$, hence there exists non-zero $\vv \in V$ such that $\vw =
        q(T)\,\vv \neq \vec{0}$. Thus, $p(T)\,\vv = \vec{0}$ gives \[
            (T - c \I)\, q(T)\, \vv = \vec{0}, \qquad T\vw = c\vw,
        \] which shows that $c$ is an eigenvalue, i.e.\ a root of the characteristic
        polynomial $f$.

        Next, suppose that $c$ is a root of the characteristic polynomial, i.e.\
        $f(c) = 0$. Thus, $c$ is an eigenvalue of $T$, hence there exists non-zero
        $\vv \in V$ such that $T\vv = c\vv$. This gives $p(T)\,\vv = p(c)\,\vv$, but
        $p(T) = 0$ identically, forcing $p(c) = 0$.
    \end{proof}

    \begin{theorem}[Cayley-Hamilton]
        The characteristic polynomial of $T$ annihilates $T$.
    \end{theorem}
    \begin{proof}
        Set $S = \adj(T - xI)$. This is a matrix with polynomial entries, satisfying
        \[
            (T - xI)S = \det(T - xI)I = f(x) I,
        \] where $f$ is the characteristic polynomial of $T$. Now, we can also
        collect the powers $x^n$ from $S$ and write \[
            S = \sum_{k = 0}^{n - 1} x^k S_k
        \] for matrices $S_k$. Now, calculate
        \begin{align*}
            f(x)I &= (T - xI)S \\
            &= (T - xI)\sum_{k = 0}^{n - 1}x^k S_k \\
            &= -x^{k}S_{k - 1} + \sum_{k = 1}^{n - 1} x^k(TS_k - S_{k - 1}) + TS_0.
        \end{align*}
        Compare coefficients with \[
            f(x)I = x^nI + a_{n - 1}x^{n - 1} + \dots + a_0I
        \] to get \[
            S_{n - 1} = -I, \qquad TS_0 = a_0I, \qquad TS_k - S_{k - 1} = a_kI \text{
            for } 1 \leq k \leq n - 1.
        \] Thus,
        \begin{align*}
            f(T) &= \sum_{k = 0}^n a_k T^k \\
            &= -T^nS_{n - 1} + \sum_{k = 1}^{n - 1} (TS_k - S_{k - 1})T^k + TS_0 \\
            &= 0. \qedhere
        \end{align*}
    \end{proof}

    \begin{corollary}
        The minimal polynomial of $T$ divides its characteristic polynomial.
    \end{corollary}
    \begin{corollary}
        The minimal polynomial of $T$ in a finite-dimensional vector space $V$ is
        at most $\dim{V}$.
    \end{corollary}

    \begin{theorem}
        The minimal polynomial for a diagonalizable linear operator $T$ in a
        finite-dimensional vector space is \[
            p(x) = (x - c_1) \dots (x - c_k),
        \] where $c_1, \dots, c_k$ are distinct eigenvalues of $T$.
    \end{theorem}
    \begin{proof}
        The diagonalizability of $T$ implies that $V$ admits a basis of eigenvectors
        of $T$. Thus, for any such eigenvector $\vv_i$, the operator $T - c_iI$ kills
        it where $c_i$ is the corresponding eigenvalue. Thus, $p(T)\vv_i$ vanishes
        for every basis vector $\vv_i$
    \end{proof}
    \begin{remark}
        The converse is also true, i.e.\ $T$ is diagonalizable if and only if the
        minimal polynomial is the product of distinct linear factors.
    \end{remark}

    \subsection{Invariant subspaces}

    \begin{definition}
        Let $T \in \algL(V)$ where $V$ is finite-dimensional, and let $W \subseteq V$
        be a subspace. We say that $W$ is invariant under $T$ if $T(W) \subseteq W$.

        If a subspace $W$ is invariant under $T$, we define the linear map
        $T_W \in\algL(W)$ as the restriction of $T$ to $W$ in the natural way, by
        setting $T_W(\vw) = T(\vw)$ for all $\vw \in W$.
    \end{definition}

    \begin{lemma}
        If $W$ is an invariant subspace under $T \in \algL(V)$, then there is a basis of
        $V$ in which $T$ has the block triangular form \[
            [T]_\beta = \begin{bmatrix}
                A & B \\ 0 & C
            \end{bmatrix},
        \] where $A$ is an $r \times r$ matrix, $r = \dim{W}$.
    \end{lemma}
    \begin{proof}
        Let $\beta_W = \{\vv_1, \dots, \vv_r\}$ be an ordered basis of $W$, and
        extend it to an ordered basis $\beta = \{\vv_1, \dots, \vv_n\}$ of $V$. Thus,
        the matrix $[T]_\beta$ has coefficients $a_{ij}$ such that \[
            T\vv_j = a_{1j}\vv_1 + \dots  + a_{rj}\vv_r + \dots + a_{nj}\vv_n.
        \] However for all $j \leq r$, $T\vv_j \in W$ by the invariance of $W$, so
        the coefficients of $\vv_{i > r}$ in the expansion of $T\vv_j$ must vanish.
        Thus, all $a_{ij} = 0$ where $i > r$, $j \leq r$.
    \end{proof}

    \begin{lemma}
        If $W$ is an invariant subspace under $T \in \algL(V)$, the characteristic
        polynomial of $T_W$ divides the characteristic polynomial of $T$, and the
        minimal polynomial of $T_W$ divides the minimal polynomial of $T$.
    \end{lemma}
    \begin{proof}
        Choose an ordered basis $\beta$ of $V$ such that \[
            [T]_\beta = \begin{bmatrix}
                A & B \\ 0 & C
            \end{bmatrix} = D.
        \] Note that the matrix of $T_W$ in the restricted basis $\beta_W$ is just
        $A$. It can be shown that \[
            \det(xI - D) = \det(xI - A)\,\det(xI - C),
        \] which immediately gives the first result.

        Now, it can also be shown that the powers of $D$ are of the form \[
            [T^k]_\beta = \begin{bmatrix}
                A^k & B_k \\ 0 & C^k
            \end{bmatrix} = D^k.
        \] Now, $T^k\vv = \vec{0}$ implies $T_W^k\vv = \vec{0}$, hence any
        polynomial which annihilates $T$ also annihilates $T_W$. This gives the
        second result.
    \end{proof}

    \begin{definition}
        Let $W$ be an invariant subspace under $T \in \algL(V)$, and let $\vv \in V$.
        We define the $T$-conductor of $\vv$ into $W$ as the set $S_T(\vv; W)$ of all
        polynomials $g$ such that $g(T)\vv \in W$.

        When $W = \{0\}$, $S_T(\vv, \{0\})$ is called the $T$-annihilator of
        $\vv$.
    \end{definition}

    \begin{lemma}
        If $W$ is invariant under $T$, then it is invariant under all polynomials of
        $T$. Thus, the conductor $S_T(\vv, W)$ is an ideal in the ring of polynomials
        $\F[x]$.
    \end{lemma}

    \begin{definition}
        If $W$ is an invariant subspace under $T \in \algL(V)$, and $\vv \in V$, then
        the unique monic generator of $S_T(\vv, W)$ is also called the $T$-conductor
        of $\vv$ into $W$.

        The unique monic generator of $S_T(\vv, \{0\})$ is also called the
        $T$-annihilator of $\vv$.
        \begin{remark}
            The $T$-annihilator of $\vv$ is the unique monic polynomial $g$ of least
            degree such that $g(T)\vv = \vec{0}$.
        \end{remark}
        \begin{remark}
            The minimal polynomial is a $T$-conductor for every $\vv \in V$, thus
            every $T$-conductor divides the minimal polynomial of $T$.
        \end{remark}
    \end{definition}

    \begin{lemma} \label{lemma:tri_span}
        Let $T \in \alg{L}(V)$ for finite-dimensional $V$, where the minimal
        polynomial of $T$ is a product of linear operators \[
            p(x) = (x - c_1)^{r_1} \dots (x - c_k)^{r_k}.
        \] Let $W$ be a proper subspace of $V$ which is invariant under $T$. Then,
        there exists a vector $\vv \in V$ such that $\vv \notin W$, and $(T - cI)\vv
        \in W$ for some eigenvalue $c$.
    \end{lemma}
    \begin{proof}
        What we must show is that the $T$-conductor of $\vv$ into $W$ is a linear
        polynomial. Choose arbitrary $\vw \in V\setminus W$, and let $g$ be the
        $T$-conductor of $\vw$ into $W$. Thus, $g$ divides the minimal polynomial of
        $T$, and hence is a product of linear factors of the form $x - c_i$ for
        eigenvalues $c_i$. Thus write \[
            g = (x - c_i)h.
        \] The minimality of $g$ ensures that $\vv = h(T)\vw \notin W$. Finally, note
        that \[
            (T - c_iI)\vv = (T - c_iI)h(T)\vw = g(T)\vw \in W. \qedhere
        \] 
    \end{proof}

    \subsection{Triangulability and diagonalizability}
    

    \begin{theorem} \label{theorem:triangulation}
        Let $T \in \alg{L}(V)$ for finite-dimensional $V$. Then, $T$ is triangulable
        if and only if the minimal polynomial is a product of linear polynomials.
    \end{theorem}
    \begin{proof}
        First suppose that the minimal polynomial is of the form \[
            p(x) = (x - c_1)^{r_1} \dots (x - c_k)^{r_k}.
        \] We want to find an ordered basis $\beta = \{\vv_1, \dots, \vv_n\}$ in
        which \[
            [T]_\beta = \begin{bmatrix}
                a_{11} & a_{12} & \cdots & a_{1n} \\
                0      & a_{22} & \cdots & a_{2n} \\
                \vdots & \vdots & \ddots & \vdots \\
                0      & 0      & \cdots & a_{nn}
            \end{bmatrix}.
        \] Thus, we demand \[
            T\vv_j = a_{1j}\vv_1 + \dots + a_{jj}\vv_j,
        \] i.e.\ each $T\vv_j$ is in the span of $\vv_1, \dots, \vv_j$.

        Apply the previous lemma on $W = \{\vec{0}\}$ to obtain $\vv_1$. Next, let
        $W_1$ be the subspace spanned by $\vv_1$ and use the lemma to obtain
        $\vv_2$. Then let $W_2$ be the subspace spanned by $\vv_1, \vv_2$ and
        use the lemma to obtain $\vv_3$, and so on. Note that at each step, the newly
        generated vector $\vv_j$ satisfies $\vv_j \notin W_{j - 1}$ and $(T -
        c_iI)\vv_j \in W_{j - 1}$, hence \[
            T\vv_j = a_{ij}\vv_1 + \dots + a_{(j-1)j}\vv_{j - 1} + c_i\vv_j
        \] as desired.

        Next, suppose that $T$ is triangulable. Thus, there is a basis in which the
        matrix of $T$ is diagonal, which immediately means that the characteristic
        polynomial is the product of linear factors $x - a_{ii}$. Furthermore, the
        diagonal elements are precisely the eigenvalues of $T$. Since the minimal
        polynomial divides the characteristic polynomial, it too is a product of
        linear polynomials.
    \end{proof}

    \begin{corollary}
        In an algebraically closed field $\F$, any $n\times n$ matrix over $\F$ is
        triangulable.
    \end{corollary}

    \begin{theorem}
        Let $T \in \alg{L}(V)$ for finite-dimensional $V$. Then, $T$ is diagonalizable
        if and only if the minimal polynomial is a product of distinct linear
        factors, i.e.\ \[
            p(x) = (x - c_1) \dots (x - c_k)
        \] where $c_i$ are distinct eigenvalues of $T$.
    \end{theorem}
    \begin{proof}
        We have already shown that if $T$ is diagonalizable, then its minimal
        polynomial must have the given form.

        Next, let the minimal polynomial of $T$ have the given form. Let $W$ be the
        subspace spanned by all eigenvectors of $V$. Suppose that $W \neq V$. Using
        the fact that $W$ is an invariant subspace under $T$ and the previous lemma,
        we find $\vv \notin W$ and an eigenvalue $c_j$ such that $\vw = (T - c_jI)\vv
        \in W$. Now, $\vw$ can be written as the sum of eigenvectors \[
            \vw = \vw_1 + \dots + \vw_k
        \] where each $T\vw_i = c_i\vw_i$. Thus for every polynomial $h$, we have \[
            h(T)\vw = h(c_1)\vw_1 + \dots + h(c_k)\vw_k \in W.
        \] Since $c_j$ is an eigenvalue of $T$, write $p = (x - c_j)q$ for some
        polynomial $q$. Further write $q - q(c_j) = (x - c_j)h$ using the Remainder
        Theorem. Thus, \[
            q(T)\vv - q(c_j)\vv = h(T)(T - c_jI)\vv = h(T)\vw \in W.
        \] Since \[
            \vec{0} = p(T)\vv = (T - c_jI)q(T)\vv,
        \] the vector $q(T)\vv$ is an eigenvector and hence in $W$. However, $\vv
        \notin W$, forcing $q(c_j) = 0$. This contradicts the fact that the factor
        $x - c_j$ appears only once in the minimal polynomial.
    \end{proof}


    \subsection{Simultaneous triangulation and diagonalization}

    \begin{definition}
        Let $V$ be a finite-dimensional vector space, and let $\algF$ be a
        family of linear operators on $V$. The family $\algF$ is said to be
        simultaneously triangulable if there exists a basis of $V$ in which every
        operator in $\algF$ is represented by an upper triangular matrix.

        An analogous definition holds for simultaneous diagonalizability.
    \end{definition}

    \begin{lemma}
        Let $\algF$ be a simultaneously diagonalizable family of linear
        operators. Then, every pair of operators from $\algF$ commute.
    \end{lemma}
    \begin{proof}
        This follows trivially from the fact that diagonal matrices commute.
    \end{proof}

    \begin{definition}
        A subspace $W$ is invariant under a family of linear operators $\algF$
        if it is invariant under every operator $T \in \algF$.
    \end{definition}

    \begin{lemma}
        Let $\algF$ be a commuting family of triangulable linear operators on
        $V$, and let $W \subset V$ be a proper subspace invariant under
        $\algF$. Then, there exists a vector $\vv \in V$ such that $\vv \notin
        W$ and $T\vv \in \span\{\vv, W\}$ for each $T \in \algF$.
    \end{lemma}
    \begin{proof}
        We observe that we can assume that $\algF$ contains only finitely many
        operators, without loss of generality. This is because of the finite
        dimensionality of $V$, which enables us to pick a finite basis of $\algL(V)$.

        Using Lemma~\ref{lemma:tri_span}, we can find vectors $\vv_1 \notin W$ and
        $c_1$ such that $(T_1 - c_1I)\vv_1 \in W$, for $T_1 \in \algF$. Define
        \[
            V_1 = \{\vv \in V: (T_1 - c_1I)\vv \in W\}.
        \] Note that $V_1$ is a subspace which properly contains $W$. Furthermore,
        $V_1$ is invariant under $\algF$ -- this uses the fact that the operators
        from $\algF$ commute. Now, let $U_2$ be the restriction of $T_2$ to $V_1$.
        Apply the lemma the find to $U_2$, $W$, $V_1$ to obtain $\vv_2 \in V_1$,
        $\vv_2 \notin W$ such that $(U_2 - c_2I)\vv_2 \in W$. Note that $(T_i - c_i
        I) \vv_2 \in W$ for $i = 1, 2$. Construct $V_2$ as before, and repeat this
        process until we have exhausted all linear operators in $\algF$.  The final
        vector $\vv_j$ satisfies the desired properties.
    \end{proof}

    \begin{theorem}
        Let $\algF$ be a commuting family of triangulable linear operators on
        $V$. There exists an ordered basis of $V$ which simultaneously triangulates
        $\algF$.
    \end{theorem}
    \begin{proof}
        The proof is identical to that of Theorem~\ref{theorem:triangulation}.
    \end{proof}
    
    \begin{theorem}
        Let $\algF$ be a commuting family of diagonalizable linear operators on $V$.
        There exists an ordered basis of $V$ which simultaneously diagonalizes
        $\algF$.
    \end{theorem}
    \begin{proof}
        We perform induction on the dimension of $V$. The theorem is trivial when
        $\dim{V} = 1$; suppose that it holds for vector spaces of dimension less than
        $n$, and let $\dim{V} = n$. Pick $T \in \algF$ such that $T$ is not a scalar
        multiple of $I_n$. Let $c_1, \dots, c_k$ be distinct eigenvalues of $T$, and
        let $W_i$ be the corresponding eigenspaces. Each $W_i$ is invariant under all
        operators which commute with $T$. Now let $\algF_i$ be the family of
        operators from $\algF$, restricted to the invariant subspace $W_i$. Note that
        each operator in $\algF_i$ is diagonalizable. Furthermore, $\dim{W_i} <
        \dim{V}$, so the induction hypothesis says that $\algF_i$ is simultaneously
        diagonalizable; let $\beta_i$ be the corresponding basis. Each vector in
        $\beta_i$ is an eigenvector for every operator in $\algF_i$. Let $\beta$
        consist of the such vectors from all $\beta_i$ generated in this way. Since
        $T$ is diagonal, this is indeed an basis of $V$, as desired.
    \end{proof}
    
\end{document}
