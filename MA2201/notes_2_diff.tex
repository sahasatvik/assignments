\documentclass[11pt]{article}

\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{amsmath, amssymb, amsthm}
\usepackage[scr]{rsfso}
\usepackage[%
    hidealllines=true,%
    innerbottommargin=15,%
    nobreak=true,%
]{mdframed}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{lipsum}
\usepackage{hyperref}

\geometry{a4paper, margin=1in, headheight=14pt}

\pagestyle{fancy}
\fancyhf{}
\renewcommand\headrulewidth{0.4pt}
\fancyhead[L]{\scshape MA2201: Analysis II}
\fancyhead[R]{\scshape Differentiation}
\rfoot{\footnotesize\it Updated on \today}
\cfoot{\thepage}

\def\C{\mathbb{C}}
\def\R{\mathbb{R}}
\def\Q{\mathbb{Q}}
\def\Z{\mathbb{Z}}
\def\N{\mathbb{N}}
\newcommand\ddx[1]{\frac{d #1}{d x}}
\newcommand\ddt[1]{\frac{d #1}{d t}}
\newcommand\dd[3][]{\frac{d^{#1}{#2}}{d {#3}^{#1}}}
\newcommand\ppx[1]{\frac{\partial #1}{\partial x}}
\newcommand\ppt[1]{\frac{\partial #1}{\partial t}}
\newcommand\pp[3][]{\frac{\partial^{#1}{#2}}{\partial {#3}^{#1}}}
\newcommand\norm[1]{\left\lVert#1\right\rVert}

\newcounter{module}
\setcounter{module}{2}

\newmdtheoremenv[%
    backgroundcolor=blue!10!white,%
]{theorem}{Theorem}[module]
\newmdtheoremenv[%
    backgroundcolor=violet!10!white,%
]{corollary}{Corollary}[theorem]
\newmdtheoremenv[%
    backgroundcolor=teal!10!white,%
]{lemma}[theorem]{Lemma}

\theoremstyle{definition}
\newmdtheoremenv[%
    backgroundcolor=green!10!white,%
]{definition}{Definition}[module]
\newmdtheoremenv[%
    backgroundcolor=red!10!white,%
]{exercise}{Exercise}[module]

\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{example}{Example}
\newtheorem*{solution}{Solution}

\surroundwithmdframed[%
    linecolor=black!20!white,%
    hidealllines=false,%
    innertopmargin=5,%
    innerbottommargin=10,%
    skipabove=0,%
    skipbelow=0,%
]{example}

\numberwithin{equation}{module}

\title{
    \Large\textsc{MA2201: Analysis II} \\
    % \vspace{10pt}
    \Huge \textbf{Differentiation} \\
    \vspace{5pt}
    \Large{Spring 2021}
}
\author{
    \large Satvik Saha%
    % \thanks{Email: \tt ss19ms154@iiserkol.ac.in}
    \\\textsc{\small 19MS154}
}
\date{\normalsize
    \textit{Indian Institute of Science Education and Research, Kolkata, \\
    Mohanpur, West Bengal, 741246, India.} \\
    % \vspace{10pt}
    % \today
}

\begin{document}
    \maketitle

    The origins of differential calculus lie in our attempts to approximate various
    functions using linear ones. Suppose that we have been given a curve described
    by the function $f$, and we want to \textit{locally} approximate the function
    around a point $x$ using a straight line. In other words, for a small shift $h$,
    we want to write \[
        f(x + h) \approx f(x) + kh.
    \] Here, $k$ is the slope of the straight line. In order to obtain $k$, we can
    rearrange the above to get \[
        k \approx \frac{f(x + h) - f(x)}{h}.
    \] As we pick smaller and smaller neighbourhoods of $x$, we want our
    approximation to get better and better. Thus, if such an approximation is
    possible, then the value of $k$ must stabilize. This means that the limit \[
        k = \lim_{h \to 0} \frac{f(x + h) - f(x)}{h}
    \] must exist. Note that this immediately forces the continuity of $f$, since \[
        \lim_{h \to 0} f(x + h) - f(x) = \lim_{h \to 0} h \cdot \lim_{h \to 0}
        \frac{f(x + h) - f(x)}{h} = 0k = 0,
    \] whereby $\lim_{x \to a} f(x) = f(a)$. Splitting the limit is justified
    because the individual limits exist.
    If such a limit $k$ exists, we call it the derivative of $f$ at $x$, denoted 
    $f'(x)$.
    % If the derivative of $f$ exists for all $x$ in some interval $(a, b)$, then we
    % say that $f$ is differentiable on $(a, b)$, and $f'\colon (a, b) \to \R$ is
    % called the derivative of $f$ over $(a, b)$.
    We are now able to write \[
        f(x + h) \approx f(x) + f'(x)h.
    \] 

    \begin{definition}[Derivative]
        The derivative of a function $f\colon [a, b] \to \R$ at a point $x \in [a,
        b]$ is defined as \[
            f'(x) = \lim_{h \to 0} \frac{f(x + h) - f(x)}{h},
        \] if such a limit exists. Note that we only demand a one-sided limit if $x$
        is an endpoint.
        If the derivative of $f$ exists at every point in $[a, b]$, we say that $f$
        is differentiable on $[a, b]$.
    \end{definition}
    \begin{example}
        Consider the map $x \mapsto x^n$, where $n \in \N$. Using the binomial
        theorem, we can write \[
            (x + h)^n = x^n + nx^{n - 1}h + \dots + h^n,
        \] which means that \[
            \ddx{}x^n = \lim_{h \to 0} \frac{1}{h}\left[(x + h)^n - x^n\right] =
            \lim_{h \to 0}\left[ nx^{n - 1} + \binom{n}{2}x^{n - 2}h + \dots + h^{n
            - 1}\right] = nx^{n - 1}.
        \] 
    \end{example}
    Note that the process of differentiation we described can be generalised to
    multivariable functions. The idea is to locally approximate a function with an
    affine function. \\

    \begin{theorem}
        If $f\colon (a, b) \to \R$ is differentiable on $(a, b)$, then it is also
        continuous on $(a, b)$.
    \end{theorem}
    
    \begin{theorem}
        Let $f\colon I\to \R$ be a continuous function. Then, \begin{enumerate}
            \itemsep0
            \item $f$ maps compact sets to compact sets.
            \item $f$ maps connected sets to connected sets.
        \end{enumerate}
    \end{theorem}
    \begin{corollary}
        A continuous function $f\colon I \to \R$ maps intervals to intervals.
    \end{corollary}
    \begin{corollary}
        A continuous function $f\colon [a, b] \to \R$ attains its minimum and
        maximum on $[a, b]$.
    \end{corollary}

    \begin{definition}
        Given $f \colon (a, b) \to \R$, a point $c \in (a, b)$ is said to be a point
        of local maximum if there exists a neighbourhood $I_c$ of $c$ such that \[
            f(c) > f(x),
        \] for all $x \in I_c\setminus\{c\}$.
        There is an analogous definition for a local minimum.
    \end{definition}
    \begin{theorem}
        If $f\colon (a, b) \to \R$ is differentiable and $c \in (a, b)$ is a point
        of local minimum or maximum, then $f'(c) = 0$.
        \begin{remark}
            The converse is not true. Note that the derivative of $x \mapsto x^3$
            vanishes at $x = 0$, but that is not a local minimum or maximum.
        \end{remark}
    \end{theorem}
    \begin{proof}
        Let $c$ be a local minimum or maximum of $f$, but suppose that $f'(c) \neq 0$.
        Define the function \[
            g\colon (a, b) \to \R, \qquad g(x) = \begin{cases}
                (f(x) - f(c))/(x - c), &\text{ if } x \neq c \\                
                f'(c), &\text{ if } x = c
            \end{cases}
        \] We note that $g$ is continuous. Also, $f'(c) = g(c) \neq 0$.
        If $g(c) > 0$, there exists a neighbourhood $I_\delta = (c - \delta, c +
        \delta)$ such that for all $x \in I_\delta$, $g(x) > 0$, from the continuity
        of $g$. This means that on $I_c$, \[
            \frac{f(x) - f(c)}{x - c} > 0,
        \] which gives $f(x) > f(c)$ on $(c, c + \delta)$ and $f(x) < f(c)$ on $(c -
        \delta, c)$. This means that $c$ cannot be a local minimum, nor a local
        maximum. There is an analogous case assuming $g(c) < 0$, which leads to the
        same contradiction. Thus, we must have $f'(c) = g(c) = 0$.
    \end{proof}

    \begin{theorem}[Rolle's Theorem]
        Let $f\colon [a, b] \to \R$ be continuous, and differentiable on $(a, b)$,
        with $f(a) = f(b)$. Then, there exists $c \in (a, b)$ such that $f'(c) = 0$.
    \end{theorem}
    \begin{proof}
        % Without loss of generality, set $f(a) = f(b) = 0$. This can be done because
        % a function defined as $f(x) - f(a)$ satisfies all the requirements.
        Set $f(a) = f(b) = \kappa$.
        From the continuity of $f$, note that the image of the closed interval $[a,
        b]$ is another closed interval $[\alpha, \beta]$. This means that $\alpha
        \leq \kappa \leq \beta$. Note that if $\alpha = \beta = \kappa$, then the
        function $f$ is identically equal to the constant $\kappa$, hence $f'(x) =
        0$ everywhere on $[a, b]$. By the continuity of $f$, it must attain its
        maximum and minimum on $[a, b]$. If $\beta > \kappa$, then the maximum is al
        least $\beta$ and is hence not attained at the endpoints, which means that
        the point of maximum lies in $(a, b)$. If $\alpha < \kappa$, then the same
        argument shows that $f$ attains a minimum in $(a, b)$. Thus, in either case,
        we have found $c \in (a, b)$ which is either a maximum or minimum of $f$,
        i.e.\ $f'(c) = 0$.
    \end{proof}

    \begin{theorem}[Mean Value Theorem]
        Let $f\colon [a, b] \to \R$ be continuous, and differentiable on $(a, b)$.
        Then, there exists $c \in (a, b)$ such that \[
            f(b) - f(a) = f'(c)\, (b - a).
        \] 
    \end{theorem}
    \begin{proof}
        Apply Rolle's Theorem on the function defined as \[
            g\colon [a, b] \to \R, \qquad
            g(x) = f(x) - f(a) - \frac{f(b) - f(a)}{b - a}\cdot(x - a).
        \] Note that $g$ is continuous on $[a, b]$, differentiable on $(a, b)$, and
        $g(a) = g(b) = 0$.
    \end{proof}

    \begin{theorem}
        Let $f\colon \R \to \R$ be differentiable, and $f'(x) > 0$ for all $x \in
        \R$. Then, $f$ is strictly increasing on $\R$.
    \end{theorem}
    \begin{proof}
        Let $x_2 > x_1$. By the mean value theorem, we pick $c \in (x_1, x_2)$
        such that \[
            f(x_2) - f(x_1) = f'(c) (x_2 - x_1) > 0. \qedhere
        \] 
    \end{proof}
    \begin{remark}
        The converse is not true. The map $x \mapsto x^3$ is strictly increasing,
        but its derivative vanishes at $0$.
    \end{remark}


    \begin{theorem}[Chain rule]
        Let $f$ and $g$ be differentiable on $\R$. Then, $f\circ g$ is also
        differentiable, with \[
            (f\circ g)' = (f' \circ g)\cdot g'.
        \] 
    \end{theorem}
    \begin{proof}
        Fix $a \in \R$.
        Define the functions \[
            \varphi\colon \R \to \R, \qquad \varphi(x) = \begin{cases}
                (g(x) - g(a))/(x - a) &\text{ if }x \neq a\\
                g'(a), &\text{ if } x = a
            \end{cases},
        \] \[
            \psi\colon \R \to \R, \qquad \psi(y) = \begin{cases}
                (f(y) - f(b))/(y - b) &\text{ if }y \neq b\\
                f'(b), &\text{ if } y = b
            \end{cases}.
        \] Note that $\varphi$ and $\psi$ are continuous. Also, when $x \neq a$, we
        have \[
            g(x) - g(a) = \varphi(x)(x - a).
        \] Set $b = g(a)$, and write \[
            f(g(x)) - f(g(a)) = \psi(g(x))(g(x) - g(a)) = \psi(g(x))\,\varphi(x)(x -
            a).
        \] Setting $h = f\circ g$, we have \[
            \frac{h(x) - h(a)}{x - a} = \psi(g(x))\varphi(x).
        \] Taking limits $x \to a$, we use the continuity of $\varphi$, $\psi$ and
        $g$ to conclude that the derivative of $h$ is indeed defined at $a$, and \[
            h'(a) = \psi(g(a))\,\varphi(a) = f'(g(a))\,g'(a). \qedhere
        \] 
    \end{proof}

    \begin{definition}[Intermediate Value Property]
        Let $f\colon (a, b) \to \R$ be such that for all $c, d \in (a, b)$ such that
        $f(c) < f(d)$ and $\lambda \in (f(c), f(d))$, there exists $x_0 \in (a, b)$ 
        such that $f(x_0) = \lambda$. Then, we say that $f$ has the intermediate
        value property.
    \end{definition}

    \begin{theorem}[Intermediate Value Theorem]
        All continuous functions $f\colon (a, b) \to \R$ have the intermediate value
        property.
    \end{theorem}

    \begin{theorem}
        Let $f\colon (a, b)\to \R$ be differentiable. Then, $f'$ satisfies the
        intermediate value property.
    \end{theorem}
    \begin{proof}
        Let $c, d \in (a, b)$ and let $\lambda \in \R$ such that $\lambda \in
        (f'(c), f'(d))$. We wish to find $x_0 \in (a, b)$ such that $f'(x_0) =
        \lambda$. Define \[
            g\colon (a, b) \to \R, \qquad g(x) = f(x) - \lambda x.
        \] Note that $g'(x) = f'(x) - \lambda$, so $g'(c) < 0$ and $g'(d) > 0$.
        Thus, $g$ is decreasing near $c$ and increasing near $d$, so we can find
        $t_1, t_2 \in (c, d)$ such that $g(t_1) < g(c)$ and $g(t_2) < g(d)$.
        This means that $g$ has no local minimum at $c$ nor $d$. From the continuity
        of $g$, there exists $x_0 \in [c, d]$ such that $g(x_0) = \inf_{[c, d]}
        g(x)$. We have already shown that $x_0$ is neither $c$, nor $d$, so $x_0 \in
        (c, d)$. Hence, $g'(x_0) = 0$, which gives $f'(x_0) = \lambda$.
    \end{proof}
    
    \begin{lemma}
        If $f\colon (a, b) \to (c, d)$ is surjective, continuous and
        strictly increasing, then $f$ is invertible with a continuous inverse.
    \end{lemma}
    \begin{theorem}[Inverse function theorem]
        Let $f\colon (a, b) \to (c, d)$ be surjective and differentiable, with 
        $f'(x) \neq 0$ everywhere. Then, $f$ is invertible, with a differentiable 
        inverse whose derivative is given by \[
            (f^{-1})'(f(x)) = \frac{1}{f'(x)}.
        \] 
    \end{theorem}
    \begin{proof}
        Given $f'(x) \neq 0$ on $(a, b)$. Then intermediate value property gives
        either $f'(x) > 0$ for all $x \in (a, b)$, or $f'(x) < 0$. Without loss of
        generality, assume the former. This means that $f$ is strictly increasing on
        $(a, b)$, continuous, and surjective. Our lemma gives the existence of a
        continuous inverse $f^{-1}$.

        Let $y \in (c, d)$, and let $x = f^{-1}(y)$. From the continuity of
        $f^{-1}$, we can always write $f^{-1}(y + \kappa) = x + h$. Thus, \[
            \lim_{\kappa \to 0} \frac{f^{-1}(y + \kappa) - f^{-1}(y)}{\kappa} =
            \lim_{\kappa \to 0} \frac{x + h - x}{\kappa} = \lim_{\kappa \to 0}
            \frac{h}{\kappa}.
        \] Note that $h \to 0$ as $\kappa \to 0$. Thus, this limit can be written as
        \[
            (f^{-1})'(y) = \lim_{h \to 0} \frac{h}{f(x + h) - f(x)} =
            \frac{1}{f'(x)}. \qedhere
        \]
    \end{proof}
    \begin{corollary}
        Let $f$ be continuously differentiable on $\R$, with $f'(x_0) \neq 0$ for
        some $x_0 \in \R$. Then, there exists some neighbourhood of $x_0$ on which
        $f$ is invertible, with a continuously differentiable inverse.
    \end{corollary}

    \begin{theorem}
        Let $f_n \to f$ pointwise and $\{f_n'\}$ converge uniformly on some interval
        $(a, b)$. Then, $f_n \to f$ uniformly.
    \end{theorem}
    \begin{proof}
        We show that $\{f_n\}$ is uniformly Cauchy on $E$. Note that for some
        fixed $t$, we can write\[
            |f_n(x) - f_m(x)| \leq |(f_n(x) - f_m(x)) - (f_n(t) - f_m(t))| +
            |f_n(t) - f_m(t)|.
        \] Using the Mean Value Theorem, the first term can be bounded as \[
            |(f_n(x) - f_m(x)) - (f_n(t) - f_m(t))| = (f_n' - f_m')(x_0)|x - t|,
        \] where $x_0$ is between $x$ and $t$.
        From the pointwise convergence of $f_n \to f$, we have \[
            |f_n(t) - f_m(t)| < \frac{\epsilon}{2}
        \] for all $n, m \geq N_t$. The uniform convergence of
        $\{f_n'\}$ means that we can find $N_0$ such that \[
            |f_n'(x_0) - f_m'(x_0)| < \frac{\epsilon}{2(b - a)}
        \] for all $n, m > N_0$. Thus, for all $x \in [a, b]$, and $n, m \geq
        N_t + N_0$, we have \[
            |f_n(x) - f_m(x)| < \frac{\epsilon}{2(b - a)}\cdot (b - a) +
            \frac{\epsilon}{2} = \epsilon.
        \] This means that $\{f_n\}$ is uniformly Cauchy on $[a, b]$, which
        gives the uniform convergence of $\{f_n\}$.
    \end{proof}
    \begin{remark}
        We only needed to use the pointwise convergence of $\{f_n\}$ at one point
        $t$. By using pointwise convergence everywhere, we can allow for unbounded 
        intervals, or the entirety of $\R$.
    \end{remark}

    \begin{theorem}
        Let $\{f_n\}$ be a sequence of differentiable functions on some bounded
        interval $(a, b)$ such that $f_n \to f$ pointwise and $\{f_n'\}$ converges
        uniformly on every $[\alpha, \beta] \subset (a, b)$.
        Then, $f$ is differentiable and $f_n' \to f'$.
        \begin{remark}
            We allow $a, b$ to be $\pm \infty$.
        \end{remark}
    \end{theorem}
    \begin{proof}
        Let $x_0 \in (a, b)$. We wish to show that the following limit exists. \[
            \lim_{x \to x_0} \frac{f(x) - f(x_0)}{x - x_0}.
        \] Define $\varphi\colon (a, b) \setminus \{x_0\} \to \R$, \[
            \varphi(x) = \frac{f(x) - f(x_0)}{x - x_0}.
        \] Also define the functions $\varphi_n\colon (a, b) \to \R$, \[
            \varphi_n(x) = \begin{cases}
                (f_n(x) - f_n(x_0))/(x - x_0) &\text{ if } x \neq x_0, \\
                f_n'(x_0) &\text{ if } x = x_0.
            \end{cases}
        \] Note that $\varphi_n$ are continuous, from the continuity of each $f_n$.
        When $x \neq x_0$, we see that $\varphi_n(x) \to \varphi(x)$. 
        For $x = x_0$, we know that $f_n'$ converges hence $\varphi_n(x_0)$ also 
        converges. This gives us pointwise convergence.

        We want to show that $\{\varphi_n\}$ converges uniformly. Fix $[\alpha,
        \beta] \subset (a, b)$ such that $x_0 \in (\alpha, \beta)$. For $x \neq
        x_0$, we have \[
            |\varphi_n(x) - \varphi_m(x)| = \left| \frac{(f_n(x) - f_m(x)) -
            (f_n(x_0) - f_m(x_0))}{x - x_0} \right|.
        \] Using the Mean Value Theorem on $g = f_n - f_m$ , we choose $c$ between $x$
        and $x_0$ such that $(x - x_0)g'(c) = g(x) - g(x_0)$. Thus, \[
            |\varphi_n(x) - \varphi_m(x)| = |f_n'(c) - f_m'(c)| < \epsilon
        \] for all $m, n \geq N$ for some $N$, given by the uniform convergence of
        $\{f_n'\}$. This shows that $\{\varphi_n\}$ also converges uniformly on
        $[\alpha, \beta]$. Note that when $x = x_0$, $|f_n'(x_0) - f_m'(x_0)|$ is
        similarly bounded.

        Now that $\{\varphi_n\}$ converges uniformly, we know that the limit
        function is continuous. Since it converges pointwise to $\varphi$ on $x \neq
        x_0$ and to $\lim_{n \to \infty} f_n'(x_0)$ when $x = x_0$, continuity gives
        the existence of the desired limit and \[
            \lim_{n \to \infty} f_n'(x_0) = \lim_{x \to x_0} \frac{f(x) - f(x_0)}{x
            - x_0} = f'(x_0),
        \] which gives the differentiability of $f$. Also note that $f_n' \to f'$.
    \end{proof}
    \begin{lemma}[Abel's Lemma]
        Let \[
            \sum_{x = 0}^\infty a_n(x - a)^n
        \] be a convergent power series on $(a - R, a + R)$. If the series converges
        absolutely for some $x = a + c$ within that interval, then it must converge
        uniformly on any closed interval $[\alpha, \beta] \subset (a - c, a + c)$.
    \end{lemma}
    \begin{proof}
        Note that for all $x \in [\alpha, \beta]$, \[
            |a_n(x - a)^n| \leq |a_n| |c|^n
        \] which gives the uniform convergence of $\sum_{n = 1}^\infty a_n(x - a)^n$
        on $[\alpha, \beta]$ by the Weierstrass M-test.
    \end{proof}
    \begin{lemma}
        If a power series converges absolutely on $(a - R, a + R)$, then it is
        differentiable, with the derivative being \[
            \sum_{n = 1}^\infty na_n(x - a)^{n - 1}.
        \]
    \end{lemma}
    \begin{proof}
        The absolute convergence of the power series gives its uniform convergence
        on every compact subset of $(a - R, a + R)$. Note that this gives the
        continuity of the power series. Now, note that \[
            \limsup_{n \to \infty} |na_n|^{1 / n} = \lim_{n \to \infty} n^{1
            /n}\cdot \limsup_{n \to \infty} |a_n|^{1 / n} = \limsup_{n \to \infty}
            |a_n|^{1 /n},
        \] so the series of derivatives of terms of the power series converges
        absolutely on the same domain. This again gives the uniform convergence of
        this series of derivatives of terms. Abel's Lemma gives uniform
        convergence on all compact subsets. Thus, by the previous theorem, our power
        series is differentiable, with the derivative equal to the series of
        derivatives of terms.
    \end{proof}
    \begin{corollary}
        A power series is infinitely differentiable on its interval of convergence.
    \end{corollary}
    \begin{example}
        Consider $f\colon \R \to \R$, $x \mapsto \sin{x}$. We want to show that
        $f'(x) = \cos{x}$. Write $f$ as a power series, \[
            f(x) = \sum_{n = 0}^\infty \frac{(-1)^{2n}x^{2n + 1}}{(2n + 1)!}.
        \] This converges absolutely on $\R$. Our lemma gives \[
            f'(x) = \sum_{n = 0}^\infty \frac{(-1)^{2n}x^{2n}}{(2n)!} = \cos{x}.
        \] 
    \end{example}
    \begin{example}
        Consider the function \[
            f\colon \R \to \R, \qquad 
            f(x) = \begin{cases}
                e^{- 1 / x}, &\text{ if }x > 0, \\
                0, &\text{ if } x \leq 0.
            \end{cases}
        \] This function is differentiable everywhere. This is easily seen when $x
        \neq 0$. For $x = 0$, the left hand limit is simply \[
            \lim_{x \to 0^-} \frac{f(x) - f(0)}{x} = 0,
        \] and from the positive side, \[
            \lim_{x \to 0^+} \frac{f(x) - f(0)}{x} = \lim_{x \to 0^+}
            \frac{e^{-1 /x}}{x} = 0.
        \] Hence, $f'(0) = 0$. Indeed, \[
            \lim_{x \to 0^+} \frac{e^{-1 /x}}{p(x)} = 0
        \] for any polynomial $p(x)$, which means that $f$ is infinitely
        differentiable, with $f^{(n)}(0) = 0$. This means that if $f$ has a power
        series centred at $x = 0$, all its coefficients must be identically zero.
        Thus, $f$ has no power series around $x = 0$.
    \end{example}

    \begin{definition}
        We notate the $n$\textsuperscript{th} derivative of a function $f$ as \[
            f^{(n)} = \dd[n]{f}{x} = \underbrace{\ddx{}\left(\ddx{}\left(\dots
            \ddx{f}\dots\right)\right)}_{n\text{ times }}.
        \] 
    \end{definition}
    \begin{example}
        Consider \[
            f\colon \R \to \R, \qquad x \mapsto x^m.
        \] Differentiating $n$ times, we write \[
            f^{(n)}(x) = \dd[n]{}{x}x^m = \begin{cases}
                m(m - 1)\dots(m - n + 1) x^{m - n} &\text{ if } n \leq m, \\
                0 &\text{ if } n > m.
            \end{cases}
        \] 
    \end{example}
    \begin{example}
        Consider \[
            f\colon \R \to \R, \qquad x \mapsto \sin{x}.
        \] Differentiating $n$ times, we write \[
            f^{(n)}(x) = \dd[n]{}{x}\sin{x} = \sin\left(x + \frac{n\pi}{2}\right).
        \] 
    \end{example}
    \begin{theorem}
        Let $f$ be differentiable $n$ times at $x$, and let it be of the form \[
            f = g\cdot h
        \] where $g$ and $h$ are also differentiable $n$ times at $x$. Then, \[
            f^{(n)}(x) = \sum_{k = 0}^n \binom{n}{k}g^{(k)}(x)h^{(n - k)}(x).
        \] 
    \end{theorem}
    \begin{proof}
        For $n = 1$, this is simply the product rule. Suppose that this is true for
        some $n \geq 1$. Then, \[
            f^{(n + 1)}(x) = \ddx{}f^{(n)}(x) = \sum_{k = 0}^n
            \binom{n}{k}\ddx{}\left[g^{(k)}(x)h^{(n - k)}(x)\right].
        \] Now, \[
            \ddx{}\left[g^{(k)}(x)h^{(n - k)}(x)\right] = g^{(k + 1)}(x)h^{(n -
            k)}(x) + g^{(k)}(x)h^{(n - k + 1)}(x).
        \] Plugging this back in and shifting indices, \[
            f^{(n + 1)}(x) = \sum_{k = 1}^{n + 1} \binom{n}{k -
            1}g^{(k)}(x)h^{(n - k + 1)}(x) + \sum_{k = 0}^{n} \binom{n}{k}
            g^{(k)}(x)h^{(n - k + 1)}(x).
        \] Separating the first and last terms and using \[
            \binom{n}{k - 1} + \binom{n}{k} = \binom{n + 1}{k}
        \] completes the proof by induction.
    \end{proof}

    \begin{lemma}
        Suppose that $p(x)$ is a polynomial of degree $n$, and we have been given the
        values \[
            b_i = \frac{p^{(n)}(a)}{n!}
        \] for all $i = 0, \dots, n$. Then, the polynomial is uniquely determined,
        as \[
            p(x) = b_0 + b_1(x - a) + \dots + b_n(x - a)^n.
        \] 
    \end{lemma}

    \begin{definition}[Taylor polynomial]
        Let $f\colon [a, b] \to \R$ be a continuous function such that $f', f'',
        \dots, f^{(n)}$ are also continuous. Then, for $x_0 \in (a, b)$, 
        the polynomial \[
            p(x) = \sum_{k = 0}^n \frac{f^{(k)}(x_0)(x - x_0)^k}{k!}
        \] is called the Taylor polynomial of $f$ of degree $n$ about the point
        $x_0$.
        \begin{remark}
            Note that \[
                p(x_0) = f(x_0), \quad
                p'(x_0) = f'(x_0), \quad
                p''(x_0) = f''(x_0), \;\; \dots \;\;
                p^{(n)}(x_0) = f^{(n)}(x_0).
            \] 
        \end{remark}
    \end{definition}
    \begin{example}
        Consider $f\colon (-1, \infty) \to \R$, $x \mapsto \log{(1 + x)}$.
        In order to expand this as a Taylor series about $x_0 = 0$, we calculate \[
            f'(x) = \frac{1}{1 + x}, \qquad f''(x) = -\frac{1}{(1 + x)^2},
            \quad\dots\quad,
            f^{(n)}(x) = \frac{(-1)^{n - 1}(n - 1)!}{(1 + x)^{n}}.
        \] This gives \[
            \frac{f^{(n)}(0)}{n!} = \frac{(-1)^{n - 1}}{n}.
        \] Thus, the Taylor polynomial of degree $n$ is given by \[
            p(x) = x - \frac{1}{2}x^2 + \frac{1}{3}x^3 - \dots + (-1)^{n -
            1}\frac{1}{n}x^n.
        \]  
    \end{example}

    \begin{definition}
        A polynomial $p$ is said to approximate a function $f$ up to the order $n$
        near a point $x_0$ if for every $\epsilon > 0$, there exists a $\delta$
        neighbourhood of $x_0$ where for all $k = 0, \dots, n$, \[
            |f^{(k)}(x) - p^{(k)}(x)| < \epsilon
        \]
    \end{definition}

    \begin{lemma}
        The Taylor polynomial of degree $n$ of a function $f$ approximates it up to 
        order $n$.
    \end{lemma}
    \begin{proof}
        Note that \[
            |f^{(k)}(x) - p^{(k)}(x)| \leq |f^{(k)}(x) - f^{(k)}(x_0)| + 
                |f^{(k)}(x_0) - p^{(k)}(x_0)| + |p^{(k)}(x_0) - p^{(k)}(x)|.
        \] For the Taylor polynomial, the central term is zero. The continuity of
        $f^{(k)}$ and $p^{(k)}$ allow us to control the remaining terms, giving the
        desired result.
    \end{proof}

    \begin{definition}[Remainder]
        Let $p$ approximate $f$ up to order $n$. Then, the remainder term is defined
        on the interval of approximation as \[
            R_{n + 1}(x) = f(x) - p(x).
        \] 
    \end{definition}

    \begin{definition}[Big $O$ and small $o$ notation]
        Let $f$ and $g$ be two functions on a neighbourhood of $x_0$. We say that $f
        \sim O(g)$ near $x_0$ if there exists $M > 0$ such that \[
            \frac{|f(x)|}{|g(x)|} \leq M
        \] for all points $x$ near $x_0$.

        We say that $f \sim o(g)$ near $x_0$ if \[
            \lim_{x \to x_0} \frac{|f(x)|}{|g(x)|} = 0.
        \] 
    \end{definition}

    \begin{theorem}[Taylor's theorem]
        Let $f\colon [a, b] \to \R$ be such that $f, f', \dots, f^{(n + 1)}$ are
        continuous. Then, \[
            f(x) = p(x) + R_{n + 1}(x)
        \] where $p$ is the Taylor polynomial of degree $n$ of 
        $f$ around some point $x_0 \in (a, b)$, and $R_{n + 1}$ is defined as \[
            R_{n + 1}(x) = \frac{f^{(n + 1)}(c)(x - x_0)^{n + 1}}{(n + 1)!}
        \] for some $c$ between $x$ and $x_0$.

        \begin{remark}
            The former form of $R_{n + 1}$ is called Lagrange's form. The following
            is Cauchy's form; for $0 < \theta < 1$, \[
                R_{n + 1}(x) = \frac{f^{(n + 1)}(x_0 + \theta(x - x_0))(x -
                x_0)^{n + 1}(1 - \theta)^n}{n!}
            \] 
        \end{remark}
    \end{theorem}
    \begin{proof}
        Recall that \[
            p(x) = f(x_0) + f'(x_0)(x - x_0) + \frac{1}{2}f''(x_0)(x - x_0)^2 +
            \dots + \frac{1}{n!}f^{(n)}(x_0)(x - x_0)^n.
        \] Write $R_{n + 1}(x) = (x - x_0)^m H(x)$, for $m \leq n + 1$ and some function
        $H$. Note that $H(x) = (f(x) - p(x))/(x - x_0)^m$ on $x \neq x_0$. Now, fix
        $x \in (a, b)$ and define $\varphi\colon [a, b] \to \R$, \[
            \varphi(t) = f(t) + f'(t)(x - t) + \frac{1}{2}f''(t)(x - t)^2 + \dots +
            \frac{1}{n!}f^{(n)}(t)(x - t)^n + (x - t)^mH(x).
        \] Note that $\varphi$ is continuous on $[a, b]$. Also, $\varphi$ is
        differentiable on $(a, b)$. Now, $\varphi(x) = f(x)$, and \[
            \varphi(x_0) = f(x_0) + f'(x_0)(x - x_0) + \dots +
            \frac{1}{n!}f^{(n)}(x_0)(x - x_0)^n + (x - x_0)^mH(x) = f(x).
        \] On the compact interval with endpoints $x$ and $x_0$, we now have a
        continuous and differentiable function $\varphi$ with $\varphi(x) =
        \varphi(x_0)$, so the Rolle's Theorem gives the existence of $c$ between
        $x$ and $x_0$ such that $\varphi'(c) = 0$. This means that the following is
        zero. \[
            f'(c) - f'(c) + f''(c)(x - c) - f''(c)(x - x_0) + 
            \dots + \frac{1}{n!}f^{(n + 1)}(c)(x - c)^n - m(x - c)^{m - 1}H(x).
        \] Cancelling terms, \[
            m(x - c)^{m - 1}H(x) = \frac{1}{n!}f^{(n + 1)}(c)(x - c)^n.
        \] Putting $m = n + 1$, we have \[
            (n + 1)H(x) = \frac{1}{n!}f^{(n + 1)}(c), \qquad
            R_{n + 1}(x) = \frac{1}{(n + 1)!}f^{(n + 1)}(c)(x - x_0)^{n + 1}.
        \] This is Lagrange's form of the remainder.
        Putting $m = 1$, we have \[
            H(x) = \frac{1}{n!}f^{(n + 1)}(c)(x - c)^n.
        \] Setting $c = x_0 + \theta(x - x_0)$, we have $(x - c) = (x - x_0)(1 -
        \theta)$, so \[
            H(x) = \frac{1}{n!}f^{(n + 1)}(x_0 + \theta(x - x_0))(x - x_0)^n(1 -
            \theta)^n.
        \] Thus, \[
            R_{n + 1}(x) = \frac{1}{n!}f^{(n + 1)}(x_0 + \theta(x - x_0))(x -
            x_0)^{n + 1}(1 - \theta)^n.
        \] This is Cauchy's form of the remainder.
    \end{proof}

    \begin{definition}[Convexity]
        The graph of a function $f$ on $\R$ is said to be convex downwards at a
        point $x_0$ if there exists an non-empty open neighbourhood of $x_0$ such
        that the tangent to the graph at $x_0$ lies below the graph on that
        neighbourhood (excluding the point $x_0$). Similarly, it is said to be
        convex upwards if the tangent lies above the graph on that deleted
        neighbourhood.

        \begin{remark}
            If the tangent line lies on opposite sides of the graph on the left and
            right sides of $x_0$, then it is called a point of inflection.
        \end{remark}
    \end{definition}

    \begin{theorem}
        Let $f\colon (a, b) \to \R$ be such that $f''$ is continuous and $f''(x_0) >
        0$ for some $x_0 \in (a, b)$. Then, the graph of $f$ is convex downwards at
        $x_0$.
    \end{theorem}
    \begin{proof}
        Using Taylor's formula, write \[
            f(x) = f(x_0) + f'(x_0)(x - x_0) + \frac{1}{2}f''(c)(x - x_0)^2,
        \] for some $c$ between $x$ and $x_0$. Note that $f(x_0) + f'(x_0)(x - x_0)$
        is the equation of the tangent line to $f$ at $x_0$. Since $f''(x_0) > 0$
        and $f''$ is continuous, we find a non-empty open neighbourhood of $x_0$ on
        which $f''(x) > 0$. On this neighbourhood (excluding the point $x_0$), \[
            \frac{1}{2}f''(c)(x - x_0)^2 > 0.
        \] This immediately gives \[
            f(x) > f(x_0) + f'(x_0)(x - x_0)
        \] on the deleted neighbourhood of $x_0$.
    \end{proof}
    
    \begin{corollary}
        If $f\colon (a, b)\to \R$ is twice differentiable with continuous $f''$, and
        $x_0 \in (a, b)$ is such that $f'(x_0) = 0$ and $f''(x_0) > 0$, then $x_0$
        is a point of local minimum.
    \end{corollary}
    \begin{proof}
        Note that $f$ is convex downwards on some neighbourhood $(c, d)$ of $x_0$,
        which means that \[
            f(x) > f(x_0) + f'(x_0)(x - x_0) = f(x_0).
        \] on $(c, d)\setminus\{x_0\}$.
    \end{proof}

    \begin{theorem}
        If $f\colon (a, b)\to \R$ is thrice differentiable with continuous $f'''$,
        and $x_0 \in (a, b)$ is such that $f''(x_0) = 0$ and $f'''(x_0) \neq 0$,
        then $x_0$ is a point of inflection.
    \end{theorem}
    \begin{proof}
        Use Taylor's formula to write \[
            f(x) = f(x_0) + f'(x_0)(x - x_0) + \frac{1}{6}f'''(c)(x - x_0)^3,
        \] for some $c$ between $x$ and $x_0$. First suppose that $f'''(x_0) > 0$,
        which means that there is a non-empty neighbourhood of $x_0$ on which
        $f'''(x) > 0$. Also, $(x - x_0)^3$ changes sign as we pass through $x_0$
        from left to right. Thus, \[
            f(x) < f(x_0) + f'(x_0)(x - x_0)
        \] on some $(x_0 - \delta, x_0)$ and \[
            f(x) > f(x_0) + f'(x_0)(x - x_0)
        \] on some $(x_0, x_0 + \delta)$. In other words, $f$ changes sign on either
        side of the tangent at $x_0$. The case where $f'''(x_0) < 0$ is similar.
    \end{proof}

    \begin{theorem}
        Let $f\colon (a, b)\to \R$ be $n$ times differentiable with continuous
        $f^{(n)}$, and $x_0 \in (a, b)$ is such that $f''(x_0) = f'''(x_0) = \dots =
        f^{(n - 1)}(x_0) = 0$ and $f^{(n)}(x_0) \neq 0$.
        \begin{enumerate}
            \itemsep0em
            \item If $n$ is even and $f^{(n)}(x_0) > 0$, then $f$ is convex
            downwards at $x_0$.
            Furthermore, if $f'(x_0) = 0$, then $x_0$ is a point of local minimum.
            \item If $n$ is odd, then $x_0$ is a point of inflection.
        \end{enumerate}
    \end{theorem}   
    \begin{proof}
        Use Taylor's formula to write \[
            f(x) = f(x_0) + f'(x_0)(x - x_0) + \frac{1}{n!}f^{(n)}(c)(x - x_0)^n
        \] for some $c$ between $x$ and $x_0$, and proceed exactly as in the
        previous two theorems.
    \end{proof}

    \begin{theorem}
        Let $f\colon [a, b] \to \R$ be continuous and twice differentiable. 
        Then, $f$ is convex on $[a, b]$ if and only if $f''(x) \geq 0$ for all $x
        \in (a, b)$.
    \end{theorem}
    \begin{proof}
        First suppose that $f$ is convex on $[a, b]$. This means that for every
        $x_1, x_2 \in [a, b]$ and $t \in [0, 1]$, \[
            f(tx_1 + (1 - t)x_2) \leq tf(x_1) + (1 - t)f(x_2).
        \] Let $x \in (a, b)$, and $h$ sufficiently small such that $x + 2h \in (a,
        b)$. Thus, \[
            f(x + h) \leq \frac{1}{2}f(x) + \frac{1}{2}f(x + 2h),
        \] so \[
            f(x + h) - f(x) \leq f(x + 2h) - f(x + h).
        \] Following the same procedure, if $x + 3h \in (a, b)$, \[
            f(x + h) - f(x) \leq f(x + 2h) - f(x + h) \leq f(x + 3h) - f(x + 2h).
        \] Now, choose $x_1, x_2 \in (a, b)$, $x_1 < x_2$. Set $nh = x_1 - x_2$ for
        some $n$. We get a chain of inequalities as above. This will give \[
            \frac{f(x_1 + h) - f(x_1)}{h} \leq \frac{f(x_2 + h) - f(x_2)}{h}.
        \] Taking limits $h \to 0$, we find $f'(x_1) \leq f'(x_2)$, hence $f'$ is
        monotonically increasing. This in turn gives $f''(x) \geq 0$.

        Next, suppose that $f''(x) \geq 0$ on $[a, b]$. Define \[
            \alpha(x) = f(x_1) + \frac{f(x_2) - f(x_1)}{x_2 - x_1}(x - x_1).
        \] We wish to show that \[
            f(x) \leq \alpha(x), \qquad F(x) = f(x) - \alpha(x) \leq 0
        \] for all $x \in [x_1, x_2]$. Suppose not, i.e.\ there is a point $x_0 \in
        [x_2, x_2]$ such that $F(x_0) > 0$. This means that the supremum of $F$ is
        attained at some point in $(x_1, x_2)$, because $F(x_1) = F(x_2) = 0$.
        Without loss of generality, let $F(x_0)$ be the supremum. Since $F$ is
        differentiable on $(x_1, x_2)$ and $x_0$ is a point of local maximum, we
        must have $F'(x_0) = 0$. Using Taylor's formula, \[
            F(x) = F(x_0) + F'(x_0)(x - x_0) + F''(c)(x - x_0)^2
        \] for some $c$ between $x$ and $x_0$. Since $F'(x_0) = 0$ and $F''(c) =
        f''(c) \geq 0$, we must have $F(x) \geq F(x_0)$. Specifically, $F(x_2) \geq
        F(x_0)$, so $0 \geq F(x_0)$ which is a contradiction.
    \end{proof}

    \begin{theorem}
        There exists a continuous, nowhere differentiable function.
    \end{theorem}
    \begin{proof}
        Let $\varphi(x) = |x|$ on $[-1, +1]$, which is continuous but not
        differentiable at $0$. Extend $\varphi$ continuously to the entirety of $\R$
        continuously, as $\varphi(x) = \varphi(x + 2)$, which means that $\varphi$
        is not differentiable at the integers. Define $f\colon \R \to \R$, \[
            f(x) = \sum_{n = 0}^\infty \left( \frac{3}{4} \right)^n\varphi(4^n x).
        \] Since $\varphi(x) \in [0, 1]$, the series converges uniformly on $\R$.
        This in turn gives the continuity of $\R$.

        To show that $f$ is not differentiable at every point $x \in \R$, construct a 
        sequence such that $h_n \to 0$ but $\lim_{n \to \infty} (f(x + h_n) - f(x)) / 
        h_n$ does not exist. We fix $x$ and $m$, and set \[
            h_n = \pm\frac{1}{2}4^{-m},
        \] where the sign is chosen such that there is no integer between $4^mx$ and
        $4^m(x + h_n)$. Now, set \[
            \gamma_n = \frac{\varphi(4^n(x + h_n)) - \varphi(4^nx)}{h_n}.
        \] Note that when $n > m$, $4^nh_n$ is an even integer, so $\gamma_n = 0$.
        When $0 \leq n \leq m$, we have $|\gamma_n| \leq 4^n$ since $|\varphi(s) -
        \varphi(t)| \leq |s - t|$. For $n = m$, $\gamma_m = 4^m$, so we now evaluate
        \[
            \left|\frac{f(x + h_n) - f(x)}{h_n}\right| = \left|\sum_{n = 0}^m
            \left(\frac{3}{4}\right)^n \gamma_n\right| \geq \frac{1}{2}(3^m + 1).
        \] However, $h_n \to 0$, which means that the desired limit does not exist.
    \end{proof}
    \begin{example}
        Weierstrass gave the example \[
            f(x) = \sum_{n = 0}^\infty a^n\cos(b^n\pi x),
        \] where $0 < a < 1$ and $b$ is a positive odd integer such that $ab > 1 +
        3\pi / 2$.
    \end{example}
    \begin{example}
        Hardy gave the example \[
            f(x) = \sum_{n = 1}^\infty \frac{\sin(n^2\pi x)}{n^2}.
        \] 
    \end{example}

\end{document}
% vim: set tabstop=4 shiftwidth=4 softtabstop=4:
