\documentclass[11pt]{article}

\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{amsmath, amssymb, amsthm}
\usepackage[scr]{rsfso}
\usepackage{bm}
\usepackage[%
    hidealllines=true,%
    innerbottommargin=15,%
    nobreak=true,%
]{mdframed}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{hyperref}

\geometry{a4paper, margin=1in, headheight=14pt}

\pagestyle{fancy}
\fancyhf{}
\renewcommand\headrulewidth{0.4pt}
\fancyhead[L]{\scshape MA3101: Analysis III}
\fancyhead[R]{\scshape \leftmark}
\rfoot{\footnotesize\it Updated on \today}
\cfoot{\thepage}

\newcommand{\C}{\mathbb{C}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}

\newcommand{\ip}[2]{\langle #1, #2 \rangle}
\newcommand{\norm}[1]{\Vert #1 \Vert}
\renewcommand{\vec}[1]{\boldsymbol{#1}}

\newcommand{\vx}{\vec{x}}
\newcommand{\vy}{\vec{y}}
\newcommand{\vv}{\vec{v}}
\newcommand{\vw}{\vec{w}}
\newcommand{\ve}{\vec{e}}
\newcommand{\va}{\vec{a}}
\newcommand{\vb}{\vec{b}}

\newcommand{\dd}[2]{\frac{d #1}{d #2}}
\newcommand{\pp}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\ddn}[3][]{\frac{d^{#1} #2}{d #3^{#1}}}
\newcommand{\ppn}[3][]{\frac{\partial^{#1} #2}{\partial #3^{#1}}}
\newcommand{\grad}{\nabla}

\newmdtheoremenv[%
    backgroundcolor=blue!10!white,%
]{theorem}{Theorem}[section]
\newmdtheoremenv[%
    backgroundcolor=violet!10!white,%
]{corollary}{Corollary}[theorem]
\newmdtheoremenv[%
    backgroundcolor=teal!10!white,%
]{lemma}[theorem]{Lemma}

\theoremstyle{definition}
\newmdtheoremenv[%
    backgroundcolor=green!10!white,%
]{definition}{Definition}[section]
\newmdtheoremenv[%
    backgroundcolor=red!10!white,%
]{exercise}{Exercise}[section]

\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{example}{Example}
\newtheorem*{solution}{Solution}

\surroundwithmdframed[%
    linecolor=black!20!white,%
    hidealllines=false,%
    innertopmargin=5,%
    innerbottommargin=10,%
    skipabove=0,%
    skipbelow=0,%
]{example}

\numberwithin{equation}{section}

\title{
    \Large\textsc{MA3101} \\
    \Huge \textbf{Analysis III} \\
    \vspace{5pt}
    \Large{Autumn 2021}
}
\author{
    \large Satvik Saha
    \\\textsc{\small 19MS154}
}
\date{\normalsize
    \textit{Indian Institute of Science Education and Research, Kolkata, \\
    Mohanpur, West Bengal, 741246, India.} \\
}

\begin{document}
    \maketitle

    \tableofcontents

    \section{Euclidean spaces}

    \subsection{$\R^n$ as a vector space}
    
    We are familiar with the vector space $\R^n$, with the standard inner product \[
        \ip{\vx}{\vy} = x_1y_1 + \dots + x_ny_n.
    \] The standard norm is defined as \[
        \norm{\vx - \vy}^2 = \ip{\vx - \vy}{\vx - \vy} = \sum_{k = 1}^n (x_i -
        y_i)^2.
    \]

    \begin{exercise}
        What are all possible inner products on $\R^n$?
        \begin{solution}
            Note that an inner product is a bilinear, symmetric map such that
            $\ip{\vx}{\vx} \geq 0$, and $\ip{\vx}{\vx} = 0$ if and only if $\vx =
            \vec{0}$. Thus, an product map on $\R^n$ is completely and uniquely
            determined by the values $\ip{\ve_i}{\ve_j} = a_{ij}$. Let $A$ be the
            $n \times n$ matrix with entries $a_{ij}$. Note that $A$ is a real
            symmetric matrix with positive entries. Now, \[
                \ip{\vx}{\ve_j} = x_1a_{1j} + \dots + x_na_{nj} = \vx^\top \vec{a}_j,
            \] where $\vec{a}_j$ is the $j^\text{th}$ column of $A$. Thus, \[
                \ip{\vx}{\vy} = \vx^\top \vec{a}_1 y_1 + \dots + \vx^\top
                \vec{a}_n y_n = \vx^\top A \vy.
            \] Furthermore, any choice of real symmetric $A$ with positive
            entries produces an inner product.
        \end{solution}
    \end{exercise}

    \begin{theorem}[Cauchy-Schwarz]
        Given two vectors $\vv, \vw \in \R^n$, we have \[
            |\ip{\vv}{\vw}| \leq \norm{\vv}\norm{\vw}.
        \] 
    \end{theorem}
    \begin{proof}
        This is trivial when $\vw = \vec{0}$. When $\vw \neq \vec{0}$, set $\lambda =
        \ip{\vv}{\vw} / \norm{\vw}^2$. Thus, \[
            0 \leq \norm{\vv - \lambda \vw}^2 = \norm{\vv}^2 - 2\lambda \ip{\vv}{\vw}
            + \lambda^2\norm{\vw}^2.
        \] Simplifying, \[
            0 \leq \norm{\vv}^2 - \frac{|\ip{\vv}{\vw}|^2}{\norm{\vw}^2}.
        \] This gives the desired result. Clearly, equality holds if and only if $\vv
        = \lambda \vw$.
    \end{proof}
    
    \begin{theorem}[Triangle inequality]
        Given two vectors $\vv, \vw \in \R^n$, we have \[
            \norm{\vv + \vw} \leq \norm{\vv} + \norm{\vw}.
        \] 
    \end{theorem}
    \begin{proof}
        Write \[
            \norm{\vv + \vw}^2 = \norm{\vv}^2 + 2\ip{\vv}{\vw} + \norm{\vw}^2
            \leq \norm{\vv}^2 + 2|\ip{\vv}{\vw}| + \norm{\vw}^2.
        \] Applying Cauchy-Schwarz gives \[
            \norm{\vv + \vw}^2 \leq (\norm{\vv} + \norm{\vw})^2.
        \] 
        Equality holds if and only if $\vv = \lambda \vw$ for $\lambda \geq 0$.
    \end{proof}

    \subsection{$\R^n$ as a metric space}

    Our previous observations allow us to define the standard metric on $\R^n$, seen
    as a point set. \[
        d(\vx, \vy) = \norm{\vx - \vy}.
    \] 

    \begin{definition}
        For any $\delta > 0$, the set \[
            B_\delta(\vx) = \{\vy \in \R^n : d(\vx, \vy) < \delta\}
        \] is called the open ball centred at $\vx \in \R^n$ with radius $\delta$.
        This is also called the $\delta$ neighbourhood of $\vx$.
    \end{definition}

    \begin{definition}
        A set $U$ is open in $\R^n$ if for every $\vx \in U$, there exists an
        open ball $B_\delta(\vx) \subset U$.
        \begin{remark}
            Every open ball in $\R^n$ is open.
        \end{remark}
        \begin{remark}
            Both $\emptyset$ and $\R^n$ are open.
        \end{remark}
    \end{definition}
    
    \begin{definition}
        A set $F$ is closed in $\R^n$ if its complement $\R^n \setminus F$ is open in
        $\R^n$.
        \begin{remark}
            Both $\emptyset$ and $\R^n$ are closed.
        \end{remark}
        \begin{remark}
            Finite sets in $\R^n$ are closed.
        \end{remark}
    \end{definition}

    \begin{theorem}
        Unions and finite intersections of open sets are open.
    \end{theorem}
    \begin{corollary}
        Intersections and finite unions of closed sets are closed.
    \end{corollary}

    \begin{definition}
        An interior point $x$ of a set $S \subseteq \R^n$ is such that there is a
        neighbourhood of $x$ contained within $S$.
    \end{definition}
    \begin{example}
        Every point in an open set is an interior point by definition. The interior
        of a set is the largest open set contained within it.
    \end{example}

    \begin{definition}
        An exterior point $x$ of a set $S \subseteq \R^n$ is an interior point of the
        complement $\R^n \setminus S$.
    \end{definition}

    \begin{definition}
        A boundary point of a set is neither an interior point, nor an exterior point.
    \end{definition}
    \begin{example}
        The boundary of the unit open ball $B_1(0) \subset \R^n$ is the sphere $S^{n
        - 1}$.
    \end{example}

    \begin{definition}
        A limit point $x$ of a set $S \subseteq \R^n$ is such that every
        neighbourhood of $x$ contains a point from $S$ other than itself.
    \end{definition}
    \begin{definition}
        The closure of a set $S \subseteq \R^n$ is the union of $S$ and its limit
        points.
        \begin{remark}
            The closure of a set is the smallest closed set containing it.
        \end{remark}
    \end{definition}

    \begin{lemma}
        Every open set in $\R^n$ is a union of open balls.
    \end{lemma}
    \begin{proof}
        Let $U\subseteq \R^n$ be open. Thus, for every $\vx\in\R^n$, we can choose
        $\delta_x > 0$ such that $B_{\delta_x}(\vx) \subset U$. The union of all such
        open balls is precisely the set $U$.
    \end{proof}

    \subsection{$\R^n$ as a topological space}
    \begin{definition}
        A topology on a set $X$ is a collection $\tau$ of subsets of $X$ such that
        \begin{enumerate}
            \itemsep0em
            \item $\emptyset \in \tau$
            \item $X \in \tau$
            \item Arbitrary union of sets from $\tau$ belong to $\tau$.
            \item Finite intersections of sets from $\tau$ belong to $\tau$.
        \end{enumerate}
        Sets from $\tau$ are called open sets.
    \end{definition}
    \begin{example}
        The Euclidean metric induces the standard topology on $\R^n$.
    \end{example}
    \begin{example}
        The discrete topology on a set $X$ is one where every singleton set is open.
        This is the topology induced by the discrete metric, \[
            d_\text{discrete}\colon X\times X \to \R, \qquad
            (x, y) \mapsto \begin{cases}
                0, &\text{ if }x = y, \\
                1, &\text{ if }x \neq y.
            \end{cases}
        \] 
    \end{example}
    \begin{example}
        Let $X$ be an infinite set. The collection of sets consisting of $\emptyset$
        along with all sets $A$ such that $X\setminus A$ is finite is a topology on
        $X$. This is called the Zariski topology.
    \end{example}
    \begin{example}
        Consider the set of real numbers, and let $\tau$ be the collection
        $\emptyset$, $\R$, and all intervals $(-x, +x)$ for $x > 0$. This constitutes
        a topology on $\R$, very different from the usual one.

        This topology cannot be induced by a metric; it is not metrizable.

        Consider the constant sequence of zeros. In this topology $(\R, \tau)$, this
        sequence converges to \emph{every} point in $\R$. Given any $\ell \in \R$, the
        open neighbourhoods of $\ell$ are precisely the sets $\R$ and the open intervals
        $(-x, +x)$ for $x > |\ell|$. The tail of the constant sequence of zeros is
        contained within every such neighbourhood of $\ell$, hence $0 \to \ell$.
        Indeed, the element zero belongs to every open set apart from $\emptyset$ in
        this topology.
    \end{example}
    \begin{definition}
        A topological space is called Hausdorff if for every distinct $x, y \in X$,
        there exist disjoint neighbourhoods of $x$ and $y$.
    \end{definition}
    \begin{example}
        Every metric space is Hausdorff.
        Given distinct $x, y$ in a metric space $(X, d)$, set $\delta = d(x, y) / 3$
        and consider the open balls $B_{\delta}(x)$ and $B_\delta(y)$.
    \end{example}
    
    \begin{lemma}
        Every convergent sequence in a Hausdorff space has exactly one limit.
    \end{lemma}
    \begin{proof}
        Consider a sequence $\{x_n\}_{n \in \N}$, and suppose that it converges to
        distinct $x_1$ and $x_2$. Construct disjoint neighbourhoods $U_1$ and $U_2$
        around $x_1$ and $x_2$. Now, convergence implies that both $U_1$ and $U_2$
        contain the tail of $\{x_n\}$, which is impossible since they are disjoint
        and hence contain no elements in common.
    \end{proof}

    \begin{definition}
        Given a topological space $(X, \tau)$ and a subset $Y\subseteq X$, the
        collection of sets $U \cap Y$ where $U \in \tau$ is a topology $\tau_Y$ on
        $Y$. We call this collection the subspace topology on $Y$, induced by the
        topology on $X$.
    \end{definition}

    \subsection{Compact sets in $\R^n$}
    
    \begin{definition}
        A set $K \subset X$ in a topological space is compact if every open cover of
        $K$ has a finite sub-cover. That is, for every collection if
        $\{U_\alpha\}_{\alpha \in A}$ of open sets such that $K$ is contained in
        their union, there exists a finite sub-collection $U_{\alpha_1}, \dots,
        U_{\alpha_k}$ such that $K$ is also contained in their union.
    \end{definition}
    \begin{example}
        All finite sets are compact.
    \end{example}
    \begin{example}
        Given a convergent sequence of real numbers $x_n \to x$, the collection
        $\{x_n\}_{n \in \N} \cup \{x\}$ is compact.
    \end{example}
    \begin{example}
        In $\R^n$, compact sets are precisely those sets which are closed and
        bounded. This is the Heine-Borel Theorem.
    \end{example}

    \begin{theorem}
        The closed intervals $[a, b] \subset \R$ are compact.
        \begin{remark}
            This can be extended to show that any $k$-cell $[a_1, b_1]\times \dots
            \times [a_n, b_n] \subset \R^n$ is compact.
        \end{remark}
    \end{theorem}
    \begin{proof}
        Let $\{U_\alpha\}_{\alpha \in A}$ be an open cover of $[a, b]$, and suppose
        that $I_1 = [a, b]$ has no finite sub-cover. Then, at least one of the
        intervals $[a, (a + b) / 2]$ and $[(a + b)/ 2, b]$ must not have a finite
        sub-cover; pick one and call it $I_2$. Similarly, one of the halves of $I_2$
        must not have a finite sub-cover; call it $I_3$. In this process, we generate
        a sequence of closed intervals $I_1 \supset I_2 \supset \dots$, none of which
        have a finite sub-cover. The length of each interval is given by \[
            |I_n| = 2^{-n + 1}\norm{b - a} \to 0.
        \] Now, pick a sequence of points $\{x_n\}$ where each $x_n \in I_n$. Then,
        $\{x_n\}$ is a Cauchy sequence. To see this, given any $\epsilon > 0$, we can
        find sufficiently large $n_0$ such that $2^{-n_0 + 1}\norm{b - a} <
        \epsilon$. Thus, $x_n \in I_n \subset I_{n_0}$ for all $n \geq n_0$, which
        means that for any $m, n \geq n_0$, we have $x_m, x_n \in I_{n_0}$ forcing\footnote{
            If $x_1, x_2 \in [a, b]$ with $x_1 < x_2$, note that $a \leq x_1 < x_2
            \leq b$, so\[
                |x_2 - x_1| = x_2 - x_1 \leq b - a.
            \] 
        } \[
            \norm{x_m - x_n} \leq |I_{n_0}| = 2^{-n_0 + 1}\norm{b - a} < \epsilon.
        \] From the completeness of $\R$, this sequence must converge in $\R$,
        specifically in $[a, b]$. Thus, $x_n \to x$ for some $x \in [a, b]$. It can
        also be seen that  the limit $x \in I_n$ for all $n \in \N$; if not, say $x
        \notin I_{n_0}$, then $x \in [a, b] \setminus I_{n_0}$ which is open, hence
        there is an open interval such that $(x - \delta, x + \delta) \cap I_{n_0} =
        \emptyset$.  However, $I_{n_0}$ contains all $x_{n \geq n_0}$, thus this
        $\delta$-neighbourhood of $x$ would miss out a tail of $\{x_n\}$.

        Now, pick the open set $U \in \{U_\alpha\}$ which covers the point $x$. Thus,
        $x \in U$ so $U$ contains some non-empty open interval $(x - \delta, x +
        \delta)$ around $x$. Choose $n_0$ such that $2^{-n_0 + 1} \norm{b - a} <
        \delta$; this immediately gives $I_{n_0} \subseteq (x - \delta, x + \delta)
        \subset U$.  This contradicts that fact that $I_{n_0}$ has no finite
        sub-cover from $\{U_\alpha\}$, completing the proof.
    \end{proof}
    \begin{remark}
        The fact that Cauchy sequences in $\R^n$ converge isn't immediately obvious;
        it is a consequence of the completeness of $\R^n$. Start by noting that $\R$
        has the Least Upper Bound property, from which the Monotone Convergence
        Theorem follows; every monotonic, bounded sequence of reals converges. It
        can also be shown that any sequence of reals with contain a monotone
        subsequence, from which it follows that every bounded sequence contains a
        convergent subsequence (Bolzano-Weierstrass). Finally, it can be shown that
        if a subsequence of a Cauchy sequence converges, then the entire sequence
        also converges to the same limit, giving us the desired result for $\R$. For
        sequence in $\R^n$, we may apply this coordinate-wise to obtain the result.
    \end{remark}

    \begin{lemma}
        Compact sets in $\R^n$ are closed and bounded.
    \end{lemma}
    \begin{proof}
        Consider a compact set $K \subset \R^n$. Let $x \in \R^n \setminus K$, and
        let $y \in K$. Since $x \neq y$, we choose open balls $U_y$ around $y$ and
        $V_y$ around $x$ such that $U_y\cap V_y = \emptyset$. Repeating this for all
        $y \in K$, we generate an open cover $\{U_y\}$ of $K$ consisting of open balls. The
        compactness of $K$ guarantees that this has a finite sub-cover, i.e.\ there
        is a finite set $Y$ such that the collection $\{U_y\}_{y \in Y}$ covers $X$.
        As a result, the finite intersection of all $V_y$ for $y \in Y$ is contained
        within $\R^n \setminus K$. Thus, $x$ is in the exterior of $K$. Since $x$ was
        chosen arbitrarily from $\R^n\setminus K$, we see that $K$ is closed.

        Now, consider the open cover $\{B_1(x)\}_{x \in K}$, and extract a finite
        sub-cover of unit open balls. The distance between any two points in $K$ is
        at most the maximum distance between the centres of any two balls in our
        sub-cover, plus two.
    \end{proof}

    \begin{lemma}
        The intersection of a closed set and a compact set is compact.
    \end{lemma}
    \begin{proof}
        Let $F \subseteq \R^n$ be closed and let $K \subseteq \R^n$ be compact.
        Suppose that the open cover $\{U_\alpha\}$ of $F \cap K$ has no finite
        sub-cover. Now the complement $U = F^c$ is open in $\R^n$, hence the
        collection $\{U_\alpha\} \cup \{U\}$ is an open cover of $K$, and hence must
        admit a finite sub-cover of $K$. In particular, this must be a finite
        sub-cover of $F \cap K$. However, we can remove the set $U$ from this
        sub-cover since it shares no element with $F \cap K$; as a result, our
        sub-cover must be a finite sub-collection of sets $U_\alpha$, contradicting
        our assumption. This shows that $F \cap K$ is compact.
    \end{proof}

    \begin{lemma}[Finite intersection property]
        Let $\{K_\alpha\}$ be a collection of compact sets in $\R^n$ which have the
        property that any finite intersection of them is non-empty. Then, \[
            \bigcap_{\alpha} K_\alpha \neq \emptyset.
        \] 
    \end{lemma}
    \begin{proof}
        Suppose to the contrary that the intersection of all $K_\alpha$ is empty. Fix
        an index $\beta$, and note that no element of $K_\beta$ lies in every
        $K_\alpha$. Set $J_\alpha = K_\alpha^c$, whence the collection $\{J_\alpha :
        \alpha \neq \beta\}$ is an open cover of $K_\beta$. This must admit a finite
        sub-cover $\{J_{\alpha_1},\dots, J_{\alpha_k}\}$ of $K_\beta$. Thus, we must
        have \[
            K_\beta^c \cup J_{\alpha_1} \cup \dots \cup J_{\alpha_k} = \R^n.
        \] This immediately gives the contradiction \[
            K_\beta \cap K_{\alpha_1} \cap \dots \cap K_{\alpha_k} = \emptyset.
            \qedhere
        \] 
    \end{proof}

    \begin{theorem}[Heine-Borel]
        Compact sets in $\R^n$ are precisely those that are closed and bounded.
    \end{theorem}
    \begin{proof}
        Given a compact set in $\R^n$, we have already shown that it must be closed
        and bounded. Next, if $F \subset \R^n$ is closed and bounded, it can be
        enclosed within a $k$-cell which we know is compact. Thus, $F$ is the
        intersection of the closed set $F$ and the compact $k$-cell, proving that
        $F$ must be compact.
    \end{proof}

    \subsection{Continuous maps}
    \begin{definition}
        A map $f\colon X \to Y$ is continuous if the pre-image of every open set from
        $Y$ is open in $X$.
    \end{definition}
    \begin{lemma}
        A map $f\colon X \to Y$ is continuous if the pre-image of every closed set from
        $Y$ is closed in $X$.
    \end{lemma}

    \begin{theorem}
        The projection maps $\pi_i\colon \R^n \to \R$, $\vx \mapsto x_i$ are continuous.
    \end{theorem}
    \begin{proof}
        Let $U \subseteq \R$ be open; we claim that $\pi_i^{-1}(U)$ is open. Pick
        $\vx \in \pi_i^{-1}(U)$, and note that $\pi_i(\vx) = x_i \in U$. Thus, there
        exists $\delta > 0$ such that $(x_i - \delta, x_i + \delta) \subset U$. Now
        examine $B_\delta(\vx)$; for any point $\vy$ within this open ball, we have
        $d(\vx, \vy) < \delta$ hence \[
            |x_i - y_i|^2 \leq \sum_{k = 1}^n (x_k - y_k)^2 = d(\vx, \vy)^2 <
            \delta^2.
        \] In other words, $\pi_i(\vy) = y_i \in (x_i - \delta, x_i + \delta)$, hence
        $\pi_i \,B_\delta(\vx) \subseteq (x_i - \delta, x_i + \delta) \subset
        U$. Thus, given arbitrary $\vx \in \pi_i^{-1}(U)$, we have found an open ball
        $B_\delta(\vx) \subset \pi_i^{-1}(U)$.
    \end{proof}

    \begin{lemma}
        Finite sums, products, and compositions of continuous functions are continuous.
    \end{lemma}
    \begin{corollary}
        A function $f\colon [a, b] \to \R^n$ is continuous if and only if the
        components, $\pi_i \circ f$, are continuous.
    \end{corollary}

    \begin{theorem}
        All polynomial functions of the coordinates in $\R^n$ are continuous.
    \end{theorem}
    \begin{example}
        The unit sphere $S^{n - 1} \subset \R^n$ is closed. It is by definition the
        pre-image of the singleton closed set $\{1\}$ under the continuous map \[
            \vx \mapsto x_1^2 + \dots + x_n^2.
        \] 
    \end{example}
    
    \begin{theorem}
        The continuous image of a compact set is compact.
    \end{theorem}
    \begin{proof}
        Let $f\colon X \to Y$ be continuous, where $Y$ is the image of the compact
        set $X$, and let $\{U_\alpha\}$ be an open cover of $Y$. Then, the collection
        $\{f^{-1}(U_\alpha)\}$ is an open cover of $X$. Using the compactness of $X$,
        extract a finite sub-cover $f^{-1}(U_{\alpha_1}), \dots,
        f^{-1}(U_{\alpha_k})$ of $X$. It follows that the collection $U_{\alpha_1},
        \dots, U_{\alpha_k}$ is a finite sub-cover of $Y$.
    \end{proof}

    \subsection{Connectedness}
    \begin{definition}
        Let $X$ be a topological space. A separation of $X$ is a pair $U, V$ of
        non-empty disjoint open subsets such that $X = U \cup V$.
    \end{definition}

    \begin{definition}
        A connected topological space is one which cannot be separated.
    \end{definition}
    \begin{lemma}
        A topological space $X$ is connected if and only if the only sets which are
        both open and closed are $\emptyset$ and $X$.
    \end{lemma}

    \begin{example}
        The intervals $(a, b) \subset \R$ are connected. To see this, suppose that
        $U$, $V$ is a separation of $(a, b)$. Pick $x \in U$, $y \in V$, and without
        loss of generality let $x < y$. Define $S = [x, y] \cap U$, and set $c =
        \sup{S}$. It can be argued that $c \in (a, b)$, but $c \notin U$, $c \notin
        V$, using the properties of the supremum.
    \end{example}

    \begin{theorem}
        The continuous image of a connected set is connected.
    \end{theorem}
    \begin{proof}
        Let $f$ be a continuous map on the connected set $X$, and let $Y$ be the
        image of $X$. If $U$, $V$ is a separation of $Y$, then it can be shown that
        $f^{-1}(U)$, $f^{-1}(V)$ constitutes a separation of $X$, which is a
        contradiction.
    \end{proof}

    \begin{definition}
        A path $\gamma$ joining two points $x, y \in X$ is a continuous map
        $\gamma\colon [a, b] \to X$ such that $\gamma(a) = x$, $\gamma(b) = y$.
    \end{definition}

    \begin{definition}
        A set in $X$ is path connected if given any two distinct points in $X$, there
        exists a path joining them.
    \end{definition}

    \begin{lemma}
        Every path connected set is connected.
    \end{lemma}
    \begin{proof}
        Let $X$ be path connected, and suppose that $U$, $V$ is a separation of $X$.
        Then, pick $x \in U$, $y \in V$, and choose a path $\gamma\colon [0, 1] \to
        X$ between $x$ and $y$. The sets $f^{-1}(U)$ and $f^{-1}(V)$ separate the
        interval $[0, 1]$, which is a contradiction.
    \end{proof}
    \begin{example}
        All connected sets are not path connected. Consider the topologist's sine
        curve, \[
            \left\{\left(x, \sin{\frac{1}{x}}\right): 0 < x \leq 1\right\} \cup \{(0,
            0)\}.
        \] 
    \end{example}


    \begin{definition}
        The $\epsilon$ neighbourhood of a set $K$ in a metric space $X$ is defined as \[
             \bigcup_{a \in K} B_\epsilon(a) = \bigcup_{a \in K} \{x \in X: d(x, a) <
             \epsilon\}.
        \] 
    \end{definition}

    \begin{exercise}
        Let $K \subseteq \R^n$ be compact, and define $f\colon \R^n \to \R$, \[
            f(x) = \operatorname{dist}(x, K) = \inf_{a \in K} d(x, a).
        \] Show that $f$ is continuous on $\R^n$, and $f^{-1}(\{0\}) = K$.
    \end{exercise}

    \begin{exercise}
        If $K \subseteq \R^n$ is compact and $K \cap L = \emptyset$, then \[
            \operatorname{dist}(K, L) = \inf_{a \in K} \operatorname{dist}(a, L) > 0.
        \] 
    \end{exercise}

    \begin{exercise}
        If $K \subseteq \R^n$ is compact and $U$ is an open set containing $K$, then
        there exists $\epsilon > 0$ such that $U$ contains the $\epsilon$
        neighbourhood of $K$.

        Is the compactness of $K$ necessary?
    \end{exercise}
    
    \section{Differential calculus}
    
    \subsection{Differentiability}
    \begin{definition}
        Let $f \colon (a, b) \to \R^n$, and let $f_i = \pi_i\circ f$ be its
        components. Then, $f$ is differentiable at $t_0 \in (a, b)$ if the following
        limit exists. \[
            f'(t_0) = \lim_{h \to 0} \frac{f(t_0 + h) - f(t_0)}{h}.
        \] 
        \begin{remark}
            The vector $f'(t_0)$ represents the tangent to the curve $f$ at the point
            $f(t_0)$. The full tangent line is the parametric curve $f(t) + f'(t_0)(t
            - t_0)$.
        \end{remark}
    \end{definition}

    \begin{definition}
        Let $U \subseteq \R^n$ be open, and let $f\colon U \to \R^m$. Then, $f$ is
        differentiable at $x \in U$ if there exists a linear transformation
        $\lambda\colon \R^n \to \R^m$ such that \[
            \lim_{h \to 0} \frac{f(x + h) - f(x) - \lambda h}{\norm{h}} = 0.
        \] The derivative of $f$ at $x$ is denoted by $\lambda = Df(x)$.
        \begin{remark}
            In a neighbourhood of $x$, we may approximate \[
                f(x + h) \approx f(x) + Df(x)(h).
            \] 
        \end{remark}
        \begin{remark}
            The statement that this quantity goes to zero means that each of the $m$
            components must also go to zero. For each of these limits, there are $n$
            axes along which we can let $h \to 0$. As a result, we obtain $m\times n$
            limits, which allow us to identify the $m\times n$ components of the
            matrix representing the linear transformation $\lambda$ (in the standard
            basis). These are the partial derivatives of $f$, and the matrix of
            $\lambda$ is the Jacobian matrix of $f$ evaluated at $x$.
        \end{remark}
    \end{definition}

    \begin{example}
        Let $T\colon \R^n \to \R^m$ be a linear map. By choosing $\lambda = T$, we
        see that $T$ is differentiable everywhere, with $DT(x) = T$ for every choice
        of $x \in \R^n$. This is made obvious by the fact that the best linear
        approximation of a linear map at some point is the map itself; indeed, the
        `approximation' is exact.
    \end{example}

    \begin{lemma}
        If $f\colon \R^n \to \R^m$ is differentiable at $x \in \R^n$, with derivative
        $Df(x)$, then
        \begin{enumerate}
            \itemsep0em
            \item $f$ is continuous at $x$.
            \item The linear transformation $Df(x)$ is unique.
        \end{enumerate}
    \end{lemma}
    \begin{proof}
        We prove the second part. Suppose that $\lambda$, $\mu$ satisfy the
        requirements for $Df(x)$; it can be shown that $\lim_{h \to 0} (\lambda -
        \mu)h / \norm{h} = 0$. Now, if $\lambda v \neq \mu v$ for some non-zero
        vector $v \in \R^n$, then \[
            \lambda v - \mu v = \frac{\lambda(tv) - \mu(tv)}{\norm{tv}} \cdot
            \norm{v} \to 0,
        \] a contradiction.
    \end{proof}

    \subsection{Chain rule}

    \begin{exercise}
        Let $T\colon \R^n \to \R^m$ be a linear transformation. Then, there exists $M
        > 0$ such that for all $\vx \in \R^n$, we have \[
            \norm{T\vx} \leq M\norm{\vx}.
        \] 
        \begin{solution}
            Set $\vv_i = T(\ve_i)$ where $\ve_i$ are the standard unit basis vectors of
            $\R^n$. Then, \[
                \norm{T\vx} = \norm{\sum_{i}x_i\vv_i} \leq \sum_{i}\norm{x_i\vv_i}
                \leq \max{\norm{\vv_i}} \sum_i |x_i|.
            \] Since each $|x_i| \leq \norm{\vx}$, set $M = n\max{\norm{\vv_i}}$ and
            write \[
                \norm{T\vx} \leq \max{\norm{\vv_i}}\sum_i |x_i| \leq
                \max{\norm{\vv_i}} \cdot n\norm{\vx} = M\norm{\vx}.
            \] 
        \end{solution}
    \end{exercise}
    \begin{theorem}
        Let $f\colon \R^n \to \R^m$, $g\colon \R^m \to \R^k$ where $f$ is
        differentiable at $a \in \R^n$ and $g$ is differentiable at $f(a) \in \R^m$.
        Then, $g\circ f$ is differentiable, with $D(g\circ f)(a) = Dg(f(a)) \circ
        Df(a)$. Note that this means that the Jacobian matrices simply multiply.
    \end{theorem}
    \begin{proof}
        Set $b = f(a) \in \R^m$, $\lambda = Df(a)$, $\mu = Dg(f(a))$. Define 
        \begin{align*}
            \varphi\colon \R^n \to \R^m,\qquad \varphi(x) &= f(x) - f(a) - \lambda(x
            - a), \\
            \psi\colon \R^m \to \R^k,\qquad \psi(y) &= g(y) - g(b) - \mu(y - b).
        \end{align*}
        We claim that \[
            \lim_{x \to a} \frac{g\circ f(x) - g\circ f(a) - \mu\circ \lambda(x
            - a)}{\norm{x - a}} = 0.
        \] Write the numerator as \[
            g\circ f(x) - g\circ f(a) - \mu\circ \lambda(x - a) = \psi(f(x)) +
            \mu(\varphi(x)).
        \] Note that \[
            \lim_{x \to a} \frac{\varphi(x)}{\norm{x - a}} = 0, \qquad 
            \lim_{y \to b} \frac{\psi(y)}{\norm{y - b}} = 0.
        \] Thus, find $M > 0$ such that $\norm{\mu(\varphi(x))} \leq
        \norm{\varphi(x)}$ for all $x \in \R^n$, hence \[
            \lim_{x \to a} \frac{\norm{\mu(\varphi(x))}}{\norm{x - a}} \leq \lim_{x
            \to a} \frac{M\norm{\varphi(x)}}{\norm{x - a}} = 0.
        \] Now write \[
            \lim_{f(x) \to b} \frac{\psi(f(x))}{\norm{f(x) - b}} = 0, 
        \] hence for any $\epsilon > 0$, there is a neighbourhood of $b$ on which \[
            \norm{\psi(f(x))} \leq \epsilon \norm{f(x) - b} = \epsilon
            \norm{\varphi(x) + \lambda(x - a)}.
        \] Apply the triangle inequality and find $M' > 0$ such that \[
            \norm{\psi(f(x))} \leq
            \epsilon\norm{\varphi(x)} + \epsilon M'\norm{x - a}.
        \] Thus, \[
            \lim_{x \to a} \frac{\norm{\psi(f(x))}}{\norm{x - a}} \leq
            \lim_{x \to a} \frac{\epsilon\norm{\varphi(x)}}{\norm{x - a}} + \epsilon
            M' = \epsilon M'.
        \] Since $\epsilon > 0$ was arbitrary, this limit is zero, completing the proof.
    \end{proof} 

    \subsection{Partial derivatives}

    \begin{definition}
        Let $U \subseteq \R^n$ be open, and let $f\colon U \to \R$. The partial
        derivative of $f$ with respect to the coordinate $x_j$ at some $a \in U$
        is defined by the following limit, if it exists. \[
            \pp{f}{x_j}(a) = \lim_{h \to 0} \frac{f(a + h\vec{e}_j) - f(a)}{h}.
        \] 
    \end{definition}
    \begin{lemma}
        If $f\colon U \to \R$ is differentiable at a point $a \in \R^n$, then \[
            Df(a)(x_1, \dots, x_n) = x_1 \,\pp{f}{x_1}(a) + \dots + x_n
            \,\pp{f}{x_n}(a).
        \] 
    \end{lemma}
    \begin{example}
        Consider \[
            f\colon \R^2 \to \R, \qquad (x, y) \mapsto \begin{cases}
                xy/(x^2 + y^2), &\text{ if } (x, y) \neq (0, 0), \\
                0, &\text{ if } (x, y) = (0, 0).
            \end{cases}
        \] Note that $f$ is not differentiable at $(0, 0)$; it is not even continuous
        there. However, both partial derivatives of $f$ exist at $(0, 0)$.
    \end{example}

    \begin{lemma}
        If $f\colon \R^n \to \R^m$ is differentiable at $a \in \R^n$, then the matrix
        representation of $Df(a)$ in the standard basis is given by \[
            [Df(a)] = \left[\pp{f_i}{x_j}(a)\right]_{ij}.
        \] 
    \end{lemma}

    \begin{lemma}
        Let $f\colon \R^n \to \R^m$ be differentiable at $a \in \R^n$, and let
        $g\colon \R^m \to \R^k$ be differentiable at $f(a) \in \R^m$. Then, the
        matrix representation of $D(g\circ f)(a)$ in the standard basis is the
        product \[
            [D(g\circ f)(a)] = [Dg(f(a))][Df(a)] = \left[\sum_{\ell = 1}^m
            \pp{g_i}{y_\ell}\pp{f_\ell}{x_j}\right]_{ij}.
        \] In other words, \[
            \pp{}{x_j}(g\circ f)_i(a) = \sum_{\ell = 1}^m \pp{g_i}{y_\ell}(f(a))
            \pp{f_\ell}{x_j}(a).
        \] 
    \end{lemma}

    \begin{example}
        Let $f\colon \R^2\to \R$ be differentiable, and let $\Gamma(f) = \{(x, y,
        f(x, y)): x, y \in \R\}$ be the graph of $f$. Now, let $\gamma\colon [-1, 1]
        \to \Gamma(f)$ be a differentiable curve, represented by \[
            \gamma(t) = (g(t), h(t), f(g(t), h(t))).
        \] Then, we can compute the derivative \[
            \gamma'(a) = \left(g'(a),\, h'(a),\, g'(a)\pp{f}{x} + h'(a)\pp{f}{y}
            \;\Big|_{(g(a), h(a))}\right)
        \] 
    \end{example}

    \begin{exercise}
        Consider the inner product map, $\ip{\cdot}{\cdot}\colon \R^n \times \R^n \to
        \R$. What is its derivative?
        \begin{solution}
            We treat the inner product as a map $g\colon \R^{2n} \to \R$, which acts
            as \[
                \ip{\vx}{\vy} \cong g(x_1, \dots, x_n, y_1, \dots, y_n) = x_1y_1 +
                \dots + x_ny_n.
            \] Now, note that \[
                \pp{g}{x_i} = y_i, \qquad \pp{g}{y_i} = x_i.
            \] Thus, \begin{align*}
                Dg(\va, \vb)(\vx, \vy) &= \sum_{i = 1}^n x_i \pp{g}{x_i}(\va, \vb) +
                \sum_{i = 1}^n y_i \pp{g}{y_i}(\va, \vb) \\
                &= \sum_{i = 1}^n x_i b_i + \sum_{i = 1}^n y_i a_i \\
                &= \ip{\vx}{\vb} + \ip{\vy}{\va}.
            \end{align*}
            In other words, the matrix representation of the derivative of the inner
            product map at the point $(\va, \vb)$ is given by $[\vb^\top \;
            \va^\top]$.
        \end{solution}
    \end{exercise}

    \begin{exercise}
        Let $\gamma\colon \R \to \R^n$ be a differentiable curve. What is the
        derivative of the real map $t \mapsto \norm{\gamma(t)}^2$?
        \begin{solution}
            We write this map as $t \mapsto \ip{\gamma(t)}{\gamma(t)}$. Consider the
            scheme \[
                \R \to \R^{2n} \to \R, \qquad
                t \mapsto \begin{bmatrix}
                    \gamma(t) \\ \gamma(t)
                \end{bmatrix} \mapsto \ip{\gamma(t)}{\gamma(t)}.
            \] Pick a point $t\in \R$, whence the derivative of the map at $t$ is \[
                \begin{bmatrix}
                    \gamma(t)^\top & \gamma(t)^\top
                \end{bmatrix} \begin{bmatrix}
                    \gamma'(t) \\ \gamma'(t)
                \end{bmatrix} = 2\ip{\gamma(t)}{\gamma'(t)}.
            \] 
        \end{solution}
        \begin{remark}
            Consider the surface $S^{n - 1} \subset \R^n$, and pick an arbitrary
            differentiable curve $\gamma\colon \R \to S^{n - 1}$. Now, the tangent
            vector $\gamma'(t)$ is tangent to the sphere $S^{n - 1}$ at any point
            $\gamma(t)$. We claim that this tangent drawn at $\gamma(t)$ is always
            perpendicular to the position vector $\gamma(t)$. This is made trivial by
            our exercise: the map $t \mapsto \norm{\gamma(t)}^2 = 1$ is a constant
            map since $\gamma$ is a curve on the unit sphere. This means that it has
            zero derivative, forcing $\ip{\gamma(t)}{\gamma'(t)} = 0$.
        \end{remark}
    \end{exercise}

    \subsubsection{Directional derivatives}

    \begin{definition}
        Let $U \subseteq \R^n$ be open, and let $f\colon U \to \R$. The directional
        derivative of $f$ along a direction $\vv \in \R^n$ at a point $a \in U$ is
        defined by the following limit, if it exists. \[
            \grad_v f(a) = \lim_{h \to 0} \frac{f(a + h\vv) - f(a)}{h}.
        \] 
    \end{definition}

    \begin{example}
        Consider \[
            f\colon \R^2 \to \R, \qquad (x, y) \mapsto \begin{cases}
                x^3/(x^2 + y^2), &\text{ if } (x, y) \neq (0, 0), \\
                0, &\text{ if } (x, y) = (0, 0).
            \end{cases}
        \] Note that $f$ is not differentiable at $(0, 0)$. However, all directional
        derivatives derivatives of $f$ exist at $(0, 0)$. Indeed, consider a
        direction $(\cos\theta, \sin\theta)$, and examine the limit \[
            \lim_{t \to 0} \frac{1}{t}\left[f(t\cos\theta, t\sin\theta) - f(0,
            0)\right] = \cos^3\theta.
        \] 
    \end{example}

    \begin{definition}
        Let $f\colon \R^n \to \R$ be differentiable. The gradient of $f$ is defined
        as the map \[
            \grad{f}\colon \R^n \to \R^n, \qquad x \mapsto
            \left[\pp{f}{x_i}(x)\right]_i.
        \] 
        \begin{remark}
            The gradient at a point $x \in \R^n$ is thought of as a vector. In
            contrast, the derivative is thought of as a linear transformation.
            Otherwise, we see that $\grad{f}(x) = [Df(x)]$.
        \end{remark}
    \end{definition}

    \begin{definition}
        Let $C^1(\R^n)$ be the set of real-valued differentiable functions on $\R^n$.
        Fix a point $a \in \R^n$, then fix a tangent vector $v \in \R^n$. Then, the
        map \[
            \grad_v\colon C^1(\R^n) \to \R, \qquad f\mapsto Df(a)(v)
        \] is a linear functional. The quantity $\grad_v f$ is called the
        directional derivative of $f$ in the direction $v$ at the point $a$.
        \begin{remark}
            We can represent $\grad_v$ as the operator \[
                \grad_v (\cdot) = D(\cdot)(a)(v) = \sum_i v_i \pp{}{x_i}\Big|_a =
                v\cdot \grad (\cdot).
            \]
        \end{remark}
    \end{definition}
    
    \begin{lemma}
        The directional derivatives $\grad_v$ form a vector space called the tangent
        space, attached to the point $a \in \R^n$. This can be identified with the
        vector space $\R^n$ by the natural map $\grad_v \mapsto v$. The standard
        basis can be informally denoted by the vectors \[
            \grad_{\ve_1} \equiv \pp{}{x_1}, \dots, \grad_{\ve_n} \equiv \pp{}{x_n}.
        \] 
    \end{lemma}


    \subsubsection{Differentiation on manifolds *}

    \begin{definition}
        A homeomorphism is a continuous, bijective map whose inverse is also continuous.
    \end{definition}

    \begin{lemma}
        Let $f\colon \R^n \to \R$ be continuous. Denote the graph of $f$ as \[
            \Gamma(f) = \{(x, f(x)): x \in \R^n\}.
        \] Then, $\Gamma(f)$ is a smooth manifold.
    \end{lemma}
    \begin{proof}
        Consider the homeomorphism \[
            \varphi\colon \Gamma(f) \to \R^n, \qquad (x, f(x)) \mapsto x.
        \] This is clearly bijective, continuous (restriction of a projection map),
        with a continuous inverse (from the continuity of $f$). Call this
        homeomorphism $\varphi$ a coordinate map on $\Gamma(f)$.
    \end{proof}

    \begin{definition}
        Let $f\colon M \to \R$ where $M$ is a smooth manifold, with a coordinate map
        $\varphi\colon M \to \R^n$. We say that $f$ is differentiable at a point $a
        \in M$ if $f\circ \varphi^{-1}\colon \R^n \to \R$ is differentiable at
        $\varphi(a)$.
    \end{definition}

    \begin{definition}
        Let $f\colon M \to \R$ where $M$ is a smooth manifold, let $\varphi\colon M
        \to \R^n$ be a coordinate map, and let $a \in M$. Let $\gamma\colon \R \to M$
        be a curve such that $\gamma(0) = a$, and further let $\gamma$ be
        differentiable in the sense that $\varphi\circ \gamma\colon \R \to \R^n$ is
        differentiable. The directional derivative of $f$ at $a$ along $\gamma$ is
        defined as \[
            \dd{}{t} f(\gamma(t))\Big|_{t = 0} = \lim_{h \to 0} \frac{f(\gamma(t +
            h)) - f(\gamma(t))}{h}\Big|_{t = 0}.
        \] Note that we are taking the derivative of $f\circ \gamma\colon \R \to \R$
        in the conventional sense.
    \end{definition}

    \begin{lemma}
        Let $\gamma_1$ and $\gamma_2$ be two curves in $M$ such that $\gamma_1(0) =
        \gamma_2(0) = a$, and \[
            \dd{}{t} \varphi\circ \gamma_1(t)\Big|_{t = 0} = \dd{}{t} \varphi\circ
            \gamma_2(t)\Big|_{t = 0}.
        \] In other words, $\gamma_1$ and $\gamma_2$ pass through the same point $a$
        at $t = 0$, and have the same velocities there. Then, the directional
        derivatives of $f$ at $a$ along $\gamma_1$ and $\gamma_2$ are the same.
    \end{lemma}

    \begin{definition}
        Let $M$ be a smooth manifold, and let $a \in M$. Consider the following
        equivalence relation on the set of all curves $\gamma$ in $M$ such that
        $\gamma(0) = a$. \[
            \gamma_1 \sim \gamma_2 \quad\Longleftrightarrow\quad \dd{}{t} \varphi\circ
            \gamma_1(t)\Big|_{t = 0} = \dd{}{t} \varphi\circ \gamma_2(t)\Big|_{t =
            0}.
        \] Each resultant equivalence class of curves is called a tangent vector at
        $a \in M$. Note that all these curves in a particular equivalence class pass
        through $a$ with the same velocity vector.

        The collection of all such tangent vectors, i.e.\ the space of all curves
        through $a$ modulo the equivalence relation which identifies curves with the
        same velocity vector through $a$, is called the tangent space to $M$ at $a$,
        denoted $T_aM$.

        \begin{remark}
            Each tangent vector $v\in T_aM$ acts on a differentiable function
            $f\colon M \to \R$ yielding a (well-defined) directional derivative at
            $a$. \[
                v\colon C^1(M) \to \R, \qquad f\mapsto \dd{}{t}
                f(\gamma_v(t))\Big|_{t = 0}.
            \]
            Thus, the tangent space represents all the directions in which taking a
            derivative of $f$ makes sense.
        \end{remark}
        \begin{remark}
            The tangent space $T_aM$ is a vector space. Upon fixing $f$, the map
            $Df(a)\colon T_aM \to \R$, $v \mapsto vf(a)$ is a linear functional on
            the tangent space.
        \end{remark}
        \begin{remark}
            Given a tangent vector $v \in T_aM$, it can be identified with its
            corresponding velocity vector in $\R^n$. Thus, the tangent space $T_aM$
            can be identified with the geometric tangent plane drawn to the manifold
            $M$ at the point $a$.
        \end{remark}
    \end{definition}
    

    \subsection{Mean value theorem}
    Consider a differentiable function $f\colon \R^n \to \R$, and fix $a \in \R^n$.
    Define the functions \[
        g_i\colon \R \to \R, \qquad g_i(x) = f(a_1, \dots, a_{i - 1}, x, a_{i + 1},
        \dots, a_n).
    \] Then, each $g_i$ is differentiable, with \[
        g_i'(x) = \pp{f}{x_i}(a_1, \dots, a_{i - 1}, x, a_{i + 1}, \dots, a_n).
    \] By applying the Mean Value Theorem on some interval $[c, d]$, we can find
    $\alpha \in (c, d)$ such that $g_i(d) - g_i(c) = g_i'(\alpha)(d - c)$. In other
    words,
    \[
        f(\dots, d, \dots) - f(\dots, c, \dots) = \pp{f}{x_i}(\dots, \alpha, \dots)(d
        - c).
    \] 

    \begin{theorem}
        Let $f\colon \R^n \to \R^m$ and $a \in \R^n$. Then, $f$ is differentiable at
        $a$ if all the partial derivatives $\partial f / \partial x_j$ exist in a
        neighbourhood of $a$ and are continuous at $a$.
    \end{theorem}
    \begin{proof}
        Without loss of generality, let $m = 1$. We claim that \[
            \lim_{h \to 0} \;\frac{1}{\norm{h}}\norm{f(a + h) - f(a) - \sum_{i = 0}^n
            \pp{f}{x_i}(a)h_i} = 0.
        \] Examine \begin{align*}
            f(a + h) - f(a) &= f(a_1 + h_1, \dots, a_n + h_n) - f(a_1, \dots, a_n) \\
            &= f(a_1 + h_1, \dots, a_n + h_n) - f(a_1 + h_1, \dots, a_{n - 1} + h_{n
            - 1}, a_n) + \\ 
            &\;\quad f(a_1 + h_1, \dots, a_{n - 1} + h_{n - 1}, a_n) - f(a_1 + h_1,
            \dots, a_{n - 1}, a_n) + \\
            &\;\quad \vdots \\
            &\;\quad f(a_1 + h_1, a_2, \dots, a_n) - f(a_1, \dots, a_n) \\
            &= \pp{f}{x_n}(c_n)h_n + \dots + \pp{f}{x_1}(c_1)h_1.
        \end{align*}
        The last step follows from the Mean Value Theorem. As $h \to 0$, each $c_i
        \to a$. Thus, \begin{align*}
            \frac{1}{\norm{h}}\norm{f(a + h) - f(a) - \sum_{i = 0}^n
            \pp{f}{x_i}(a)h_i} 
            &= \frac{1}{\norm{h}}\norm{\sum_{i = 0}^n \left(\pp{f}{x_i}(c_i) -
            \pp{f}{x_i}(a)\right)h_i} \\
            &\leq \sum_{i = 0}^n \left|\pp{f}{x_i}(c_i) - \pp{f}{x_i}(a)\right|
            \frac{|h_i|}{\norm{h}} \\
            &\leq \sum_{i = 0}^n \left|\pp{f}{x_i}(c_i) - \pp{f}{x_i}(a)\right|.
        \end{align*}
        Taking the limit $h \to 0$, observe that $\partial f / \partial x_i (c_i) \to
        \partial f / \partial x_i (a)$ by the continuity of the partial derivatives,
        completing the proof.
    \end{proof}

    \begin{corollary}
        All polynomial functions on $\R^n$ are differentiable.
    \end{corollary}

    \begin{theorem}
        Let $f\colon \R^n \to \R$ be differentiable with continuous partial
        derivatives, and let $a \in \R^n$ be a point of local maximum. Then, $Df(a) =
        0$.
    \end{theorem}
    \begin{proof}
        We need only show that each \[
            \pp{f}{x_i}(a) = 0.
        \] This must be true, since $a$ is also a local maximum of each of the
        restrictions $g_i$ as defined earlier.
    \end{proof}

    \subsection{Inverse and implicit function theorems}
    \begin{theorem}[Inverse function theorem]
        Let $f\colon \R^n \to \R^n$ be continuously differentiable on a neighbourhood
        of $a \in \R^n$, and let $\det(Df(a)) \neq 0$. Then, there exist neighbourhoods
        $U$ of $a$ and $W$ of $f(a)$ such that the restriction $f\colon U \to W$ is
        invertible. Furthermore, $f^{-1}$ is continuous on $U$ and differentiable on
        $U$.
    \end{theorem}

    \begin{lemma}
        Consider a continuously differentiable function $f\colon \R^n \to \R$, and
        let $M$ denote the surface defined by the zero set of $f$. Then, $M$ can be
        represented as the graph of a differentiable function $h\colon \R^{n - 1} \to
        \R$ at those points where $Df \neq 0$.
    \end{lemma}
    \begin{proof}
        Without loss of generality, suppose that $\partial f / \partial x_n \neq 0$
        at some point $a \in M$. It can be shown that the map \[
            F\colon \R^n \to \R^n , \qquad x \mapsto (x_1, x_2, \dots, x_{n - 1},
            f(x))
        \] is invertible in a neighbourhood $W$ of $a$, with a continuous and
        differentiable inverse of the form \[
            G\colon \R^n \to \R^n, \qquad u \mapsto (u_1, u_2, \dots, u_{n - 1},
            g(u)).
        \] Since $F\circ G$ must be the identity map on $W$, we demand \[
            (x_1, x_2, \dots, x_{n - 1}, f(x_1, x_2, \dots, x_{n - 1}, g(x))) = (x_1,
            x_2, \dots, x_{n - 1}, x_n).
        \] Thus, the zero set of $f$ in this neighbourhood of $a$ satisfies $x_n =
        0$, hence \[
            f(x_1, x_2, \dots, x_{n - 1}, g(x_1, x_2, \dots, x_{n - 1}, 0)) = 0.
        \] In other words, the part of the surface $M$ in the neighbourhood of $a$ is
        precisely the set of points \[
            (x_1, x_2, \dots, x_{n - 1}, g(x_1, x_2, \dots, x_{n - 1}, 0)).
        \] Simply set \[
            h\colon \R^{n - 1} \to \R, \qquad x \mapsto g(x_1, x_2, \dots, x_{n - 1},
            0),
        \] whence the surface $M$ is locally represented by the graph of $h$.
    \end{proof}
    \begin{remark}
        Note that by using \[
            f(x_1, \dots, x_{n - 1}, h(x_1, \dots, x_{n - 1})) = 0
        \] on the surface, we can use the chain rule to conclude that for all $1 \leq
        i < n$, we have \[
            \pp{f}{x_i}(a) + \pp{f}{x_n}(a)\pp{h}{x_i}(a_1, \dots, a_{n - 1}) = 0.
        \] 
    \end{remark}

    \begin{theorem}[Implicit function theorem]
        Let $f\colon \R^n\times \R^m \to \R^m$ be continuously differentiable in an
        open set containing $(a, b)$, with $f(a, b) = 0$. Let $\det(\partial f^j /
        \partial x_{n + k} (a, b)) \neq 0$. Then, there exists an open set $U \subset
        \R^n$ containing $a$, an open set $V \subset \R^m$ containing $b$, and a
        differentiable function $g\colon U \to V$ such that $f(x, g(x)) = 0$.

        \begin{remark}
            The condition on the determinant can be rephrased as
            $\operatorname{rank}{Df(a, b)} = m$.
        \end{remark}
    \end{theorem}

    \begin{theorem}
        Let $f\colon \R^n \to \R$ be continuously differentiable, and let $M$ be the
        surface defined by its zero set. Furthermore, let $\grad{f}(a) \neq 0$ for
        some $a \in M$; thus, $M$ can be locally represented by a graph on $\R^{n -
        1}$. Then, $\grad{f}(a)$ is normal to the tangent vectors drawn at $a$ to
        $M$; in fact, the perpendicular space of $\grad{f}(a)$ is precisely the
        tangent space $T_aM$.
    \end{theorem}
    \begin{proof}
        Consider a tangent vector drawn at $a$ to $M$, represented by the
        differentiable curve $\gamma\colon \R \to M$, $\gamma(0) = a$; note that
        we use the identification $\gamma'(0) = v \in \R^n$. Then, calculate \[
            \dd{}{t}f(\gamma(t))\Big|_{t = 0} = Df(\gamma(0))(\gamma'(0)) = Df(a)(v).
        \] On the other hand, we have $f(\gamma(t)) = 0$ identically. Thus, \[
            v\cdot \grad{f}(a) = Df(a)(v) = 0. \qedhere
        \]
    \end{proof}

    \subsection{Taylor's theorem}
    
    \begin{theorem}
        Let $f\colon \R^n \to \R^n$ have continuous second order partial derivatives.
        Then, \[
            \frac{\partial^2 f}{\partial x_i\partial x_j} = \frac{\partial^2
            f}{\partial x_j\partial x_i}.
        \] 
    \end{theorem}

    \begin{theorem}
        Let $f\colon \R^2 \to \R$ have continuous second order partial derivatives,
        and let $(x_0, y_0) \in \R^2$. Then, there exists $\epsilon > 0$ such that
        for all $\norm{(x - x_0, y - y_0)} < \epsilon$, 
        \begin{align*}
            f(x, y) \;=\; &f(x_0, y_0) + \pp{f}{x}(x - x_0) + \pp{f}{y}(y - y_0) \\
            & + \frac{1}{2}\ppn[2]{f}{x}(x - x_0)^2 + \frac{1}{2}\ppn[2]{f}{y}(y -
            y_0)^2 \\
            & + \frac{\partial^2 f}{\partial x\partial y}(x - x_0)(y - x_0) + R(x,
            y),
        \end{align*}
        where as $(x, y) \to (x_0, y_0) \to 0$, the remainder term vanishes as \[
            \frac{|R(x, y)|}{\norm{(x - x_0, y - y_0)}^2} \to 0.
        \] All partial derivatives here are evaluated at $(x_0, y_0)$.
    \end{theorem}
    \begin{proof}
        This follows from applying the Taylor's Theorem in one variable to the real
        function $g\colon \R \to \R$, $t \mapsto f((1 - t)(x_0, y_0) + t(x, y))$.
    \end{proof}


    \subsection{Critical points and extrema}
    \begin{definition}
        We say that $a \in \R^n$ is a critical point of $f\colon \R^n \to \R$ if all
        $\partial f / \partial x^j = 0$ there.
    \end{definition}

    \begin{lemma}
        All points of extrema of a differentiable function are critical points.
    \end{lemma}
    \begin{proof}
        We already know that $Df(a) = 0$ where $a$ is either a point of maximum or
        minimum.
    \end{proof}

    \begin{example}
        In order to find a point of extrema of a $C^2$-smooth function $f\colon \R^2
        \to \R$, we first identify a critical point $(x_0, y_0)$. Next, we must find
        a neighbourhood of $(x_0, y_0)$ which contains no other critical points -- to
        do this, apply Taylor's Theorem. Indeed, we see that \[
            f(x, y) = f(x_0, y_0) + A(x - x_0)^2 + 2B(x - x_0)(y - y_0) + C(y -
            y_0)^2 + R_2.
        \] For non-degeneracy of solutions, we demand $AC - B^2 \neq 0$, i.e.\ at
        $(x_0, y_0)$, we want \[
            \left[\frac{\partial^2 f}{\partial x \partial y}\right]^2 \neq \pp{f}{x}
            \pp{f}{y}.
        \] 

        If $AC - B^2 > 0$ and $\partial^2f / \partial x^2 > 0$, then we have found a
        point of minima; if $\partial^2 f / \partial x^2 < 0$, then we have found a
        point of maximum. If $AC - B^2 < 0$, then we have found a saddle point.
    \end{example}

    \begin{example}
        Suppose that we wish to maximize the function $f\colon \R^2 \to \R$, given an
        equation of constraint $g = 0$, where $g\colon \R^2 \to \R$. Using the method
        of Lagrange multipliers, we look for solutions of the system \[
            \begin{cases}
                \grad{f}(x, y) + \lambda \grad{g}(x, y) = 0, \\
                g(x, y) = 0.
            \end{cases}
        \]
    \end{example}




    \section{Integral calculus}
    
    \subsection{Path integrals}
    
    \begin{definition}
        A closed curve $\gamma\colon [a, b] \to \R^n$ is closed if $\gamma(a) =
        \gamma(b)$. It is called simple if it has no self intersections.
    \end{definition}
    
    \begin{definition}
        Let $p, q\colon U \to \R$ be continuous, where $U \subseteq \R^2$ is an open
        set, and let $\gamma \colon [a, b] \to U$ be piecewise smooth, i.e.\ smooth
        on $(a, b)$ at all but finitely many points. Then, we define \[
            \int_\gamma p\:dx + q\:dy = \int_a^b p(\gamma(t))\:\gamma_1'(t) +
            q(\gamma(t)) \:\gamma_2'(t) \:dt.
        \] 
    \end{definition}


\end{document}
